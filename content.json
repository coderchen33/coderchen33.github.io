{"pages":[],"posts":[{"title":"计算机网络开篇","text":"计算机网络 搞起来 哇咔咔 互联网、因特网与万维网 互联网的组成 计算机网络类别 计算机网络的性能 计算机网络体系结构 互联网、因特网与万维网 互联网：网络把主机连接起来，而互联网是通过某种协议将多种不同的网络连接起来， 因此互联网是网络的网络。 国际标准的互联网写法是internet，字母i小写！ 因特网：采用TCP/IP协议族作为通信的规则的互联网。国际标准的因特网写法是Internet，字母I大写！ TCP/IP协议由很多协议组成，不同类型的协议又被放在不同的层，其中，位于应用层的协议就有很多，比如FTP、SMTP、HTTP。所以，因特网提供的服务一般包括有：www（万维网）服务、电子邮件服务（outlook）、远程登录（QQ）服务、文件传输（FTP）服务、网络电话等等 万维网(www)：只要应用层使用的是HTTP协议，就称为万维网。万维网又称环球网，是一种建立在Internet上的全球性的、交互的、动态、多平台、分布式、图形信息系统。它只是建立在Internet上的一种网络服务。 之所以在浏览器里输入百度网址时，能看见百度网提供的网页，就是因为你的个人浏览器和百度网的服务器之间使用的是HTTP协议在交流。 三者关系：互联网包含因特网，因特网包含万维网 目前，围绕互联网发展的业务主要有两类，分别是ISP（因特网服务提供商）与ICP（因特网网内容提供商） ISP(Internet Service Provider):指向广大用户综合提供互联网接入业务、信息业务和增值业务的电信运营商（比如中国电信、移动、联通提供的宽度服务）ISP拥有从因特网管理机构申请到的多个IP地址，同时拥有通信线路以及路由器等连网设备。中国的电信、网通等都是ISP，我们通常所说的上网就是指通过某个ISP接入到因特网，IP地址的管理机构不会把一个单个的IP地址分配给单个用户，而是把一批IP地址有偿地分配给经审查合格的ISP。现在的因特网是由全世界无数个大大小小的ISP所共同拥有的。 ICP(Internet Content Provider):向广大用户综合提供互联网信息和增值业务的企业（比如：新浪、搜狐、腾讯）。其实，我们说的ICP域名备案就是指互联网内容提供商向工信部提交经营审核。 互联网的组成因特网的拓扑结构虽然非常复杂，并且在地理上覆盖全球，单从工作方式上看，可以划分为以下两大块： 网络边缘：由所连接在因特网上的主机组成。这部分是用户直接使用的，用来进行通信和资源共享。主机也称为端系统 网络核心：由大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的 在互联网边缘的端系统之间的通信方式通常可划分为两类： 客户服务器方式（Client/Server, C/S方式）：客户和服务器都是指通信中所涉及到的两个应用进程。最主要的特征是：客户是服务请求方，服务器是服务提供方C/S方式是因特网上最常用的传统方式，我们在网上发送电子邮件或在网站上查找资料时，都是客户服务器方式。 对等方式（Peer-to-Peer, P2P方式）：两个主机在通信时并不区分哪一个是服务请求者还是服务提供者，这要两个主机都运行了对等连接软件，他们就可以进行平等的、对等连接通信。这时，双方都可以下载对方已经存储在硬盘中的共享文档，因此这种工作方式也称为P2P文件共享。 还有一种被称为浏览器服务器的方式（B/S, Browser/Server方式），可以看作是客户服务器方式的特例。只不过客户端是浏览器而已。 互联网核心部分 电路交换 ：电路交换用于电话通信系统，两个用户要通信之前需要建立一条专用的物理链路，并且在整个通信过程中始终占用该链路。由于通信的过程中不可能一直在使用传输线路，因此电路交换对线路的利用率很低，往往不到 10%。 分组交换 ：分组交换采用存储转发技术。每个分组都有首部和尾部，包含了源地址和目的地址等控制信息，在同一个传输线路上同时传输多个分组互相不会影响，因此在同一条传输线路上允许同时传输多个分组，也就是说分组交换不需要占用传输线路。 计算机网络类别按照网络的作用范围进行分类 广域网WAN:通过长距离(例如跨越不同国家)运送主机发送数据。 作用范围通常为几十到几千公里 城域网MAN: 一般是一个城市。 作用距离为5~50km 局域网LAN: 用计算机或工作站通过高速通信线路相连。作用距离为1km左右。学校或企业大都拥有多个互连的局域网称为校园网或企业网。 个人局域网PAN:把属于个人的电子设备用无线技术连接起来的网络，也常称为无线个人局域网(WPAN)。 作用距离10m左右。 按照网络的使用者进行分类 公用网： 指电信公司出资建造的大型网络。需要向电信公司交纳一定的费用 专用网： 指某个部门因为特殊业务工作需要而建造的网络。不向本单位以外的人提供服务。如军队、铁路、银行、电力系统等。 用来把用户接入到互联网的网络 接入网AN： 搭建起用户与互联网连接的”桥梁”。 计算机网络的性能 比特率：连接在计算机网络上的主机在数字信道上传送数据的速率。比特率速率的单位是b/s（比特每秒），有时候也写为bps，即bit persecond 带宽：表示在单位时间内从网络的某一点到另一点所能通过的最高数据率,单位是bit/s。 吞吐量：在单位时间内通过某个网络的实际数据量。是表示实际网络传输是的速率。 受网络带宽或网络的额定速率的限制。 时延：数据从网络一端传送到另一端所需要的时间。(1) 发送时延： 主机或路由器发送数据帧所需要的时间(2) 排队时延： 分组经过路由器后，要先在输入队列中排队等待处理(3) 处理时延： 主机或路由器收到分组时需要花费一定时间进行处理，如分析首部、提取数据部分、差错检测等。(4) 传播时延： 电磁波在信道中传播一定距离花费的时间。约为光速。 总时延 = 发送时延 + 排队时延 + 处理时 + 延传播时延 另外，在描述数据量大小时，往往使用字节（byte）作为度量单位。一个字节（记为大写的B）代表8个bit即1byte = 8bit在实际上网应用中，下载软件时常常看到诸如下载速度显示为176KB/s，103KB/s等宽带速率大小字样，因为ISP提供的线路带宽使用的单位是比特（bit），而一般下载软件显示的是字节（Byte）（1Byte=8bit），所以要通过换算，才能得实际值。我们以1M宽带为例，按照换算公式换算一下：1Mb/s=1024*1024b/s=1024K b/s=1024/8KB/s=128KB/s 计算机网络体系结构 OSI模型是国际标准组织提出过一个标准框架，只要遵循OSI标准，一个系统就可以和世界上任何地方的、也遵循统一标准的其他任何系统进行通信。但它既复杂又不实用。TCP/IP是一个四层的体系结构，包含应用层、传输层、网际层和网际接口层。不过从实质上讲，TCP/IP只有最上面的三层，网际接口层并没有什么具体内容。因此学习计算机网络的原理时往往采取折中的办法，即总和OSI和TCP/IP的优点，采用一种五层协议的体系结构。 下面看一下每层的具体作用： 应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等协议。数据单位为报文。 传输层 ：为进程提供通用数据传输服务。由于应用层协议很多，定义通用的传输层协议就可以支持不断增多的应用层协议。传输层包括两种协议： 传输控制协议 TCP，提供面向连接、可靠的数据传输服务，数据单位为报文段；用户数据报协议 UDP，提供无连接、尽最大努力的数据传输服务，数据单位为用户数据报。 TCP 主要提供完整性服务，UDP 主要提供及时性服务。 网络层 ：为主机提供数据传输服务。而传输层协议是为主机中的进程提供数据传输服务。网络层把传输层传递下来的报文段或者用户数据报封装成分组。 数据链路层 ：网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同一链路的主机提供数据传输服务。数据链路层把网络层传下来的分组封装成帧。 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 OSI中的剩余两层： 表示层 ：数据压缩、加密以及数据描述，这使得应用程序不必关心在各台主机中数据内部格式不同的问题。 会话层 ：建立及管理会话。五层协议没有表示层和会话层，而是将这些功能留给应用程序开发者处理。 数据传输过程： 在向下的过程中，需要添加下层协议所需要的首部或者尾部，而在向上的过程中不断拆开首部和尾部。","link":"/2020/03/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BC%80%E7%AF%87/"},{"title":"数据链路层","text":"主要内容 数据链路层的三个基本问题：封装成帧、透明传输、差错检测 点对点信道PPP协议 和 广播信道CSMA/CD协议 局域网、以太网、MAC地址和适配器 集线器、网桥、交换机 虚拟局域网(VLAN) 以太网网络接入PPPoE协议 链路层提供的服务1.封装成帧所有在因特网上传送的数据都是以IP数据报为传送单位的，网络层的IP数据报传送到数据链路层就成为帧的数据部分，在帧的数据部分的前面和后面添加上首部和尾部，构成一个完整的帧。 图为RFC 1042和以太网的封装格式其中CRC字段用于帧内后续字节差错的循环冗余码检验 最大传送单元(MTU)(Maximum Transfer Unit)：每一种链路层协议都规定了帧的数据部分的长度上限。比如上图中以太网数据帧的长度最大值分别是1500字节。 路径MTU:如果两台主机之间的通信要通过多个网络，那么每个网络的链路层就可能有不同的MTU。重要的不是两台主机所在网络的MTU的值，重要的是两台通信主机路径中的最小MTU。它被称作路径MTU。注意：从A到B的路由可能与从B到A的路由不同，因此路径MTU在两个方向上不一定是一致的 2.透明传输透明传输，即无论什么样的比特流都能够通过数据链路层传输。 帧使用首部和尾部进行定界，如果帧的数据部分含有和首部尾部相同的内容，那么帧的开始和结束位置就会被错误的判定。 解决这种矛盾的方法是，将数据中可能出现的控制字符的前面插入转义字符“ESC”，而在接收端再删除该转义字符，这种方法被称为字节填充。这个过程透明传输的内容是转义字符，用户察觉不到转义字符的存在。 3.差错检测现实的通信线路都不会是理想的，比特在传输过程中可能会产生差错：1可能会变成0，0也可能会变成1。为了保证数据传输的可靠性，必须采用各种差错检测措施。目前在数据链路层广泛使用的是循环冗余检验CRC（Cyclic Redundancy Check） 我们并没有要求数据链路层向网络层提供“可靠传输”服务，是因为：“可靠传输”指的是数据链路层发送端发送什么，在接收端就收到什么。而传输差错除了上面提到的比特差错外，还会有一些其他差错。如帧丢失、帧重复或帧失序。 信道分类点对点信道一对一通信因为不会发生碰撞，因此也比较简单，使用 PPP(Point-to-PointProtocol)协议进行控制。 PPP协议我们知道，因特网用户通常都要连接到某个ISP才能接入到因特网。PPP协议就是用户计算机和ISP进行通信时所使用的数据链路层协议。 PPP协议由三个部分组成：（1）一个将IP数据报封装到串行链路的方法。（2）一个用来建立、配置和测试数据链路连接的链路控制协议LCP（Link Control Protocol）。（3）一套网络控制协议NCP（Network Control Protocol）。 当用户拨号接入ISP后，就建立了一条从用户PC机到ISP的物理连接。这时，用户PC机向ISP发送一系列的LCP分组（封装成多个PPP帧），以便建立LCP连接。这些分组及其响应选择了将要使用的一些PPP参数。接着还要进行网络层配置，NCP给新接入的用户PC机分配一个临时的IP地址。这样，用户PC机就成为一个拥有IP地址的主机了。 当用户通信完毕时，NCP释放网络层连接，收回原来分配出去的IP地址。接着，LCP释放数据链路层连接。最后释放的是物理层的连接。 PPP帧结构 F 字段为帧的定界符 A 和 C 字段暂时没有意义 FCS 字段是使用 CRC 的检验序列 信息部分的长度不超过 1500 广播信道一对多通信 一个节点发送的数据能够被广播信道上所有的节点接收到。局域网就是一个很好的例子 由于所有的节点都在同一个广播信道上发送数据，因此需要有专门的控制方法进行协调，避免发生冲突（冲突也叫碰撞）。 主要有两种控制方法进行协调，一个是使用信道复用技术(代价较高，不适用于局域网)，一是使用 CSMA/CD（载波监听多点接入/碰撞检测）协议。 CSMA/CD协议CSMA/CD 表示载波监听多点接入 / 碰撞检测。 多点接入 ：说明这是总线型网络，许多主机以多点的方式连接到总线上。 载波监听 ：不管在发送前还是在发送中，每个主机都必须不停地监听信道。在发送前，如果监听到信道正在使用，就必须等待。 碰撞检测 ：在发送中，如果监听到信道已有其它主机正在发送数据，就表示发生了碰撞。虽然每个主机在发送数据之前都已经监听到信道为空闲，但是由于电磁波的传播时延的存在，还是有可能会发生碰撞。 记端到端的传播时延为 τ，最先发送的站点最多经过 2τ 就可以知道是否发生了碰撞，称2τ为争用期 。只有经过争用期之后还没有检测到碰撞，才能肯定这次发送不会发生碰撞。 当发生碰撞时，站点要停止发送，等待一段时间再发送。这个时间采用截断二进制指数退避算法来确定。从离散的整数集合 {0, 1, .., (2k-1)} 中随机取出一个数，记作 r，然后取r倍的争用期作为重传等待时间。 局域网局域网主要特点是网络为一个单位所拥有，且地理范围和站点数目均有限。 局域网具有广播功能，从一个站点可很方便地访问全网，局域网上的主机可共享连接在局域网上的各种硬件和软件资源。 局域网主要有以太网、令牌环网、FDDI 和 ATM 等局域网技术，目前以太网占领着有线局域网市场。 按照网络拓扑结构对局域网进行分类: 以太网由于以太网技术的快速发展，目前在局域网市场中占据了绝对优势，现在以太网几乎成为了局域网的同义词。 传统以太网是总线型网络结构，但随着集线器和双绞线的出现，以太网主要是星型网络拓扑结构。从表面上看，使用集线器的局域网在物理上是一个星型网，但由于集线器使用电子器件模拟实际电缆线工作，因此在逻辑上仍然是一个总线网，使用的还是CSMA/CD协议。 MAC地址MAC 地址是链路层地址，长度为 6 字节（48位），用于唯一标识网络适配器（网卡）。 一台主机拥有多少个网络适配器就有多少个 MAC 地址。例如笔记本电脑普遍存在无线网络适配器和有线网络适配器，因此就有两个 MAC 地址。 MAC帧格式： 如上图所示，MAC地址由五个字段组成。前两个字段为目的地址和源地址。第三个字段是类型字段，用来标识上一层使用的是什么协议，以便把收到的MAC帧的数据上交给上一层的这个协议。第四个字段是数据字段，最后一个是帧检验序列FCS（使用CRC检验）。 适配器计算机与外界局域网的连接是通过适配器。 计算机的硬件地址就在适配器的ROM中，而计算机的IP地址则在计算机的存储器中。 适配器有帧过滤功能。适配器从网络上每收到一个MAC帧就先用硬件检查MAC帧中的目的地址，如果是发往本站的帧则收下，否则丢弃。发往本站的帧包括以下三种帧： 单播帧，即收到的帧的MAC地址与本站的硬件地址相同。 广播帧，即发送给本局域网上所有站点的帧。（全1） 多播帧，即发送给本局域网上一部分站点的帧。 局域网的扩展在物理层扩展局域网集线器 采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备，叫做集线器(hub) 集线器使用电子器件模拟实际电缆线工作，因此在逻辑上仍然是一个总线网 由于是总线网结构,因此集线器使用的还是CSMA/CD协议。所以同一个时间只能有两个端口是互联的，其它的端口都处于载波侦听状态，无法使用网络带宽。 集线器工作在物理层， 作用于比特而不是帧.当一个比特到达接口时，集线器重新生成这个比特，并将其能量强度放大，从而扩大网络的传输距离，之后再将这个比特发送到其它所有接口。如果集线器同时收到两个不同接口的帧，那么就发生了碰撞。 优点： 使原来属于不同碰撞域的局域网上的计算机能够进行跨碰撞域的通信。 扩大了局域网覆盖的地理范围。 缺点 碰撞域增大了，但总的吞吐量并未提高。 如果不同的碰撞域使用不同的数据率，那么就不能用集线器将它们互连起来。 在数据链路层扩展局域网网桥 工作在数据链路层。具有过滤帧的功能,当网桥收到一个帧时，并不是向所有的接口转发此帧，而是先检查此帧的目的MAC地址，然后再确定将该帧转发到哪一个接口。 优点 过滤通信量。 扩大了物理范围。 提高了可靠性。 可互连不同物理层、不同 MAC 子层和不同速率（如10Mb/s 和 100 Mb/s 以太网）的局域网 缺点 存储转发增加了时延。 在MAC子层并没有流量控制功能。 具有不同 MAC子层的网段桥接在一起时时延更大。 网桥只适合于用户数不太多(不超过几百个)和通信量不太大的局域网，否则有时还会因传播过多的广播信息而产生网络拥塞。这就是所谓的广播风暴。 交换机 交换式集线器淘汰了网桥，并且由于性能远超普通集线器，淘汰了在物理层的集线器。目前以太网主要使用交换机 交换机工作在数据链路层，（又称以太网交换机、交换式集线器)。交换机实质上就是一个多接口的网桥。它不会发生碰撞，能根据 MAC 地址进行存储转发。 以太网交换机工作在全双工方式。区别于传统的使用集线器的总线以太网使用CSMA/CD协议，以半双工的方式工作，以太网交换机一般工作在全双工方式，不适用CSMA/CD协议 以太网交换机具有并行性 即能同时连通多对接口，使多对主机能同时通信。所以相互通信的主机都是独占传输媒体，无碰撞地传输数据。比如对于普通 10 Mb/s 的共享式以太网，若共有 N个用户，则每个用户占有的平均带宽只有总带宽(10 Mb/s)的 N 分之一。使用以太网交换机时，虽然在每个接口到主机的带宽还是 10 Mb/s，但由于一个用户在通信时是独占而不是和其他网络用户共享传输媒体的带宽，因此对于拥有 N 对接口的交换机的总容量为 N*10 Mb/s。这正是交换机的最大优点。 以太网交换机是即插即用的 以太网交换机内部的帧交换表（又称为地址表）是通过自学习算法自动地逐渐建立起来的。 虚拟局域网虚拟局域网（Virtual Local Area Network，VLAN）是一组逻辑上的设备和用户，通过端口分配、MAC地址分配等方式将同一局域网内的主机划分为不同的区域（VLAN），不同区域之间的主机无法直接通信（即使它们都在同一个有线局域网中），而同一区域内的主机之间可以正常通信，这就好像一个局域网一样，因此叫做虚拟局域网。 与传统局域网相比优点： 网络设备的移动、添加和修改的管理开销减少 可以控制广播活动 可以提高网络的安全性 完全隔离的两个VLAN如何通信？使用VLAN干线连接来建立虚拟局域网，每台交换机上的一个特殊接口被设置为干线接口，以互连VLAN交换机。该干线端口属于所有VLAN，发送到任何VLAN的帧经过干线链路转发到其他交换机。 交换机如何区别到达干线端口的帧属于哪个VLAN？IEEE 定义了一种扩展的以太网帧格式 802.1Q，它在标准以太网帧上加进了4字节首部VLAN标签，用于表示该帧属于哪一个虚拟局域网。 使用以太网进行网络接入以太网接入的一个重要特点是它可以提供双向的带宽通信，并且可以根据用户对带宽的需求灵活的进行带宽升级。 然而以太网帧格式标准中，在地址字段部分并没有用户名字段，也没有让用户键入密码来鉴别用户身份过程，于是人们想到将PPP协议再封装到以太网中来传输，就是1999年公布的PPPoE协议。因此，PPPoE协议是宽带上网的主机使用的链路层协议。","link":"/2020/03/07/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"title":"网络层(一)","text":"主要内容 虚拟互联网络的概念 IP地址和物理地址的关系 传统的分类的IP地址（子网掩码）和无分类域间路由选择CIDR 网络层服务及虚拟互联网的概念因为网络层是整个互联网的核心，因此应当让网络层尽可能简单。 网络层向上只提供简单的、无连接的、尽最大努力交互(不可靠)的数据报服务。 无连接：IP并不需要事先建立连接或者维护任何关于后续数据报的状态信息。每个数据报的处理相互独立，并且可以不按发送顺序接收。 不可靠：它不能保证IP数据报能成功地到达目的地。IP仅提供最好的传输服务 所传送的分组可能出错、丢失、重复和失序，当然也不保证分组交付的时限。 使用 IP 协议，可以把异构的物理网络连接起来，使得在网络层看起来好像是一个统一的网络称为虚拟互联网络，由于使用的是IP协议也称为IP网。 与IP协议配套使用的还有三个协议： 地址解析协议 ARP (Address Resolution Protocol) 网际控制报文协议 ICMP (Internet Control Message Protocol) 网际组管理协议 IGMP (Internet Group Management Protocol) 逆地址解析协议(RARP) (DHCP协议(应用层协议)已经包含了RARP协议，所以RARP协议现在已经不使用了) IP地址和物理地址物理地址是数据链路层和物理层使用的地址，而IP地址（32位）是网络层和以上各层使用的地址，是一种逻辑地址。 下图为3个局域网用2个路由器R1和R2连接起来，现在主机H1要和主机H2进行通信。 通信路径为 H1-&gt;经过R1转发-&gt;经过R2转发-&gt;H2 其中路由器R1和R2同时连接在两个局域网上，因此有两个硬件地址 实际在不同层次中的不同区间的源地址和目的地址 我们可以知道： 在IP层抽象的互联网上只能看到IP数据报 路由器只根据目的IP地址进行路由选择 在局域网链路层只能看见MAC地址。 (从表中可以看到在从H1到H2通信过程中MAC帧首部的源地址和目的地址发生变化，在上面的IP层看不见这种变化) IP层抽象的互联网屏蔽了下层复杂细节，只要我们在网络层上讨论问题，就能够使用统一的、抽象的IP地址研究主机和主机或路由器之间的通信。 ARP协议网络层使用的是IP地址，最终在实际网络的链路上传送数据帧时，最终还是必须使用硬件地址。这就需要解决一个问题，如何根据一个机器的IP地址找出其对应的物理地址 地址解析协议ARP就是根据IP地址找出其对应的物理地址。 ARP工作过程： 当主机A要向本局域网上的某台主机B发送IP数据报，就先在其ARP高速缓存中查看有无主机B的IP地址。 如果ARP高速缓存中有该IP地址到MAC地址的映射，就在ARP高速缓存中查出其对应的硬件地址，再把硬件地址写入MAC帧中，然后通过局域网把该MAC地址发往此硬件地址。 如果没有，此时主机A就自动运行ARP。首先通过广播的方式发ARP请求分组，主机B收到该请求后会发送ARP响应分组给主机A告知其MAC地址。 主机A收到主机B的ARP响应分组后，向其高速缓存中写入主机B的IP地址到 MAC 地址的映射。 ARP协议抓包分析 ARP协议注意: ARP协议只在局域网中工作,如果要找的主机和源主机不在同一个局域网上(如上面的H1和H2)，H1就无法解析出另一个局域网H2主机。而是把路由器R1的IP地址解析为硬件地址HA3,以便能把IP数据报传送到路由器R1。 之后再由R1进行分组转发，直至最终交付给主机H2。 每个主机都有一个ARP高速缓存，里面有本局域网上的各主机和路由器的IP地址到MAC地址的映射表，并且这个映射表还可以动态更新。 IP地址划分IP地址就是给互联网上的每一台主机(或路由器)的每一个接口分配一个在全世界范围内唯一的32位的标识符。对IP地址的编址方式经历了三个历史阶段： 分类的IP地址 （最基本编址方法） 子网的划分 （对最基本编址方法的改进） 构成超网 （无分类编址，1993年提出后很快得到推广应用） 分类的IP地址由两部分组成，网络号和主机号。IP 地址 ::= {&lt; 网络号 &gt;, &lt; 主机号 &gt;} 这些32位的地址通常写成四个十进制的数，其中每个整数对应一个字节。这种表示方法称作“点分十进制表示法（Dotted decimal notation）”区分各类地址的最简单方法是看它的第一个十进制整数，如下图所示。 常用IP 子网的划分由三部分组成，网络号、子网号和主机号。IP地址 ::= {&lt;网络号&gt;, &lt;子网号&gt;, &lt;主机号&gt;} 为什么需要划分子网呢，因为上面的两级IP地址虽然方便，但却有很多缺点： IP地址空间的利用率有时很低 给每一个物理网络分配一个网络号会使路由表变得太大而使网络性能变坏 两级IP地址不够灵活 所以为了解决上面的问题，所以在分类IP地址中增加了一个子网字段，使得两级IP地址变为三级IP地址。这种方法叫做划分子网 划分子网的注意: 划分子网纯属一个单位内部的事情。单位对外仍然表现为一个网络。 划分子网的方法是从主机号借用若干个位作为子网号subnet-id，而主机号host-id也就相应减少了若干个位。 凡是从其他网络发送给本单位某个主机的IP数据报，仍然是根据 IP 数据报的目的网络号 net-id，先找到连接在本单位网络上的路由器。然后此路由器在收到 IP 数据报后，再按目的网络号 net-id 和子网号 subnet-id 找到目的子网(需要借助于子网掩码)。最后就将 IP 数据报直接交付目的主机。 无分类编址CIDR（构造超网）由网络前缀号和主机号组成。IP地址::= {&lt; 网络前缀号 &gt;, &lt; 主机号 &gt;} 无分类编址CIDR消除了传统 A类、B类和C类地址以及划分子网的概念，并且网络前缀的长度可以根据需要变化。 CIDR的记法上采用在IP地址后面加上网络前缀长度的方法，例如 128.14.35.7/20表示前20位为网络前缀。 CIDR的地址掩码可以继续称为子网掩码，子网掩码1的长度为网络前缀的长度。 一个组织通常被分配成一块连续的地址，即具有相同前缀的一段地址，这种情况下，该组织内部的设备的IP地址将共享共同的前缀。剩余的32-x比特用于区分该组织内部设备。 一个 CIDR地址块中有很多地址，一个CIDR表示的网络就可以表示原来的很多个网络，并且在路由表中只需要一个路由就可以代替原来的多个路由，减少了路由表项的数量。把这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为构成超网。 子网掩码上面使用子网的IP地址和无分类编址CIDR都提到了子网掩码，什么是子网掩码呢？它的作用是什么呢？我们举个例子就可以理解了 下图表示某单位拥有一个B类IP地址，网络地址是145.13.0.0（网络号是145.13） 现把上图中的网络划分为三个子网，这里假定子网号占用8位，因此主机号只剩8位。所划分的子网分别是：145.13.3.0 145.13.7.0、145.13.21.0 划分子网后，整个网络对外仍表现为一个网络，其网络地址仍为145.13.0.0，但是路由器R1在收到外来的数据报后，再根据数据报的目的地址把它转发到相应的子网。 假定有一个数据报的目的地址是145.13.3.10已经到达路由器R1，那么这个路由器如何把它转发到子网145.13.3.0呢？这就需要借助子网掩码（subnetmask）来实现了。 路由器会把子网掩码和收到的数据报地址的目的IP地址145.13.3.10进行按位“与”操作，得出所要找的子网的网络地址 可以看到，子网掩码与IP地址进行“与”操作之后，就将主机号“过滤”掉了，只剩下了网络号与子网号。 实际上，因特网的标准规定：所有网络必须使用子网掩码。即便一个网络没有划分子网，也要使用默认子网掩码。默认子网掩码中1的位置和IP地址中的网络号字段正好相对应，因此，两者进行“与”操作后，就能得出该IP地址的网络地址。 A类、B类、C类地址的默认子网掩码是固定的： 例，已知 IP 地址是141.14.72.24（注意是IP数据报中的目的地址），子网掩码是 255.255.192.0。试求子网网络地址。","link":"/2020/03/07/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"title":"网络层(二)","text":"主要内容 IP数据报格式 IP层分组转发算法 路由选择协议 ICMP协议、ping、traceroute 路由器的功能就是进行分组转发和路由选择，路由器是如何在知道IP地址时进行分组转发或者路由选择呢？ 为此，我们需要先了解下IP数据报的格式。 IP数据报的格式 版本 : 有 4（IPv4）和 6（IPv6）两个值； 首部长度 : 占4位，因此最大值为15。值为1表示的是1个32位字的长度，也就是4字节（即单位是4个字节，最多表示60个字节）。因为IP首部固定部分长度为20字节，因此该值最小为5。如果可选字段的长度不是4字节的整数倍，就用尾部的填充部分来填充。最常用的首部长度是20字节，这时不适用任何选项。 区分服务 : 占8位，用来获得更好的服务，一般情况下不使用。 总长度 : 包括首部长度和数据部分长度。单位为字节。因此数据报最大长度为2^16-1=65535字节。但我们知道数据链路层中规定了数据帧中的数据字段的最大长度即最大传送单元MTU（通常为1500字节）。当数据报总长度超过MTU时，就必须把过长的数据报进行分片处理。在进行分片时，数据报首部中的总长度字段指分片后的每一个分片的首部长度与该分片的数据长度的总和。 标识 : 在数据报长度过长从而发生分片的情况下，相同数据报的不同分片具有相同的标识符。 标志 ： 占3位，但目前只有两位有意义。其中最低位记为MF。MF=1即表示后面还有分片的数据报，MF=0表示这是若干数据报中的最后一个。中间位记为DF，意思是不能分片，只有当DF=0时才允许分片。 片偏移 : 和标识符一起，用于发生分片的情况。表明较长的分组在分片后，某片在原分组中的相对位置，也就是说相对于用户数据字段的起点。片偏移的单位为8字节。 生存时间 ：TTL，它的存在是为了防止无法交付的数据报在互联网中不断兜圈子。以路由器跳数为单位，当 TTL为0时就丢弃数据报。 协议 ：指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP、UDP 等。 首部检验和 ：因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。 源地址：32位 目的地址：32位 分片： 分组转发算法使用分类IP的分组转发算使用分类的IP地址的转发，可以看到路由表中主要是以下两个信息： （目的网络地址， 下一跳地址） 以路由器R2的路由表为例，由于R2同时连接在网络2和3上，因此只要目的主机在网络2或3上，都可以通过接口0或1有路由器R2直接进行交付（当然还要利用地址解析协议ARP才能找到这些主机的硬件地址）。若目的主机在网络1中，则下一跳路由应为R1，其IP地址为20.0.0.7。路由器R1和R2由于同时连接在网络2上，因此从路由器R2把分组转发给路由器R1是很容易的。 使用分类IP的分组转发算法过程 从数据报的首部提取目的主机的 IP 地址 D, 得出目的网络地址为 N。 若网络 N 与此路由器直接相连，则把数据报直接交付目的主机 D；否则是间接交付，执行步骤3。 若路由表中有目的地址为 D 的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由器；否则，执行步骤4。 若路由表中有到达网络 N 的路由，则把数据报传送给路由表指明的下一跳路由器；否则，执行步骤5。 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器；否则，执行步骤6。 报告转发分组出错。 上面步骤5中提到的默认路由对于连接在小网络上的主机发送IP数据报时是十分有用的。如下图中连接在网络N1上的任何一个主机中的路由表只需要三个项目即可。 本网络主机路由，其目的网络就是本网络N1,因而不需要路由转发。 到网络N2的路由，对应的下一条路由器是R2 默认路由，只要目的网络是其他网络(不是N1或N2),就一律选择默认路由，把数据报先交付路由器R1,让R1再转发给互联网中的下一个路由，一直转发到目的网络上的路由器，最后进行直接交付。 使用子网的分组转发算法在划分子网的情况下，分组转发算法必须做相应改动，路由表中必须包含以下三项内容:目的网络地址、子网掩码、下一跳地址 使用子网的分组转发算法 从收到的数据报的首部提取目的IP地址D。 先判断是否为直接交付。对路由器直接相连的网络逐个进行检查：用各网络的子网掩码和D逐位“与”，看结果是否和相应的网络地址匹配，若匹配，则把分组进行直接交付（当然还需要把D转换成物理地址，把数据报封装成帧发送出去），转发任务结束。否则是间接交付，执行步骤3。 若路由表中有目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由。否则执行步骤4 对路由表中的每一行（目的网络地址、子网掩码、下一跳地址），用其中的子网掩码和D逐位相“与”，其结果为N。若N与该行的目的网络地址匹配，则把数据报传动给该行指明的下一跳路由。否则执行步骤5. 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器；否则，执行步骤6。 报告转发分组出错。 比如对于上面的图中H1与主机H2之间进行通信，首先H1会将H2的IP地址128.30.33.128和本子网的网络地址128.30.33.0相与，得到128.30.33.0，与自己本子网的网络地址不同，则判断为间接交付，将数据报交付给路由器R1，由R1进行转发。 路由器收到数据报后，根据路由表中的记录，将IP地址与子网掩码进行逐位相与，如先看R1路由表的第一行。将子网掩码和目的IP逐位相与后得到128.30.33.128，与这一行的目的网路地址128.30.33.0比较不匹配。 则继续寻找下一行。 可以看到在第二行中匹配成功，则不再继续找下去而是R1把分组从接口1直接交付主机H2。 使用CIDR的分组转发算法在使用CIDR时，由于采用了网络前缀的这种方法，因此在路由表中的项目也要有相应改变。这时，每个项目由两个内容构成：网络前缀、下一跳地址 但是在查找时可能会得到不止一个匹配结果，此时应当采用最长前缀匹配来确定应该匹配哪一个。这是因为网络前缀越长，其地址块越小，因而路由选择就越具体。 具体转发流程和使用子网的分组转发方法类似，只不过中间是用掩码相与后，再使用最长前缀匹配进行选择，不在赘述。 比如，大学下属四系希望ISP把转发给四系的数据报直接发送到四系而不经过大学路由器，则在ISP路由器的路由表中至少有以下两个项目，即206.0.68.0/22(大学) 和206.0.71.128/25。假定目的IP地址为D=206.0.171.130,则按照下图进行掩码逐位与后都匹配，根据最长匹配前缀原理，应当选择后者。 注意： CIDR使用32位的地址掩码，其中1的长度为网络前缀的长度，例如/20的地址掩码为11111111 11111111 11110000 0000。 虽然CIDR不使用子网了，但是仍然可将地址掩码称为子网掩码。注意这个不使用子网的意思是说没有在32位地址中指明若干位作为子网字段，但分配到一个CIDR地址块后，仍然可以在本单位继续划分子网。 并且这些子网的网络前缀比整个单位网路的网络前缀长。 由于CIDR使用的是地址块，因此在路由表中可以直接利用CIDR地址块来查找目的网络，使得路由表中的一个项目可以表示原来传统分类地址的多个路由。这种路由聚合也称为构成超网 路由选择协议路由选择协议就是用来解决路由器中的路由表是如何获得的。 互联网采用的路由选择协议主要是自适应的(即动态的)，分布式路由选择协议。 互联网可以划分为许多较小的自治系统 AS，一个 AS 可以使用一种和别的 AS 不同的路由选择协议。在目前的互联网中，一个大的ISP就是一个自治系统。这样互联网就把路由选择协议划分为两大类： -内部网关协议IGP ： 即在一个自治系统内部使用的路由选择协议。主要有RIP 和 OSPF协议-外部网关协议EGP： 若源主机和目的主机处在不同的自治系统中，当数据报传送到一个自治系统边界时，就需要使用一种协议将路由选择信息传递到另一个自治系统中。这样的协议就是外部网关协议。主要有BGP协议。 内部网关协议 内部网关协议RIP RIP 是一种基于距离向量的路由选择协议。距离是指跳数，直接相连的路由器跳数为 1。跳数最多为 15，超过 15 表示不可达。 RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。 距离向量算法： 对地址为 X 的相邻路由器发来的 RIP 报文，先修改报文中的所有项目，把下一跳字段中的地址改为 X，并把所有的距离字段加 1； 对修改后的 RIP 报文中的每一个项目，进行以下步骤： 若原来的路由表中没有目的网络 N，则把该项目添加到路由表中； 否则：若下一跳路由器地址是 X，则把收到的项目替换原来路由表中的项目；否则：若收到的项目中的距离 d 小于路由表中的距离，则进行更新（例如原始路由表项为 Net2, 5, P，新表项为 Net2, 4, X，则更新）；否则什么也不做。 若 3 分钟还没有收到相邻路由器的更新路由表，则把该相邻路由器标为不可达，即把距离置为 16。 RIP 协议实现简单，开销小。但是 RIP 能使用的最大距离为 15，限制了网络的规模。并且当网络出现故障时，要经过比较长的时间才能将此消息传送到所有路由器。 内部网关协议 OSPF 开放最短路径优先 OSPF，是为了克服 RIP 的缺点而开发出来的。 开放表示 OSPF 不受某一家厂商控制，而是公开发表的；最短路径优先表示使用了 Dijkstra 提出的最短路径算法 SPF。 OSPF 具有以下特点： 向本自治系统中的所有路由器发送信息，这种方法是洪泛法。 发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。 只有当链路状态发生变化时，路由器才会发送信息。 所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。 外部网关协议 BGPBGP（Border Gateway Protocol，边界网关协议） AS 之间的路由选择很困难，主要是由于： 互联网规模很大； 各个 AS 内部使用不同的路由选择协议，无法准确定义路径的度量； AS 之间的路由选择必须考虑有关的策略，比如有些 AS 不愿意让其它 AS 经过。 BGP 只能寻找一条比较好的路由，而不是最佳路由。 每个 AS 都必须配置 BGP 发言人，通过在两个相邻 BGP 发言人之间建立 TCP 连接来交换路由信息。 网际控制报文协议ICMPICMP协议是为了能够更有效地转发IP数据报和提高交付成功的机会。但ICMP不是高层协议(虽然ICMP数据报文作为IP层数据)，而是IP层协议。 ICMP报文： ICMP报文种类有两种： ICMP差错报告报文 ICMP询问报文 常见ICMP报文类型 ICMP差错报告报文具有相同的格式： 其中取需要进行差错报告的IP数据报数据字段前8个字节是为了得到运输层端口号(对于TCP和UDP)以及运输层报文的发送序号(对于TCP)。 ICMP常见的应用就是ping和traceroute ping ping是分组网间嗅探，用来测试两台主机的连通性。 ping是应用层直接使用网络层，没有经过传输层的TCP或UDP ping百度的抓包分析： 可以看到执行ping命令后，直接进行了4次request和reply。在下面的请求报文中可以看到和上面的ICMP报文格式一一对应。这里的 Type=8,code=0, 校验是正确，且这是一个Echo请求报文。我们再点击Responseframe:8，这里说明响应报文在序号8。详情如下： 同样的Type=8,code=0校验正确，且最下面根据请求和响应的时间戳计算出来的响应延迟。 tracerouteTraceroute是ICMP的另一个应用，用来跟踪一个分组从源点到终点的路径。traceroute是UNIX系统中的名字，在windows系统中是tracert。 traceroute发送的IP数据报封装的是无法交付的UDP用户数据报（使用了非法端口号），并由目的主机发送终点不可达差错报告报文。 源主机向目的主机发送一连串的IP数据报。第一个数据报P1的生存时间TTL设置为1，当P1到达路径上的第一个路由器R1时，R1收下它并把TTL减 1，此时TTL等于0，R1就把P1丢弃，并向源主机发送一个ICMP时间超过差错报告报文； 源主机接着发送第二个数据报P2，并把TTL设置为2。P2先到达 R1，R1收下后把TTL减 1 再转发给R2，R2收下后也把TTL减1，由于此时TTL等于0，R2就丢弃P2，并向源主机发送一个ICMP时间超过差错报文。 不断执行这样的步骤，直到最后一个数据报刚刚到达目的主机，主机不转发数据报，也不把TTL值减1。但是因为数据报封装的是无法交付的UDP，因此目的主机要向源主机发送ICMP终点不可达差错报告报文。 之后源主机知道了到达目的主机所经过的路由器IP地址以及到达每个路由器的往返时间。 UDP抓包(traceroute和tracert的不同)","link":"/2020/03/09/%E7%BD%91%E7%BB%9C%E5%B1%822/"},{"title":"HTTP(一)","text":"主要内容 HTTP协议概念和特点 HTTP状态码 HTTP请求方法 HTTP报文 URL和URI Cookie和Session HTTP概念和特点什么是HTTP呢？ HTTP (HyperText Transfer Protocol)是超文本传输协议，超文本指的是包含文字、图片、音频、视频的资源。 HTTP 协议是双向的。 HTTP 是一个在计算机世界里专门在两点之间传输数据的约定和规范 HTTP特点(针对http/1.1而言)： 参考文章硬核！30 张图解 HTTP 常见的面试题 优点 简单 HTTP 基本的报文格式就是header + body，头部信息也是 key-value 简单文本的形式，易于理解。 灵活和易于扩展 HTTP协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。 同时 HTTP 由于是工作在应用层（ OSI 第七层），则它下层可以随意变化。HTTPS 也就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层，HTTP/3 甚至把 TCPP 层换成了基于 UDP 的 QUIC。 应用广泛和跨平台从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用片地开花，同时天然具有跨平台的优越性。 缺点 HTTP 协议里有优缺点一体的双刃剑，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。 无状态双刃剑 无状态的好处，因为服务器不会去记忆 HTTP 的状态，所以能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。 无状态的坏处，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。 对于无状态的问题，可以使用比较简单的方式用 Cookie技术解决。 明文传输双刃剑 明文传输的好处是在传输过程中的信息是可方便阅读的，通过浏览器的 F12 控制台或 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。 明文传输的坏处在于HTTP 的所有信息毫无隐私可言，很容易就能被窃取 不安全 不安全是HTTP协议比较严重的缺点，可以使用 HTTPS 解决 HTTP状态码状态码用来描述客户端向服务器端发送请求时，服务端返回的请求结果。 1XX 1xx 类状态码属于提示信息，是协议处理中的一种中间状态，实际用到的比较少。 2XX 一般是请求成功 200 OK 正常处理 204 No Content 成功处理，但服务器没有新数据返回，显示页面不更新 206 Partial Content 对服务器进行范围请求，只返回一部分数据 3XX 一般表示重定向 301 Moved Permanently 请求的资源已分配了新的URI中，URL地址改变了。【永久重定向】 302 Found 请求的资源临时分配了新的URI中，URL地址没变【转发】 303 See Other 与302相同的功能，但明确客户端应该采用GET方式来获取资源 304 Not Modified 发送了附带请求，但不符合条件【返回未过期的缓存数据】 307 Temporary Redirect 与302相同，但不会把POST请求变成GET 4XX 表示客户端出错了 400 Bad Request 请求报文语法错误了 401 Unauthrized 需要认证身份 403 Forbidden 没有权限访问 404 Not Found 服务器没有这个资源 5XX 服务器出错了 500 Internal Server Erro 内部资源出错了 503 Service Unavailable 服务器正忙 HTTP请求方法 我们比较常用的就是GET和POST了 GET和POST区别： GET方法时请求从服务器获取资源，而POST方法是向URI指定资源提交数据。 GET方法是安全且幂等的，而POST方法是不安全且不幂等的。 GET请求响应能被缓存，而POST请求响应不能缓存(一般也不缓存) GET请求参数以查询字符串出现在URL中，而POST请求参数存储在实体中。 对于第2点补充下安全和幂等的概念： 在 HTTP 协议里，所谓的「安全」是指请求方法不会「修改」服务器上的资源状态。 所谓的「幂等」，意思是方法执行多次，得到的结果都是「相同」的。 那么很明显 GET方法就是安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。 POST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。 HTTP报文HTTP报文可以分为报文首部、空行、报文主体两块。客户端的HTTP报文称为请求报文，服务器端的叫做响应报文。 对于请求报文主要包含：a、请求行：包含请求方法、URI、HTTP版本信息 (GET /HTTP/1.1)b、请求首部字段（请求头）c、空行d、请求内容实体 对于响应报文主要包含：a、响应行：包含HTTP版本、状态码、状态码的原因短语(HTTP/1.1 200 OK)b、响应首部字段（响应头）c、空行d、请求内容实体 下面是我们访问www.baidu.com的请求头和响应头。请求头： 响应头： 我们知道请求头和响应头一共包含四种首部字段，下面直接给出参数，以供以后查阅 请求首部字段 响应首部字段 通用首部字段 实体首部字段 URL和URIURI是统一资源标识符URL是统一资源定位符 对于URI和URL的理解，可以这样理解：对于网络中的所有资源，我们需要统一来对资源进行唯一标识 URI就是一种规定，说可以根据不同的协议来用不同的方式来表示资源的定位标识符。而URL和URN就是一种具体的实现。 URL就是通过网络资源的位置来唯一标识网络上的资源，URN就是通过编号来进行标识。 Cookie和SessionCookie Cookie是解决HTTP/1.1的无状态连接的 Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。 Cookie的用途 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） Cookie的创建和工作过程服务器发送的响应报文包含 Set-Cookie 首部字段，客户端得到响应报文后把 Cookie 内容保存到浏览器中。 HTTP/1.0 200 OKContent-type: text/htmlSet-Cookie: yummy_cookie=chocoSet-Cookie: tasty_cookie=strawberry 客户端之后对同一个服务器发送请求时，会从浏览器中取出 Cookie 信息并通过 Cookie 请求首部字段发送给服务器。 GET /sample_page.html HTTP/1.1Host: www.example.orgCookie: yummy_cookie=choco; tasty_cookie=strawberry SessionSession 可以用来将用户信息存储在服务器端，存储在服务器端的信息更加安全。 Session 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。 使用 Session 维护用户登录状态的过程如下 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID； 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后拿到 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。 应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。","link":"/2020/03/10/HTTP/"},{"title":"HTTP/1.1、HTTP2和HTTP3","text":"主要内容 HTTP/1.1和HTTP/1.0对比(长连接、短连接) HTTP/1.1和HTTP/2对比 HTTP/2和HTTP/3对比 HTTP1.1与HTTP/1.0对比 HTTP/1.1默认使用持久化连接(长连接)，而HTTP/1.0使用短连接。 管线化 客户端可以同时发出多个HTTP请求，而不用一个个等待响应 断点续传 实际上就是利用HTTP消息头使用分块传输编码，将实体主体分块传输。 长连接和短连接短连接就是：每请求一个资源都要重新进行一次HTTP连接。 比如我们请求一个网页，这个网页中包含html文档、js文件、css文件、图片等，对每个资源都进行一次HTTP连接。 短连接过程：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接 长连接就是：建立一次连接，多次请求均由这个连接完成！ 只要客户端和服务器端任意一端没有明确提出断开TCP连接，就一直保持连接。 当然如果这个连接阻塞了，还是会开新的TCP连接的 长连接的过程：建立连接——数据传输…（保持连接）…数据传输——关闭连接 由于HTTP协议传输层是TCP协议，所以HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 管线化长连接使管线化连接成为了可能，客户可以不等待响应，直接发送下一个请求，从而做到同时并行发送多个请求，节约时间。 HTTP/1.1、HTTP/2和HTTP/3的发展HTTP2和HTTP/1.1对比虽然上面提到了HTTP/1.1的很多优点，但是HTTP/1.1仍然存在着很多不足： 队头阻塞(Head of line blocking) HTTP/1.1通过管道技术实现一次性发送多个请求,但是这种技术在接收响应时，要求必须按照发送请求的顺序返回。如果，第一个请求被堵塞了，则后面的请求即使处理完毕了，也需要等待，这就是队头阻塞。 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分 发送冗长的首部。每次互相发送相同的首部造成的浪费较多 没有请求优先级控制 请求只能从客户端开始，服务器只能被动响应 针对上述问题HTTP/2对HTTP/1.1做了以下改进： 多路复用 HTTP/2 是可以在一个连接中并发多个请求或回应，而不用按照顺序一一对应。也就不会再出现「队头阻塞」问题，降低了延迟，大幅度提高了连接的利用率。 图片来自文章HTTP2和HTTPS不来了解下 二进制格式HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了二进制格式。头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧和数据帧。 数据流HTTP2连接上传输的每个帧都关联到一个“流”。流是一个独立的，双向的帧序列可以通过一个HTTP2的连接在服务端与客户端之间不断的交换数据。此外，客户端还可以指定数据流的优先级。优先级高的请求，服务器就先响应该请求。 头部压缩HTTP/2 会压缩头（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的分。这就是所谓的HPACK 算法 服务器推送HTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务不再是被动地响应，也可以主动向客户端发送消息。 例如，在浏览器刚请求 HTML 的时候，就提前把可能会用到的 JS、CSS 文件等静态资源主动发给客户端，减少延时的等待，也就是服务器推送 HTTP/3和HTTP/2对比HTTP/2 主要的问题在于：多个 HTTP 请求在复用一个 TCP 连接，下层的 TCP 协议是不知道有多少个 HTTP 请求的。 所以一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。 HTTP/1.1 中的管道传输中如果有一个请求阻塞了，那么队列后请求也统统被阻塞住了 HTTP/2 多请求复用一个TCP连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。 这都是基于 TCP 传输层的问题，所以HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！ UDP 发生是不管顺序，也不管丢包的，所以不会出现 HTTP/1.1 的队头阻塞 和 HTTP/2 的一个丢包全部重传问题。 大家都知道 UDP 是不可靠传输的，但基于 UDP 的QUIC 协议 可以实现类似 TCP 的可靠性传输。 QUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响。 TL3 升级成了最新的 1.3 版本，头部压缩算法也升级成了 QPack。 HTTPS 要建立一个连接，要花费 6 次交互，先是建立三次握手，然后是 TLS/1.3 的三次握手。QUIC 直接把以往的 TCP 和 TLS/1.3 的 6 次交互合并成了 3 次，减少了交互次数。","link":"/2020/03/10/HTTP-1-1%E3%80%81HTTP2%E5%92%8CHTTP3/"},{"title":"Java常用类","text":"","link":"/2020/03/10/Java%E5%B8%B8%E7%94%A8%E7%B1%BB/"},{"title":"物理层","text":"主要内容 物理层的任务 信道复用技术 宽带接入技术 物理层任务主要任务：确定与传输媒体的接口的一些特性。 机械特性 ：指明接口所用接线器的形状和尺寸、引线数目和排列、固定和锁定装置等。 电气特性：指明在接口电缆的各条线上出现的电压的范围。4V— 6V表示二进制“0”,用-4V—-6V表示二进制“1” 功能特性：指明某条线上出现的某一电平的电压表示何种意义。 过程特性 ：指明对于不同功能的各种可能事件的出现顺序。 信道复用技术 频分复用：所有主机在相同的时间占用不同的频率带宽资源 时分复用：所有主机在不同的时间占用相同的频率带宽资源 统计时分复用：是对时分复用的一种改进，不固定每个用户在时分复用帧中的位置，只要有数据就集中起来组成统计时分复用帧然后发送 宽带接入技术 ADSL非对称数字用户线ASDL是用数字技术对模拟电话用户线进行改造。 ADSL把0~4kHZ的低频谱留给传统电话使用，而把原来没有被利用的高端频谱留给用户上网使用。 由于我们在上网时主要是从互联网上下载各种文档，而向互联网发送的信息量一般都不打。因此ADSL的下行(从ISP到用户)带宽都远大于上行(从用户到ISP)带宽，因此成为非对称的。 ADSL 的特点： 上行和下行带宽做成不对称的。 ADSL 在用户线（铜线）的两端各安装一个 ADSL 调制解调器。 我国目前采用的方案是离散多音调 DMT，DMT 调制技术采用频分复用的方法 ADSL 采用自适应调制技术使用户线能够传送尽可能高的数据率 光纤同轴混合网（HFC网）在有线电视网的基础上开发的一种居民宽带接入网。 用户接口盒 UIB (User Interface Box) 要提供三种连接，即： 使用同轴电缆连接到机顶盒 (set-top box)，然后再连接到用户的电视机。 使用双绞线连接到用户的电话机。 使用电缆调制解调器连接到用户的计算机。 FTTx FTTx 表示 Fiber To The…（光纤到…），例如： 光纤到户 FTTH (Fiber To The Home)：光纤一直铺设到用户家庭，可能是居民接入网最后的解决方法。 光纤到大楼 FTTB (Fiber To The Building)：光纤进入大楼后就转换为电信号，然后用电缆或双绞线分配到各用户。 光纤到路边 FTTC (Fiber To The Curb)：光纤铺到路边，从路边到各用户可使用星形结构双绞线作为传输媒体。","link":"/2020/03/07/%E2%80%9C%E7%89%A9%E7%90%86%E5%B1%82/"},{"title":"应用层","text":"主要内容 域名系统DNS 文件传送协议 FTP 电子邮件协议SMTP、POP3、IMAP DHCP协议 最重要的http协议单独写一篇博文啦 使用TCP和UDP的各种常见应用层协议 域名系统DNSDNS 是一个通过分层服务器实现的分布式数据库，提供了主机名和 IP 地址之间相互转换的服务。这里的分布式数据库是指，每个站点只保留与它相关的那部分数据。 DNS可以使用UDP或者TCP进行传输，使用的端口号都为 53。大多数情况下DNS使用UDP进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传从而保证可靠性。 在两种情况下会使用TCP进行传输： 如果返回的响应超过的 512 字节（UDP 最大只支持512字节的数据）。 区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）。 DNS工作流程如下：本地域名服务器采用递归查询 需要注意两点： 主机向本地域名服务器进行查询一般是采用递归查询，递归查询就是说如果主机所询问的本地域名服务器不知道被查询域名的IP地址，那么本地域名服务器就以DNS客户的身份向其他根域名服务器发送查询请求报文(即替主机进行查询)，而不是让该主机自己进行下一步查询，因此递归查询返回的结果要么是所查询的IP地址，要么是报错，表示没有对应的IP地址。 本地域名服务器向根域名服务器进行查询通常是进行迭代查询(即上图a，当然也可以像图b那样使用递归查询) 迭代查询就是当根域名服务器收到本地域名服务器发送的请求报文后，要么给出所要查询的IP，要么告诉本地域名服务器向哪个域名服务器进行查询(而不是替本地域名服务器进行后续查询)，通常是告诉本地域名服务器自己知道的顶级域名服务器的IP地址，本地域名服务器向顶级域名服务器进行查询，顶级域名服务器要么告诉IP地址，要么告诉本地域名服务器向哪个根域名服务器进行查询，最后查到IP后，将结果返回个主机。 文件传送协议 FTP文件传送协议 FTP (File Transfer Protocol) 是互联网上使用得最广泛的文件传送协议。 FTP 提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限 文件传输过程 打开熟知端口（端口号为 21），使客户进程能够连接上。 等待客户进程发出连接请求。 启动从属进程来处理客户进程发来的请求。从属进程对客户进程的请求处理完毕后即终止，但从属进程在运行期间根据需要还可能创建其他一些子进程。 回到等待状态，继续接受其他客户进程发来的请求。主进程与从属进程的处理是并发地进行。 当客户进程向服务器进程发出建立连接请求时，要寻找连接服务器进程的熟知端口 (21)，同时还要告诉服务器进程自己的另一个端口号码，用于建立数据传送连接。 服务器进程用自己传送数据的熟知端口 (20) 与客户进程所提供的端口号码建立数据传送连接。 由于 FTP 使用了两个不同的端口号，所以数据连接与控制连接不会发生混乱。 FTP是使用了两个TCP连接的。好处： 使协议更加简单和更容易实现。 在传输文件时还可以利用控制连接（例如，客户发送请求终止传输）。 电子邮件协议SMTP、POP3、IMAP 注意： 不要将邮件读取协议 POP 或 IMAP 与邮件传送协议 SMTP 弄混。 发信人的用户代理向源邮件服务器发送邮件，以及源邮件服务器向目的邮件服务器发送邮件，都是使用 SMTP 协议。 而 POP 协议或 IMAP 协议则是用户从目的邮件服务器上读取邮件所使用的协议。 DHCP协议为了将软件协议做成通用的和便于移植，协议软件的编写者把协议软件参数化。在软件协议运行之前，必须给每一个参数赋值。在协议软件中给这些参数赋值的动作叫做协议配置。 互联网广泛使用的动态主机配置协议 DHCP (Dynamic Host Configuration Protocol) 提供了即插即用连网(plug-and-play networking) 的机制。 并不是每个网络上都有 DHCP 服务器，这样会使 DHCP 服务器的数量太多。现在是每一个网络至少有一个 DHCP 中继代理，它配置了 DHCP 服务器的 IP 地址信息。 注意： DHCP报文仅为UDP数据报的数据部分 DHCP 服务器分配给 DHCP 客户的 IP 地址的临时的，因此 DHCP 客户只能在一段有限的时间内使用这个分配到的IP 地址。","link":"/2020/03/10/%E5%BA%94%E7%94%A8%E5%B1%82/"},{"title":"传输层","text":"主要内容 传输层概念、端口号、套接字 UDP传输特点和数据报格式 TCP传输特点和数据报格式 可靠传输原理 TCP可靠运输的实现 TCP流量控制、拥塞控制 三次握手和四次握手抓包分析 传输层运输层向它上面应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最底层。 两个主机进行通信实际上就是两个主机中的应用进程互相通信。应用进程之间的通信又称为端到端的通信。 传输层有两种不同的协议：用户数据报协议UDP和传输控制协议TCP 学习这两个重要协议之前，先简单了解下端口的概念。 端口端口(16位)用来对 TCP/IP 体系的应用进程进行区分。 端口号只具有本地意义，即端口号只是为了标志本计算机应用层中的各进程。在因特网中不同计算机的相同端口号是没有联系的。 端口号分为两类： 服务端使用的端口号服务器端使用的端口又分为两类：熟知端口和登记端口。熟知端口数值一般为0~1023(比如我们常见的http使用的就是80端口)。登记端口数值为1024 ~ 49151，这些端口是为没有熟知端口号的应用程序使用的。使用这个范围的端口号必须在 IANA 登记，以防止重复。 服务器端常见端口号： 客户端使用的端口号数值为49152~65535，留给客户进程选择暂时使用，仅在客户进程运行时才动态选择，因此又叫短暂端口号。当服务器进程收到客户进程的报文时，就知道了客户进程所使用的动态端口号。通信结束后，这个端口号可供其他客户进程以后使用。 套接字套接字针对的是TCP连接，每一条TCP连接有两个端点，那两个端点称为套接字（socket） 端口号拼接到IP地址后面构成套接字。套接字socket=（IP地址：端口号） 所以每一条TCP连接唯一地被通信两端的两个端点（两个套接字）所确定。即：TCP连接::={socket1,socket2}={(IP1:port1),(IP2:port2)} 用户数据报协议UDPUDP 只在IP的数据报服务之上增加了很少一点的功能，即端口的功能和差错检测的功能。 UDP特点 无连接 (即发送数据之前不需要建立连接) 不可靠 (UDP使用尽最大努力交付，即不保证可靠交付，同时也不使用拥塞控制) 面向报文 (对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部) 支持一对一、一对多、多对一、多对多的交互通信 UDP没有拥塞机制(网络出现拥塞不会使源主机发送速率降低，对于一些实时应用如视频会议等十分合适) UDP首部开销小 只有8个字节，比TCP20个字节要短。 UDP首部 首部字段只有 8 个字节，包括：源端口 源端口号，在需要对方回信时调用目的端口 目的端口号 在重点交付报文时使用长度：UDP用户数据报的长度，其最小值是8（仅有首部）检验和 检测UDP用户数据报在传输中是否有错，有错就丢弃。 注意： 虽然在UDP之间的通信要用到其端口号，但由于UDP通信是无连接的，因此不需要使用套接字。 在计算检验和时，要在UDP用户数据报之前添加12个字节的“伪首部”。这个伪首部既不向下传递也不向上递交。仅仅是为了计算检验和。 UDP在计算检验和时是把首部和数据部分一起检验，而IP数据报首部检验和只检验IP数据报首部，不检验数据部分 传输控制协议TCPTCP除了基本的数据交付和差错检查外，还提供了可靠数据传输和拥塞控制服务 TCP特点 面向连接 (应用程序使用TCP协议前必须先建立TCP连接) 只能是点对点的（一对一） (每一条TCP连接只能有两个端点) 提供可靠交付 (TCP连接传送的数据无差错、不丢失、不重复，按序到达) 全双工通信 允许通信双方的应用进程再任何时候都能发送数据。 面向字节流 （把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块） TCP首部 源端口和目的端口：各占2个字节，分别写入源端口号和目的端口号。 序号 ：占4个字节。序号范围是[0,2^32 - 1],用于对字节流进行编号。例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 ：占4个字节，是期望收到对方下一个报文段的第一个数据字节的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。若确认号=N，则表明：到序号N-1为止的所有数据都已正确收到 数据偏移 ：占4位，指的是TCP报文段数据起始处距离报文段起始处的偏移量，实际上指的是TCP报文段的首部长度。 保留：占6位，保留为今后使用，但目前应置为0. 紧急URG:当URG=1时，表明紧急指针字段有效。告诉系统此报文段中有紧急数据，应尽快传送，而不要按原来的排队顺序来传送。 确认 ACK ：仅当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 推送PSH：发送方把PSH置1，并立即创建一个报文段发送出去，接收方TCP收到PSH=1的报文段，就尽快地交付接收应用进程，而不用等到整个缓存都填满了后再向上交付。很少使用 复位RST：当RST=1时，表明TCP连接中出现严重差错，必须释放连接，然后再重新建立运输连接。 同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 ：占2个字节。值是[0,2^16 - 1]之间的整数。窗口指的是发送本报文段一方的接收窗口。窗口值告诉对方：从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量（以字节为单位）。窗口字段明确指出了现在允许对方发送的数据量。窗口值经常在动态变化。 检验和：占2个字节。检验和字段检验的范围包括首部和数据这两部分。和UDP一样，在计算检验和时，要在TCP报文段的前面加上12字节的伪首部。 紧急指针：占2个字节。紧急指针仅在URG=1时才有意义，它指出本报文段中的紧急数据的字节数。 选项： 长度可变，最长可达40字节。当没有使用选项是，TCP首部为20字节。 可靠传输原理理想的传输条件有以下两个特点： 传输信道不产生差错。 不管发送方以多快的速度发送数据，接收方总是来得及处理收到的数据。然而实际的网络都不具备以上两个理想条件。必须使用一些可靠传输协议，在不可靠的传输信道实现可靠传输 那么在TCP中就有两种方式来实现可靠传输了 停止等待协议 连续 ARQ 协议 停止等待协议“停止等待”就是每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。 停止等待协议有两种情况： 无差错情况如下图a，A发送分组M1，发送完就暂停，等待B的确认。B收到了M1就向A发送确认。A收到了对M1的确认后就再发送下一个分组M2。之后收到确认后再发送M3 出现差错可能M1在传输过程中直接丢失，B未收到分组则什么也不做或者B收到分组后检测出了差错，就丢弃M1并且也什么都不做。 对于上面分组丢失的问题，可靠传输协议的解决： A 为每一个已发送的分组都设置了一个超时计时器。 A 只要在超时计时器到期之前收到了相应的确认，就撤销该超时计时器，继续发送下一个分组 M2 。 注意： 在发送完一个分组后，必须暂时保留已发送的分组的副本，以备重发。 分组和确认分组都必须进行编号。 超时计时器的重传时间应当比数据在分组传输的平均往返时间更长一些。 像上述的这种可靠传输协议常称为自动重传请求 ARQ (Automatic Repeat reQuest)。即重传的请求是自动进行的，接收方不需要请求发送方重传某个出错的分组。 为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输。 流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停顿下来等待对方的确认。这样可使信道上一直有数据不间断地传送。 连续ARQ协议滑动窗口协议是TCP协议的精髓，现在简单介绍下 发送方维持的发送窗口，它的意义是：位于发送窗口内的分组都可连续发送出去，而不需要等待对方的确认。这样，信道利用率就提高了。 连续 ARQ 协议规定，发送方每收到一个确认，就把发送窗口向前滑动一个分组的位置。 即不必对收到的分组逐个发送确认，而是对按序到达的最后一个分组发送确认，这样就表示：到这个分组为止的所有分组都已正确收到了。 优点：容易实现，即使确认丢失也不必重传。缺点：不能向发送方反映出接收方已经正确收到的所有分组的信息。 如果发送方发送了前 5 个分组，而中间的第 3 个分组丢失了。这时接收方只能对前两个分组发出确认。发送方无法知道后面三个分组的下落，而只好把后面的三个分组都再重传一次。这就叫做 Go-back-N（回退 N），表示需要再退回来重传已发送过的 N 个分组。 TCP可靠传输的实现上面我们已经了解了可靠传输的简单原理，现在我们看先TCP是如何具体实现的。 TCP 连接的每一端都必须设有一个发送窗口和一个接收窗口。两端的四个窗口经常处于动态变化之中。 TCP 的可靠传输机制用字节为单位使用字节序号进行控制。TCP 所有的确认都是基于序号而不是基于报文段。 TCP超时重传的时间选择不是固定不变的，而是自适应的（使用特定的算法估算较为合理的重传时间）。 TCP的流量控制 TCP流量控制通过滑动窗口实现，就是让发送方的发送速率不要太快，要让接收方来得及接收。 发送方的发送窗口不能超过接收方给出的接收窗口的数值。 如果B向A发送了零窗口的报文段后不久，B的接收缓存又有了一些存储空间，B向A发送非零窗口报文段，但是这个报文段丢失了，那么就会造成A一直等待收到B发送的非零窗口的通知，而B也一直等待A发送的数据，如果没有其他措施，这种互相等待的死锁局面会一直延续下去。 为了解决上述问题，TCP为每个连接接设有一个持续计时器，只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。 若持续计时器设置的时间到期，就发送一个零窗口探测报文段。对方在确认这个探测报文段时给出现在窗口值，如果窗口仍然是零，那么收到这个报文段的一方就重新设置持续计时器。如果窗口不是零就，就可以打破死锁的僵局。 TCP的拥塞控制拥塞控制原理出现拥塞的条件：对资源需求 &gt; 可用资源 即使增大资源也不是能解决拥塞的问题的。不但不能解决拥塞问题，而且还可能使网络的性能更坏。 拥塞引起的重传并不会缓解网络的拥塞，反而会加剧网络的拥塞。因为会引起更多的分组流入网络和被网络中的路由器丢弃。 拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载 拥塞控制和流量控制的区别 拥塞控制是全局性的，而流量控制是指点对点通信量的控制 拥塞控制是防止过多的数据注入到网路中，而流量控制是控制发送端发送数据的速率，以便接收端来得及接收。 拥塞控制的作用：注意横坐标代表的是单位时间内输入给网络的分组数目。 TCP拥塞控制方法有四种：慢开始、拥塞避免、快重传、快恢复。 拥塞的判断： 重传定时器超时 收到三个相同（重复）的 ACK（丢失个别报文段时，快重传对接收到的报文段的重复确认） 拥塞控制流程图：其中ssthresh为慢开始门限 TCP连接管理TCP连接建立(3次握手) 客户端A向服务器端B发出连接请求报文段，这时首部中的同步位SYN=1，同时选择一个初始序号seq=x。TCP规定，SYN报文段不能携带数据，但要消耗掉一个序号。这时，TCP客户进程进入SYN-SENT（同步已发送）状态。 B收到连接请求报文段后，如同意建立连接，则向A发送确认。在确认报文段中应把SYN位和确认ACK为都置1，确认号是ack=x+1，同时也为自己选择一个初始序号seq=y。这个报文段也不能携带数据，但同样要消耗一个序号。这时TCP服务器进程进入SYN-RCVD（同步收到）状态。 TCP客户进程收到B的确认后，还要向B给出确认。确认报文段的ACK置1.确认号ack=y+1，而自己的序号seq=x+1。TCP的标准规定，ACK报文段可以携带数据，但如果不携带数据则不消耗序号。这时，TCP连接已建立，A进入ESTABLISHEN（已建立连接）状态。 三次握手的理解第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常所以三次握手就能确认双发收发功能都正常，缺一不可。 理解清楚了之后,我们就知道为什么不能连接建立不能两次或者四次了 两次：如果没有第三次握手，服务器端无法知道自己发送和对方接收是否正常。例如，如果A发出的第一个连接请求报文没有丢失而是滞留在网络结点上，之后A再次发出请求与B建立连接，传送完数据后释放连接。这时A第一次发送的请求（失效报文）才到达B，B会误认为是A的又一个请求报文，就向A发送确认报文，同意建立连接。这时如果没有第三次握手，只要B发出确认，新的连接就直接建立，但是A没有发送建立连接请求，就会造成B一直等待A发送数据，造成B的资源浪费。 四次：没有必要，因为三次握手已经能够确保双方的收发功能正常。 抓包验证下吧第一次握手 第二次握手 第三次握手 TCP连接释放(4次挥手) 数据传输结束后，A和B处于ESTABLISHEN状态。A的应用进程先向其TCP发出连接释放报文段，并停止再发送数据，主动关闭TCP连接。A把连接释放报文段首部的FIN置1，其序号seq=u，等于前面已经传送过的数据的最后一个字节的加1.这时A进入FIN-WAIT-1（终止等待1）状态，等待B的确认。 B收到连接释放报文段后即发出确认，确认号是ack=u+1，而这个报文段自己的序号是v。等于B前面已经传送过的数据的最后一个字节的序号加1。然后B进入CLOSE-WAIT（关闭等待）状态。TCP服务器进程这时应通知高层应用进程，因而从A到B这个方向的连接就释放了，这时的TCP连接处于半关闭状态。即A已经没有数据要发送了，但B若发送数据，A仍要接受。A收到来自B的确认后，就进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。 若B已经没有要向A发送的数据，其应用进程就通知TCP释放链接。这时B发出的报文段必须使FIN=1。现假定B的序号为w（在半关闭状态B可能由发送了一些数据）。B还必须重复上次已经发送过的确认号ack=u+1。这时B就进入LAST-ACK（最后确认）状态，等待A的确认。 A在收到B的连接释放报文段后，必须对此发出确认。在确认报文段中把ACK置1，确认号ack=w+1，而自己的序号是seq=u+1。然后进入到TIME-WAIT（时间等待）状态。请注意，现在TCP连接还没有释放掉。必须经过时间等待计时器设置的时间2MSL（最长报文段寿命）后，A才进入到CLOSED状态。时间MSL叫做最长报文段寿命（Maximum Segment Lifetime）。 这里抓包就不仔细分析了 A 必须等待 2MSL 的时间： 为了保证 A 发送的最后一个 ACK 报文段能够到达 B。（因为A最后发送的ACK报文段有可能丢失，因而B收不到A发送的FIN+ACK报文段的确认。B会超时重传FIN+ACK报文段，而A就能在2MSL内收到这个重传的FIN+ACK报文段，接着A重传一次确认，重新启动2MSL计时器。最后正常释放连接。如果A不等待2MSL而是在发送完ACK确认报文后立即释放连接，就无法收到B重传的FIN+ACK报文，因而也就不会发送确认报文，这样B就无法进入CLOSED状态） 防止 “已失效的连接请求报文段”出现在本连接中。A 在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以使本连接持续的时间内所产生的所有报文段，都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。 此外，TCP还设有一个保活计时器。服务器每收到一次客户数据，就重新设置保活计时器(时间通常为2小时)。若两小时没有收到客户的数据，则服务器发送一个探测报文段，以后每隔75s发送一次。若一连发送10个探测报文段后仍无客户的响应，服务器就认为客户端出现了故障，接着就关闭这个连接。","link":"/2020/03/09/%E4%BC%A0%E8%BE%93%E5%B1%82/"},{"title":"HTTP和HTTPS","text":"主要内容 HTTPS = HTTP + SSL/TSL HTTPS之混合加密 HTTPS之数字证书 HTTPS之摘要算法 HTTPS连接建立过程 HTTP的不足以及HTTPS的改进我们之前提到HTTP协议是不安全的，主要原因在于： 窃听风险 通信使用明文（不加密），内容可能会被窃听。比如，账号信息容易泄漏。 冒充风险 不验证通信方的身份，因此有可能遭遇伪装。比如，访问假的淘宝、拼多多。 篡改风险 无法证明报文的完整性，所以有可能已遭篡改。比如，网页上植入垃圾广告。 HTTPS在HTTP和TCP层加入了SSL/TLS协议来解决上述风险。 SSL(Secure Socket Layer)指安全套接层，发展到SSL3.0之后IETF对SSL3.0进行了标准化并添加了少数机制，之后更名为TLS1.0(Transport Layer Security 安全传输层协议)。 HTTP+加密+认证+完整性保护=HTTPS HTTPS解决方式： 混合加密的方式实现信息的机密性，解决了窃听的风险。 （加密） 将服务器公钥放入到数字证书中，解决了冒充的风险。 （认证） 摘要算法的方式来实现完整性，摘要算法用于校验数据的完整性，解决了篡改的风险。（完整性保护） 混合加密算法对称密钥加密对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。优点：运算速度快；缺点：无法安全地将密钥传输给通信方。 非对称密钥加密(公开密钥加密)非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。 公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。 非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。 优点：可以更安全地将公开密钥传输给通信发送方；缺点：运算速度慢。 混合加密HTTPS结合了公开密钥安全和对称密钥运算速度快的优点，使用混合加密方式。 在通信建立前采用 非对称加密 的方式交换会话秘钥，后续就不再使用非对称加密。 在通信过程中全部使用对称加密的会话秘钥的方式加密明文数据 数字证书我们从上面知道客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。这里存在一个问题，如何保证公钥不被篡改和信任度？即无法证明公开密钥本身就是货真价实的公开密钥。 所以这里就需要借助第三方权威机构 CA （数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。 摘要算法 这部分参考文章硬核！30 张图解 HTTP 常见的面试题 摘要算法用来实现完整性，能够为数据生成独一无二的「指纹」，用于校验数据的完整性，解决了篡改的风险。 客户端在发送明文之前会通过摘要算法算出明文的「指纹」，发送的时候把「指纹 + 明文」一同加密成密文后，发送给服务器，服务器解密后，用相同的摘要算法算出发送过来的明文，通过比较客户端携带的「指纹」和当前算出的「指纹」做比较，若「指纹」相同，说明数据是完整的。 HTTPS连接建立过程SSL/TLS 协议基本流程： 客户端向服务器索要并验证服务器的公钥。 双方协商生产「会话秘钥」。 双方采用「会话秘钥」进行加密通信。 前两步也就是 SSL/TLS 的建立过程，也就是握手阶段。 SSL/TLS 的「握手阶段」涉及四次通信，可见下图： SSL/TLS 协议建立的详细流程： ClientHello 首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。 在这一步，客户端主要向服务器发送以下信息： （1）客户端支持的 SSL/TLS 协议版本，如 TLS 1.2 版本。 （2）客户端生产的随机数（Client Random），后面用于生产「会话秘钥」。 （3）客户端支持的密码套件列表，如 RSA 加密算法。 SeverHello 服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello。服务器回应的内容有如下内容： （1）确认 SSL/ TLS 协议版本，如果浏览器不支持，则关闭加密通信。 （2）服务器生产的随机数（Server Random），后面用于生产「会话秘钥」。 （3）确认的密码套件列表，如 RSA 加密算法。 （4）服务器的数字证书。 3.客户端回应 客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。 如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息： （1）一个随机数（pre-master key）。该随机数会被服务器公钥加密。 （2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。 （3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。 上面第一项的随机数是整个握手阶段的第三个随机数，这样服务器和客户端就同时有三个随机数，接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。 服务器的最后回应 服务器收到客户端的第三个随机数（pre-master key）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后，向客户端发生最后的信息： （1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。 （2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。 至此，整个 SSL/TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。","link":"/2020/03/10/HTTP%E5%92%8CHTTPS/"},{"title":"深入理解JVM开篇","text":"深入理解Java虚拟机搞起来，哇咔咔!!! 理解JVM的重要性就不用我多说了。理解JVM不管是面试还是对Java的进一步理解都是十分重要的。 之前自己看过一遍《深入理解Java虚拟机》之后，看的自己怀疑人生，所以最近有时间再来读一遍，顺便做一些总结。 JVM体系结构整个JVM的体系结构如下图所示。 上面这张图比较关键，之后几篇文章，我们也主要从上面这张图入手，再去读一遍《深入理解Java虚拟机》这本书。","link":"/2020/03/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E5%BC%80%E7%AF%87/"},{"title":"深入理解JVM之Java堆垃圾回收过程","text":"主要内容 Java堆内存结构 Java堆内存回收过程 GC日志分析 这个本来想上篇文章写的，但是考虑到上篇文章篇幅过长，所以就重新开了一章，上篇文章可以说是我们理解Java垃圾回收过程需要的基本知识，这篇文章才是Java垃圾回收过程的核心。所以没看过上篇文章的可以先去看一下我的上一篇文章 四大垃圾收集算法和七大垃圾收集器 Java堆内存结构为了更清楚的了解Java堆内存垃圾回收过程，我们首先先看下Java堆内存结构。 Java堆内存结构在逻辑上分为新生区、老年区和永久区。其中永久区又分为Eden区、Survivor 0 区和Survivor 1区如下图所示： 对于上面这张图需要注意： Java堆逻辑上分为了新生区、老年区和永久区，但物理上只有新生区和老年区 Java8之后已经将原来的永久区更换为了元空间 Java堆内存回收过程1. 对象在新生代Eden区分配 大多数情况下，我们的对象都是优先分配的Eden区，大对象直接进入老年区。 大对象指的是需要大量连续内存空间的对象，最典型的大对象就是那种很长的字符串和数组。 JVM中通过参数-XX:PretenureSizeThreshold设定，大于这个值的对象为大对象，直接在老年代分配 2. 当Eden区满时，虚拟机发起一次Minor GC Minor GC是使用的复制算法，上篇文章中我们特意提到过这段话 HotSpot 虚拟机的 Eden 和 Survivor0 和 Survivor1大小比例默认为 8:1:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 就不够用了，此时需要依赖于老年代进行空间分配担保，也就是借用老年代的空间存储放不下的对象。 如下图所示，Java堆中新生代占了1/3空间，老年代占了2/3空间，并且新生代中Eden区：From区：To区是8:1:1 Minor GC的具体过程： 1. Eden、SurvivorFrom 复制到 SurvivorTo，年龄+1首先，当Eden区满的时候会触发第一次GC,把还活着的对象拷贝到Survivor From区，当Eden区又满时再次触发GC的时候会扫描Eden区和From区域,对这两个区域进行垃圾回收，经过这次回收后还存活的对象,则直接复制到To区域（如果有对象的年龄已经达到了老年的标准，则赋值到老年代区），同时把这些对象的年龄+1 这里注意： 经研究98%的对象都是朝生夕灭的，也就是说，基本上第一次GC就能把大部分垃圾进行回收。即大部分对象基本只在Eden区就被回收。 如果扫描Eden区和Survivor From区后有多于 10% 的对象存活，那么一块 Survivor To就不够用了，此时需要借用老年代的空间存储放不下的对象 2. 清空 Eden、SurvivorFrom 然后，清空Eden和Survivor From中的对象 3. SurvivorTo和SurvivorFrom互换最后，Survivor To和Survivor From互换，原SurvivorTo成为下一次GC时的SurvivorFrom区。部分对象会在From和To区域中复制来复制去,如此交换15次(由JVM参数MaxTenuringThreshold决定,这个参数默认是15),最终如果还是存活,就存入到老年代 3. 长期存活的对象进入老年代 从上面的了解中我们知道，进入老年代的对象有： 大对象直接进入老年代 Eden区和Survivor From区Minor GC后有多于 10% 的对象存活需要空间分配担保。 存活年龄超过15的进入到老年代(动态年龄判定:并不一定要超过15，只要Survivor中相同年龄所有对象大小总和大于Survivor一般空间，就直接进入老年代) 4. 老年代进行Full GC 触发条件： 老年代空间不足 Minor GC后进入老年代的平均大小大于老年代的可用内存。 Eden区和Survivor From区Minor GC后有多于 10% 的对象存活需要移动到老年代，且老年代可用内存不足。 调用System.gc()时系统建议执行Full GC，但是不一定执行 5. OOM heap Space异常 当老年代进行Full GC后，空间仍然不足，则抛出OOM异常。 上面的整个过程，可以简略的用下图表示：注意：Java8之后已经变成了元空间，而且元空间使用的是本机的物理内存。 Minor GC和Full GC的区别 Minor GC: 回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC:同时回收新生代和老年代（即对整个堆进行GC），它会导致 Stop The World（简称 STW）,造成性能开销非常大，应该避免。 GC日志分析首先，我们需要先了解下几个参数 Xms : 设置堆初始分配大小，默认为物理内存的1/64 Xmx: 堆最大分配内存，默认为物理内存的1/4 -XX:+PrintGCDetails : 输出详细的GC处理日志 为了产生GC过程，我们可以首先通过下面的Java程序，看下自己电脑默认的堆内存大小和最大堆内存。 123456789101112public class test { public static void main(String[] args){ //返回Java虚拟机堆能使用的最大内存量 Long maxMemory = Runtime.getRuntime().maxMemory(); //返回Java虚拟机中堆初始内存总量 long totalMemory = Runtime.getRuntime().totalMemory(); //格式化输出 System.out.println(\"MAX_MEMORY = \" + maxMemory+ \"字节\" + (maxMemory/1024/1024)+\"MB\"); System.out.println(\"TOTAL_MEMORY = \" + totalMemory+ \"字节\" + (totalMemory/1024/1024)+\"MB\"); }} 我的电脑内存为8G，可以得到如下数据,可以看到最大内存量约为8G的1/4，堆内存大小约为8G的1/64。 12MAX_MEMORY = 1883242496字节1796MBTOTAL_MEMORY = 128974848字节123MB 我们在IDEA的VM option下(或者在命令行下)设置VM参数-Xms1024m -Xmx1024m -XX:+PrintGCDetails将JVM初始堆内存和最大堆内存均设置为1024M,并打印GC日志。再次运行刚才的程序得到下图 可以看到和上一次相比，我们的JVM参数将初始堆内存和最大堆内存变为了981M(981.5M)，约为1024M。 并且，我们可以验证Java堆逻辑上分为新生代、养老代和元空间，实际上只有新生代和养老代(305664/1024+699392/1024=981.5M)。 接下来我们设置-Xms10m -Xmx10m -XX:+PrintGCDetails为10M，并通过下面的程序，在新生代中不断new对象，以查看GC日志 12345678public class test { public static void main(String[] args){ String str = \"Hello World\"; while(true){ str += str; } }} 输出入下图，可以看到经过GC和Full GC后最终产生了java.lang.OutOfMemoryError: Java heap space异常。 GC日志 贴一下我们上面运行的GC日志 1[GC (Allocation Failure) [PSYoungGen: 1965K-&gt;488K(2560K)] 1965K-&gt;768K(9728K), 0.0011237 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 参照下图就可以知道具体含义 Full GC日志 这里贴一下产生OOM前最后一次的Full GC日志 1[Full GC (Allocation Failure) [PSYoungGen: 0K-&gt;0K(2560K)] [ParOldGen: 4825K-&gt;4803K(7168K)] 4825K-&gt;4803K(9728K), [Metaspace: 3189K-&gt;3189K(1056768K)], 0.0057874 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 这里注意下已经从永久区变成了元空间，其他没变 Full GC日志规律：","link":"/2020/03/16/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E4%B9%8BJava%E5%A0%86%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E8%BF%87%E7%A8%8B/"},{"title":"深入理解JVM之运行时数据区域","text":"主要内容 JVM运行时数据区域 栈、堆、方法区的关系 还是把这张图放上来，今天我们主要学习下运行时的数据区域，也是JVM的核心 运行时数据区域本地方法栈 为本地方法服务。具体做法是Native Method Stack中登记native方法，在Execution Engine 执行时加载本地方法库。 会抛出StackOverflowError和OutOfMemoryError错误(Error) 这里提一下Native Interface本地方法接口，本地接口的作用是融合不同的编程语言为 Java 所用，它的初衷是融合 C/C++程序。比如我们在Thread类中的很多方法都是native方法。现在已经很少使用 程序计数器 一块较小的内存空间，可以看做是当前线程所执行的字节码的行号指示器。 字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环跳转、异常处理等功能都需要依赖这个计数器完成。 方法区 用于存储已经被虚拟机加载的类的结构信息(Class)、常量、静态常量、即时编译后的代码等数据。注意这里不包含实例变量，实例变量是存在堆内存中的 上面讲的是规范，在不同虚拟机中具体实现是不一样的，最典型的就是永久代和元空间。 从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。元空间存储类的元信息，静态变量和运行时常量池等放入堆中。 Java栈 栈中主要存储8种基本类型变量+引用类型变量+实例方法 每个 Java 方法在执行的同时会创建一个栈帧。栈帧用于存储局部变量表(基本类型+引用类型)、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 栈的大小和具体JVM的实现有关，通常在256K~756K之间,约等于1Mb左右。 会抛出StackOverflowError和OutOfMemoryError错误(Error) 堆 几乎所有实例对象都在这里分配内存，是垃圾收集的主要区域（”GC 堆”）。 堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 栈、堆、方法区关系 HotSpot是使用指针的方式来访问对象： Java堆中会存放访问类元数据(类模板信息)的地址， reference存储的就直接是对象的地址 运行时常量池和字符串常量池傻傻分不清 在JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代 在JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代 在JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)","link":"/2020/03/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA%E5%9F%9F/"},{"title":"深入理解JVM之类加载器","text":"主要内容 类加载机制概述 类加载的过程和时机 类加载器有几种 双亲委派机制 还是把这张图放上来，今天我们主要学习下类加载器和执行引擎。 类加载机制概述我们知道我们自己写的类讲过javac编译后成为.class文件，虚拟机通过类加载器把这些描述类的数据从Class文件加载到内存，之后对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 类是在运行期间第一次使用时动态加载的，而不是一次性加载所有类。 类加载的过程和时机类的整个生命周期： 类的加载过程包括：加载、连接、初始化 1. 加载 加载是类加载的一个阶段，注意不要混淆。加载过程完成以下三件事： 通过类的完全限定名称获取定义该类的二进制字节流。 将该字节流表示的静态存储结构转换为方法区的运行时存储结构。 在内存中生成一个代表该类的 Class 对象，作为方法区中该类各种数据的访问入口。 2. 验证 确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 3. 准备 为类变量(static关键字修饰)在方法去中分配内存 设置类变量初值(通常为0值) 4. 解析 将常量池的符号引用替换为直接引用的过程。 5. 初始化 真正开始执行类中定义的Java字节码。即根据我们的代码去初始化类变量和其他资源 类加载器3+1类加载器的作用是： 通过类的完全限定名称获取定义该类的二进制字节流。(所以类加载器只作用于上述过程中的加载阶段) 由类加载器和这个类本身一同确立这个类在虚拟机中的唯一性。 对于类加载器我们需要明确： 只是将class文件加载到jvm虚拟机中，之后的验证、准备、解析、初始化不归它管。 将该字节流表示的静态存储结构转换为方法区的运行时存储结构。 在内存中生成一个代表该类的Class对象（hotspot虚拟机就是将该Class对象放到方法区中），作为方法区中该类各种数据的访问入口。 通过class文件开头的特定标识(CAFE BABE)确定是否是class文件 类加载器分类 类加载器分为两类： **虚拟机自带的类加载器** - 启动类加载器(BootStrapClassLoader) - 扩展类加载器(ExtClassLoader) - 应用程序类加载器(AppClassLoader) 用户自定义类加载器Java.lang.ClassLoader的子类，用户可以定制类的加载方式 类加载器如下图所示: 启动类加载器 启动类加载器是java的顶级类加载器，加载$JAVA_HOME/jre/lib下面的核心类库。这也是为什么我们装上jdk后，就可以直接使用Object类、String类和我们的ArrayList类等。比如下图就是jre/lib下面的rt.jar包中的Object.class。 我们试下看是不是BootStrap类加载器： 1234567public class MyObject { public static void main(String[] args) { Object object = new Object(); System.out.println(object.getClass().getClassLoader()); System.out.println(object.getClass().getClassLoader().getParent()); }} 运行结果如下图所示输出的是null，是因为BootStrap类加载器它本身是虚拟机的一部分，所以它并不是一个JAVA类，也就是无法在java代码中获取它的引用。所以BootStrap类加载器在程序中获得的是null。 123nullException in thread \"main\" java.lang.NullPointerException at com.company.MultiThread.MyObject.main(MyObject.java:10) 扩展类加载器 用于加载$JAVA_HOME/jre/ext下的类库。因为Java设计之初，不可能想到之后会用在哪些方面，应该提供哪些基本类，所以才会有这个扩展类加载器。主要加载下面这些包，加载完后是javax.xxx下面的类。 应用程序类加载器 AppClassLoader应用类加载器,又称为系统类加载器,它负责加载用户类路径（ClassPath）上所指定的类库，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。我们平时写的类都是通过这个类加载器加载的。 12345678910111213public class MyObject { public static void main(String[] args) {// Object object = new Object();// System.out.println(object.getClass().getClassLoader());// System.out.println(object.getClass().getClassLoader().getParent()); //这里和上面的区别在于这个MyObject类是我们自己定义的 MyObject myObject = new MyObject(); System.out.println(myObject.getClass().getClassLoader().getParent().getParent()); System.out.println(myObject.getClass().getClassLoader().getParent()); System.out.println(myObject.getClass().getClassLoader()); }} 从下面的结果中我们就能够看到，首先是BootStrapClassLoader、然后是ExtClassLoader，最后才是我们的AppClassLoader 123nullsun.misc.Launcher$ExtClassLoader@1b6d3586sun.misc.Launcher$AppClassLoader@18b4aac2 自定义类加载器 步骤： 继承ClassLoader 重写findClass()方法 调用defineClass()方法 这里就不写了,贴篇文章Java类加载机制及自定义加载器 双亲委派机制 当一个类收到了类加载请求，他首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，每一个层次类加载器都是如此，因此所有的加载请求都应该传送到启动类加载其中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的Class），子类加载器才会尝试自己去加载。 即加载流程为： 当AppClassLoader加载一个class时，它首先不会自己去尝试载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。 当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。 如果BootStrapClassLoader加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），会使用ExtClassLoader来尝试加载； 若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。 我们通过public abstract class ClassLoader下面的loadClass方法的源码来看下是否是上面所说的那样。 1234567891011121314151617181920212223242526272829303132333435363738394041protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { //首先检查这个class是否已经加载过了 Class&lt;?&gt; c = findLoadedClass(name); //如果类没有加载过 if (c == null) { long t0 = System.nanoTime(); try { //如果有父类的加载器则让父类的加载器加载 if (parent != null) { c = parent.loadClass(name, false); } else { //如果父类的加载器为null说明已经递归到了BootStrapClassLoader //BootStrapClassLoader无法通过get获取 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found // from the non-null parent class loader } if (c == null) { //如果BootStrapClassLoader仍然没有加载过，则会回来，尝试自己加载class // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 我们写个小的demo验证下。比如我们就自己创建一个java.lang.String类 1234567package java.lang;public class String { public static void main(String[] args) { System.out.println(\"Hello\"); }} 运行结果如下图所示： 123错误: 在类 java.lang.String 中找不到 main 方法, 请将 main 方法定义为: public static void main(String[] args)否则 JavaFX 应用程序类必须扩展javafx.application.Application 可以看到程序报错，且提示我们在java.lang.String中找不到main方法，可是我们明明上面已经写了main方法。这就是因为双亲委派模型，加载String类时一直往上，找到BootStrapClassLoader后进行加载，原来java自定义的String类中一定不会有main方法，所以才会出现上述结果。 从上面的小demo中我们也就知道了双亲委派的作用： 双亲委派机制的作用 防止重复加载同一个.class。通过委托去向上面问一问，加载过了，就不用再加载一遍。保证数据安全。 保证核心.class不能被篡改。通过委托方式，不会去篡改核心.clas，即使篡改也不会去加载，即使加载也不会是同一个.class对象了。这样保证了Class执行安全。也称为沙箱安全机制","link":"/2020/03/15/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E4%B9%8B%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/"},{"title":"Java多线程之Thread类源码解析","text":"主要内容 Thread类常用API 线程的生命周期 中断interrupt 安全的停止一个线程 Thread类常用API 最常用的两个方法 12Thread.currentThread().getName() //得到当前线程名Thread.sleep(long mills) //线程睡眠 其他的建议看一下Thread类的注释和各个方法的源码，都挺简单的，直接上结论，就不贴图了。 线程能被标记为守护线程，也可以是用户线程setDaemon(boolean on) 每个线程均分配一个name，默认为（Thread-自增数字）的组合set和get方法 每个线程都有优先级.高优先级线程优先于低优先级线程执行. 1-10，默认为5set和get方法 main所在的线程组为main，构造线程的时候没有现实的指定线程组，线程组默认和父线程一样 当线程中的run()方法代码里面又创建了一个新的线程对象时,新创建的线程优先级和父线程优先级一样. 当且仅当父线程为守护线程时,新创建的线程才会是守护线程. 当JVM启动时,通常会有唯一的一个非守护线程(这一线程用于调用指定类的main()方法) 这里提一下守护线程 Daemon 守护线程是程序运行时在后台为其他线程提供服务的线程，不属于程序中不可或缺的部分。（比如JVM的垃圾回收线程就是守护线程） 当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。 main() 属于非守护线程。 在线程启动之前使用 setDaemon(boolean on) 方法可以将一个线程设置为守护线程。 123456789101112131415161718public class DaemonDemo { public static void main(String[] args) { Thread t1 = new Thread(()-&gt;{ try { //让main线程先执行 Thread.sleep(1000); System.out.println(\"t1 run\"); } catch (InterruptedException e) { e.printStackTrace(); } },\"t1\"); t1.setDaemon(true); t1.start(); System.out.println(\"main run\"); }} 运行结果只有main线程执行,说明main线程执行完后，已经没有非守护线程，这时程序终止，也就不会输出守护线程执行。 线程的生命周期在Thread类源码中的State枚举中有以下六种状态，下面我们分别解释下： 12345678public enum State { NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED; } 新建(NEW) 线程刚创建, 尚未启动 可运行(RUNNABLE) 正在 Java 虚拟机中运行。但是在操作系统层面，它可能处于运行状态，也可能等待资源调度（例如处理器资源），资源调度完成就进入运行状态。所以该状态的可运行是指可以被运行，具体有没有运行要看底层操作系统的资源调度。 阻塞(BLOCKED) 请求获取(监视器锁)从而进入synchronized 函数或者代码块，但是其它线程已经占用了该监视器锁，所以处于阻塞状态。要结束该状态进入从而RUNABLE需要其他线程释放监视器锁。 无限期等待(WAITING) 等待其它线程显式地唤醒。 进入方法 退出方法 没有设置 Timeout 参数的 Object.wait() 方法 Object.notify() / Object.notifyAll() 没有设置 Timeout 参数的 Thread.join() 方法 被调用的线程执行完毕 LockSupport.park() 方法 LockSupport.unpark(Thread) 这里要区分 BLOCKED 和 WATING 的区别。阻塞是被动的，它是在等待获取 monitor lock。而等待是主动的，通过调用 Object.wait() 等方法进入。 限期等待(TIMED_WAITING) 无需等待其它线程显式地唤醒，在一定时间之后会被系统自动唤醒。 进入方法 退出方法 Thread.sleep() 方法 时间结束 设置了 Timeout 参数的 Object.wait() 方法 时间结束 / Object.notify() / Object.notifyAll() 设置了 Timeout 参数的 Thread.join() 方法 时间结束 / 被调用的线程执行完毕 LockSupport.parkNanos() 方法 LockSupport.unpark(Thread) LockSupport.parkUntil() 方法 LockSupport.unpark(Thread) 调用 Thread.sleep() 方法使线程进入限期等待状态时，常常用“使一个线程睡眠”进行描述。调用 Object.wait() 方法使线程进入限期等待或者无限期等待时，常常用“挂起一个线程”进行描述。睡眠和挂起是用来描述行为，而阻塞和等待用来描述状态。 死亡(TERMINATED) 可以是线程结束任务之后自己结束，或者产生了异常而结束。 所以我们可以画出线程的转换的整个流程图: 注意：这里为了方便解释，将Runnable分成了可运行和运行中。 我们看一下上面用到的几个方法的源码 yield()方法 1public static native void yield(); 因为是native方法，无法看源码。但从注释中我们知道 给调度器一个提示，当前线程愿意让出自己的用的处理器，但是调度器也可以忽略。也就是说使用yield()方法不一定能让出处理器。 因为不一定能让出处理器，所以yield()方法很少使用 sleep()方法 sleep方法有个重载方法，可以设置纳秒数。但两个方法都是直接调用了下面的本地方法。 1public static native void sleep(long millis) throws InterruptedException; 注意：sleep方法会释放cpu的时间片，但是不会释放锁 join()方法 在线程中调用另一个线程的 join() 方法，会将当前线程挂起(底层调用的是Object的wait()方法)。 join方法一共有三个重载方法： 123456//不带时间，实际调用的是第2个方法，只不过参数设置为0public final void join() throws InterruptedException//带时间参数public final synchronized void join(long millis) throws InterruptedException//带时间参数，只不过可以设置纳秒值和2基本相同public final synchronized void join(long millis, int nanos) throws InterruptedException 所以我们看下的源代码： 12345678910111213141516171819202122232425public final synchronized void join(long millis) throws InterruptedException { long base = System.currentTimeMillis(); long now = 0; //如果等待时间&lt;0,则抛出异常 if (millis &lt; 0) { throw new IllegalArgumentException(\"timeout value is negative\"); } //如果等待时间=0,则一直等待，直到线程死亡 if (millis == 0) { while (isAlive()) { wait(0); } } else {//否则等待设定时间结束 while (isAlive()) { long delay = millis - now; if (delay &lt;= 0) { break; } wait(delay);//调用Object的wait方法,让当前线程等待 now = System.currentTimeMillis() - base; } } } demo 1234567891011121314151617181920212223public class T { private int sum; public void add(){ for (int i = 0; i &lt; 1000; i++) { sum++; } } public static void main(String[] args) { T t = new T(); Thread t1 = new Thread(()-&gt;t.add(),\"t1\"); t1.start(); try { t1.join(); } catch (InterruptedException e) { e.printStackTrace(); } //没有join方法让t1线程执行完的话，经常会出现0 System.out.println(t.sum); }} 中断关于中断，Thread类中一共有四个方法： 1234public void interrupt();public static boolean interrupted();public boolean isInterrupted();private native boolean isInterrupted(boolean ClearInterrupted); 接下来我们根据源码分析一下这几个方法。 interrupt()方法 通过调用一个线程的 interrupt() 来中断该线程，注意这里的中断不会真正停止一个线程，而仅仅是设置了一个中断标志(中断状态设为true)。 只能该线程自身调用，否则可能会抛出SecurityException异常。 如果该线程处于阻塞、限期等待或者无限期等待状态，调用interrupt()方法，中断状态会被清除(interrupt()方法会直接将其标记为true,但由于处于阻塞状态，会立即将true改为false即将中断状态清除)，并且抛出InterruptedException异常，从而提前结束该线程。 不能中断 I/O 阻塞和 synchronized 锁阻塞。 源码如下所示 123456789101112131415public void interrupt() { //检查是否有权限 if (this != Thread.currentThread()) checkAccess(); //IO阻塞 synchronized (blockerLock) { Interruptible b = blocker; if (b != null) { interrupt0(); //本地方法，设置中断标志 b.interrupt(this); return; } } interrupt0(); //本地方法，设置中断标志 } 使用中断方式终止处于阻塞、无限期等待、限期等待的状态 123456789101112131415161718192021222324252627public class InterruptDemo { public static void main(String[] args) { Thread t1 = new Thread(()-&gt;{ try { Thread.sleep(500); System.out.println(Thread.currentThread().getName()+\"run\"); } catch (InterruptedException e) { e.printStackTrace(); } },\"t1\"); t1.start(); //调用interrupt方法直接抛异常 t1.interrupt(); //让t1线程先执行 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(t1.isInterrupted()); System.out.println(\"main run\"); }} 调用interrupt()方法直接终止上述状态，抛出InterruptException异常，同时抛出InterruptException异常后会将标志位置为false。 123456java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.company.MultiThread.MyRunnable.run(MyRunnable.java:13) at java.lang.Thread.run(Thread.java:748)falsemain run interrupted()方法和isInterrupted()方法 interrupted()方法（静态方法） 检测中断并清除中断状态 注意interrupted()方法作用于当前线程而不是调用interrupted方法的线程 isInterrupted()方法 检测中断不清除中断状态 注意interrupted()方法作用于调用此方法的实例的线程 interrupted()方法调用的是带有boolean参数的本地isInterrupted(boolean ClearInterrupted)方法。 123public static boolean interrupted() { return currentThread().isInterrupted(true); } isInterrupted方法同样调用的是带有boolean参数的本地isInterrupted(boolean ClearInterrupted)方法。 1234//isInterrupted()public boolean isInterrupted() { return isInterrupted(false); } 12//isInterrupted(boolean ClearInterrupted)方法private native boolean isInterrupted(boolean ClearInterrupted); 测试这两个方法作用对象的就不写了，贴一篇文章好了Thread类中interrupt（）、interrupted（）和isInterrupted（）方法详解 其实很容易理解，因为interrupted调用时是使用静态方法调用，所以是作用于当前线程，而isInterrupted方法通过对象调用，所以作用于调用此方法的对象。 安全的终止一个线程对于一个线程，可以有暂停，恢复和终止操作，对应的就是Thread类中的suspend()、rusume()和stop()方法。但是看源码知道这些方法因为某些原因(容易死锁、不能正常释放资源等)已经不建议使用了。 对于线程的暂停和恢复，可以使用等待唤醒机制来替代(之后再补上链接) 对于线程的终止，我们有两种方法： 使用上面提到的中断 使用一个boolean变量显式的控制(推荐) 比如一个线程的 run() 方法执行一个无限循环，并且没有执行 sleep() 等会抛出 InterruptedException 的操作，如何中断一个线程。 使用中断的demo 12345678910111213public class InterruptDemo { public void m1(){ while(!Thread.currentThread().isInterrupted()){ System.out.println(Thread.currentThread().getName()+\"--run\"); } } public static void main(String[] args) { Thread t1 = new Thread(()-&gt;new InterruptDemo().m1(),\"t1\"); t1.start(); //不设置t1.interrupt()会一直循环，设置了之后直接跳出循环。 //t1.interrupt(); }} 使用boolean变量的demo 1234567891011121314151617181920212223public class FlagStop { //标志位flag private volatile boolean flag; public void m1(){ while(!flag){ System.out.println(\"死循环中\"); } System.out.println(\"跳出了循环\"); } public void setFlag(boolean flag) { this.flag = flag; } public static void main(String[] args) { FlagStop flagStop = new FlagStop(); new Thread(()-&gt;flagStop.m1(),\"t1\").start(); //不设置会一直死循环，设置后跳出死循环 //flagStop.setFlag(true); }}","link":"/2020/03/12/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8BThread%E7%B1%BB%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"Java多线程之synchronized锁和显式Lock锁","text":"主要内容 synchronized锁使用 synchronized锁的底层原理 synchronized锁升级过程 volatile关键字 显式Lock锁 synchronized和显式Lock的区别 在多线程编程中，一个非常重要的问题是如何保证线程的安全性问题。非线程安全会在多个线程对同一个对象中的实例变量进行并发访问时发生，产生的问题就是”脏读”，即取到的数据其实是被更改过的。 比如下面这个例子: 123456789101112131415public class MyRunnable implements Runnable { private int count = 0; @Override public void run() { count++; //只是为了显示线程不安全 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\"---\"+count); }} 使用两个线程同时操作实例变量count 123456789public class test { public static void main(String[] args) { MyRunnable myRunnable = new MyRunnable(); Thread t1 = new Thread(myRunnable,\"t1\"); Thread t2 = new Thread(myRunnable,\"t2\"); t1.start(); t2.start(); }} 结果如下，理论上应该是一个是1一个是2。但实际上运行的结果两个都是2。 12t1---2t2---2 为了解决上面的问题，Java 提供了两种锁机制来控制多个线程对共享资源的互斥访问 JVM实现的synchronized关键字 JDK1.5之后加入的显式Lock锁 synchronized锁synchronized锁概述 synchronized可以用来修饰方法或者以同步块的形式使用，主要是确保多个线程在同一时刻只能有一个线程处于方法或者同步块中。 synchronized锁锁的是对象，不是代码块。 synchronized锁是互斥锁(一次只能允许一个线程拿到锁) synchronized保证了线程的原子性、可见性和有序性 这里我们先看下synchronized锁如何使用，再对原子性、可见性和有序性做一个解释。 如何使用synchronized锁 对于同步方法块，锁是synchronized的括号里配置的对象 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象 synchronized锁作用于同步方法块对于上面的例子，我们可以把需要同步的代码部分放入synchronized代码块中即可。 123456789101112public class Demo { private int count = 0; //private Object o = new Object(); public void m1() { //synchronized(o) synchronized (this){ count++; System.out.println(Thread.currentThread().getName()+\"---\"+count); } }} 上面synchronized括号中是this，即当前实例对象。但是不一定使用this。可以随便使用一个对象(因为每个对象都有一个监视器锁)。上面注释部分就是使用的一个object锁，但不推荐使用这种方法。 synchronized锁作用于普通同步方法 这里synchronized作用于普通方法increase(),实际上相当于同步代码块中使用this锁。 123456789public class Demo { private int count = 0; //等价于synchronized(this) private synchronized void increase(){ count++; System.out.println(Thread.currentThread().getName()+\"---\"+count); }} synchronized锁作用于静态同步方法如果上面的increase()方法是静态方法,这里就相当于同步代码块中使用当前类的对象，即MyRunnable.class锁 12345678public class Demo { private static int count = 0; //等价于synchronized(MyRunnable.class) private synchronized static void increase(){ count++; System.out.println(Thread.currentThread().getName()+\"---\"+count); }} 原子性、可见性和有序性 原子性：某一个操作是不可分割的。 可见性：当一个线程修改了共享变量的值，其他线程能够立即得知这个修改。 有序性: 程序按照我们代码的顺序进行顺序执行 原子性: 比如我们常见的count++就不是一个原子操作，而是需要经过三步：1. 获取count值 2. count值+1 3. +1后的值重新赋值给count。很多线程不安全都是因为某个操作不是原子性的，使数据混乱出错。 可见性: 对于可见性而言，Java虽然支持多个线程同时访问一个对象或者对象的一个成员变量，但实际上是每个线程拥有这个变量的一个拷贝，所以当某个线程修改了这个变量后，其他线程看到的这个变量并不一定是最新的，这就是不可见。可见性是指这个变量改变后，会同步刷新到共享内存，并告知其他线程访问该变量必须从共享内存中获取。后面要讲的volatile关键字也是可见的。 有序性:一般情况下，CPU和编译器为了提升程序执行的效率，会按照一定的规则允许对指令进行优化，即调整实际指令运行的顺序。指令重排不会对单线程的程序造成任何不利的影响，但是多线程环境下将会产生一些影响。指令重排的前提条件是指令调整后不会影响单线程程序的执行 synchronized保证了原子性、可见性和有序性也就是说使用synchronized之后，被保护的代码块不会被进行重排序，并且是一次被执行的，没有任何线程会同时访问，同时当执行完synchronized之后，修改后的变量对其他的线程是可见的 synchronized锁原理 在学习之前我们需要先复习下Java对象在JVM中的构成在JVM中，对象在内存中分为三块区域： 1.对象头 Mark Word（标记字段）：默认存储对象的HashCode，分代年龄和锁标志位信息。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。 Class Metadata Address：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 2.实例数据这部分主要是存放类的数据信息，父类的信息。 3.对其填充由于虚拟机要求对象起始地址必须是8字节的整数倍，填充数据不是必须存在的，仅仅是为了字节对齐。 Tip：不知道大家有没有被问过一个空对象占多少个字节？就是8个字节，是因为对齐填充的关系哈，不到8个字节对其填充会帮我们自动补齐。 可以简单的用下图表示: 对使用synchronized修饰方法和代码块的代码进行反编译 123456789101112131415package com.company.review;public class SynchronizedTest { //修饰方法 public synchronized void test1(){ System.out.println(\"test1\"); } public void test2(){ // 修饰代码块 synchronized (this){ System.out.println(\"test2\"); } }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108PS E:\\javaProject\\MultiThread\\src\\com\\company\\review&gt; javac -encoding utf-8 SynchronizedTest.javaPS E:\\javaProject\\MultiThread\\src\\com\\company\\review&gt; javap -p -v -c SynchronizedTest.classClassfile /E:/javaProject/MultiThread/src/com/company/review/SynchronizedTest.class Last modified 2020-7-13; size 592 bytes MD5 checksum 76c487ec7c36a8c7a8a7c43f41691610 Compiled from \"SynchronizedTest.java\"public class com.company.review.SynchronizedTest minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #7.#20 // java/lang/Object.\"&lt;init&gt;\":()V #2 = Fieldref #21.#22 // java/lang/System.out:Ljava/io/PrintStream; #3 = String #12 // test1 #4 = Methodref #23.#24 // java/io/PrintStream.println:(Ljava/lang/String;)V #5 = String #13 // test2 #6 = Class #25 // com/company/review/SynchronizedTest #7 = Class #26 // java/lang/Object #8 = Utf8 &lt;init&gt; #9 = Utf8 ()V #10 = Utf8 Code #11 = Utf8 LineNumberTable #12 = Utf8 test1 #13 = Utf8 test2 #14 = Utf8 StackMapTable #15 = Class #25 // com/company/review/SynchronizedTest #16 = Class #26 // java/lang/Object #17 = Class #27 // java/lang/Throwable #18 = Utf8 SourceFile #19 = Utf8 SynchronizedTest.java #20 = NameAndType #8:#9 // \"&lt;init&gt;\":()V #21 = Class #28 // java/lang/System #22 = NameAndType #29:#30 // out:Ljava/io/PrintStream; #23 = Class #31 // java/io/PrintStream #24 = NameAndType #32:#33 // println:(Ljava/lang/String;)V #25 = Utf8 com/company/review/SynchronizedTest #26 = Utf8 java/lang/Object #27 = Utf8 java/lang/Throwable #28 = Utf8 java/lang/System #29 = Utf8 out #30 = Utf8 Ljava/io/PrintStream; #31 = Utf8 java/io/PrintStream #32 = Utf8 println #33 = Utf8 (Ljava/lang/String;)V{ public com.company.review.SynchronizedTest(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: return LineNumberTable: line 6: 0 public synchronized void test1(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED //这里 Code: stack=2, locals=1, args_size=1 0: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #3 // String test1 5: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 9: 0 line 10: 8 public void test2(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter //这里 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #5 // String test2 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: aload_1 13: monitorexit //这里 14: goto 22 17: astore_2 18: aload_1 19: monitorexit //这里 20: aload_2 21: athrow 22: return Exception table: from to target type 4 14 17 any 17 20 17 any LineNumberTable: line 14: 0 line 15: 4 line 16: 12 line 17: 22 StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 17 locals = [ class com/company/review/SynchronizedTest, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4}SourceFile: \"SynchronizedTest.java\" 同步代码块大家在上面可以看到几处我标记的 上面提到过的对象头，他会关联到一个monitor对象（monitor对象在HotSpot虚拟机中它是由ObjectMonitor实现的（C++实现）） 当我们进入同步代码块的时候，执行monitorenter，就会获取当前对象的一个所有权，这个时候monitor进入数为1，当前的这个线程就是这个monitor的owner。 如果你已经是这个monitor的owner了，你再次进入，就会把进入数+1. 同理，当他执行完monitorexit，对应的进入数就-1，直到为0，才可以被其他线程持有。 所有的互斥，其实在这里，就是看你能否获得monitor的所有权，一旦你成为owner就是获得者。 同步方法 不知道大家注意到方法那的一个特殊标志位没，ACC_SYNCHRONIZED。 同步方法的时候，一旦执行到这个方法，就会先判断是否有标志位，如果ACC_SYNCHRONIZED标志被设置，那么线程在执行方法前会先去获取对象的monitor对象，如果获取成功则执行方法代码，执行完毕后释放monitor对象，如果monitor对象已经被其它线程获取，那么当前线程被阻塞。 另外为什么会有两个monitorexit呢？其实第二个monitorexit是来处理异常的，仔细看反编译的字节码，正常情况下第一个monitorexit之后会执行goto指令，而该指令转向的就是22行的return，也就是说正常情况下只会执行第一个monitorexit释放锁，然后返回。而如果在执行中发生了异常，第二个monitorexit就起作用了，它是由编译器自动生成的，在发生异常时处理异常然后释放掉锁。 JVM对synchronized的优化 从最近几个jdk版本中可以看出，Java的开发团队一直在对synchronized优化，其中最大的一次优化就是在jdk6的时候，新增了两个锁状态，通过锁消除、锁粗化、自旋锁等方法使用各种场景，给synchronized性能带来了很大的提升。 1.锁升级 下图是锁的升级方向： 接下来我们就谈一谈每一个锁升级状态，因为对象头中存储了锁的信息，这里就再贴一下对象头中存储的信息图片: 32位jvm中markword的信息（64位同理） 1.1 偏向锁 为什么引入偏向锁 因为经过HotSpot的作者大量的研究发现，大多数时候是不存在锁竞争的，常常是一个线程多次获得同一个锁，因此如果每次都要竞争锁会增大很多没有必要付出的代价，为了降低获取锁的代价，才引入的偏向锁。 偏向锁过程当锁对象第一次被线程获取的时候，虚拟机就会把对象头中的标志位设为“01”，即偏向锁模式。同时使用CAS把获得到的这个锁的线程ID记录在对象的Mark Word中。 如果CAS操作成功持有偏向锁的线程以后每次进入这个锁相关的同步块时，只需要检查Markword的锁标记位为偏向锁，以及当前线程ID是否为Markword中的ThreadId即可，不再进行任何同步操作。 1.2 轻量级锁 为什么引入轻量级锁? 轻量级锁考虑的是竞争锁对象的线程不多，而且线程持有锁的时间也不长的情景。因为阻塞线程需要CPU从用户态转到内核态，代价较大，如果刚刚阻塞不久这个锁就被释放了，那这个代价就有点得不偿失了，因此这个时候就干脆不阻塞这个线程，让它自旋这等待锁释放。 轻量级锁过程还是跟Mark Word 相关，如果这个对象是无锁的，jvm就会在当前线程的栈帧中建立一个叫锁记录（Lock Record）的空间，用来存储目前锁对象的Mark Word 拷贝，然后把Lock Record中的owner指向当前对象。 JVM接下来会利用CAS尝试把对象原本的Mark Word 更新为指向Lock Record的指针，成功就说明加锁成功，改变锁标志位为偏向锁，执行相关同步操作。 如果失败了，就会判断当前对象的Mark Word是否指向了当前线程的栈帧，是则表示当前的线程已经持有了这个对象的锁，可以直接进入同步块继续执行。否则说明有多个线程竞争锁。 轻量级锁升级为重量级锁若当前只有一个等待线程2，则该线程将通过自旋进行等待（自旋锁）。但是当自旋超过一定的次数时，轻量级锁便会升级为重量级锁。但是如果自旋次数过多(默认为10次)也不行，因为自旋是要消耗CPU的，因此自旋的次数是有限制的（默认为10次）如果自旋次数到了线程1还没有释放锁，或者又有线程3来获取锁，那么这个时候轻量级锁就会膨胀为重量级锁。重量级锁把除了拥有锁的线程都阻塞，防止CPU空转。 1.3 重量级锁 升级为重量级锁后，MarkWord中存储的就是指向重量级锁的指针，后面等待锁的线程也要进入阻塞状态。 下图为整个锁升级的过程: 3.锁粗化 按理来说，同步块的作用范围应该尽可能小，仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作数量尽可能缩小，缩短阻塞时间，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。 但是加锁解锁也需要消耗资源，如果存在一系列的连续加锁解锁操作，可能会导致不必要的性能损耗。 锁粗化是虚拟机通过扩大锁的范围，避免反复加锁和释放锁。比如下面method3经过锁粗化优化之后就和method4执行效率一样了。 4.锁消除 消除锁是虚拟机另外一种锁的优化，这种优化更彻底。Java虚拟机在JIT编译时，通过对运行上下文的扫描，经过逃逸分析，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间。比如下面代码的method1和method2的执行效率是一样的，因为object锁是私有变量，不存在所得竞争关系。 synchronized锁注意事项 上面几个只是synchronized的最基本使用，对于synchronized仍然有几点需要注意： 同步方法和非同步方法可以同时运行(只有在调用加锁的部分时，才需要看能否拿到锁，不需要锁的部分则是直接执行。容易产生脏读问题) synchronized是可重入锁,即同步方法中可以调用另一个同步方法 死锁问题 脏读问题只对写进行了加锁，没有对读进行加锁。可能在写过程中进行了读操作。解决也很好解决，就是对读使用和写相同的锁，即在读上也加上synchronized。 1234567891011121314151617181920public class Account { private String name; private double balance; public synchronized void set(String name,double balance){ this.name = name; //设定完name后，可能线程会执行getBalance方法 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } this.balance = balance; } //加上synchronized即可解决 public /*synchronized*/ double getBalace(String name){ return this.balance; }} 1234567891011121314151617181920212223public class test { public static void main(String[] args) { Account account = new Account(); new Thread(()-&gt;account.set(\"a\",100)).start(); //先让线程执行 try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(account.getBalace(\"a\")); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(account.getBalace(\"a\")); }} 两次结果不一致，产生脏读问题。 120.0100.0 可重入锁同步方法中可以调用另一个同步方法，也就是说一个线程已经拥有某个对象的锁，再次申请的时候仍然会得到该对象的锁（常见于子类继承父类，子类同步方法调用父类同步方法） 1234567891011121314151617181920public class T { public synchronized void m1(){ System.out.println(\"m1 start\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } m2(); } public synchronized void m2(){ System.out.println(\"m2\"); } public static void main(String[] args) { new Thread(()-&gt;new T().m1()).start(); }} 运行结果： 12m1 startm2 死锁问题两个或者多个线程互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行。 1234567891011121314151617181920212223242526272829303132333435363738394041package com.company.review;public class DeadLock { private Object A = new Object(); private Object B = new Object(); public static void main(String[] args) { DeadLock deadLock = new DeadLock(); new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"尝试拿锁A\"); synchronized (deadLock.A){ System.out.println(Thread.currentThread().getName()+\"拿到锁A\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\"尝试拿锁B\"); synchronized (deadLock.B){ System.out.println(Thread.currentThread().getName()+\"拿到锁B\"); } } },\"t1\").start(); new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"尝试拿锁B\"); synchronized (deadLock.B){ System.out.println(Thread.currentThread().getName()+\"拿到锁B\"); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\"尝试拿锁A\"); synchronized (deadLock.A){ System.out.println(Thread.currentThread().getName()+\"拿到锁A\"); } } },\"t2\").start(); }} 结果如下图所示： 当方法(代码块)执行完毕后会自动释放锁，不需要做任何的操作。 当一个线程执行的代码出现异常时，其所持有的锁会自动释放(要想不释放，需要用try-catch处理异常)。 释放锁的时机 当方法(代码块)执行完毕后会自动释放锁，不需要做任何的操作。 当一个线程执行的代码出现异常时，其所持有的锁会自动释放。 不会由于异常导致出现死锁现象~ volatile关键字之前我们提到过volatile关键字 volatile是一种轻量级的同步机制（效率比synchronized高很多） volatile仅仅用来保证该变量对所有线程的可见性和有序性，但不保证原子性 volatile底层原理 需要用到JMM的知识，这里先放两张图。 JMM中有如下规定： 所有的共享变量都存储于主内存。这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。 线程对变量的所有的操作(读，取)都必须在工作内存中完成，而不能直接读写主内存中的变量 正是因为有上述规定，所以当多个线程操作同一个变量时，实际上是每个线程将这个变量从主内存拷贝到线程独有的工作内存，所以当某个线程修改了这个变量后，其他线程看到的这个变量并不一定是最新的，这就是不可见。 volatile关键字的作用就在于当其他线程要使用这个变量时，通知其他线程从主内存中读取值，保证每次读取都是公共内存中的值。 验证volatile的可见性 我们之前通过标志位停止一个死循环就用到了volatile的可见性 123456789101112131415161718192021public class FlagStop { private volatile boolean flag; public void m1(){ while(!flag){ System.out.println(\"死循环中\"); } System.out.println(\"跳出了循环\"); } public void setFlag(boolean flag) { this.flag = flag; } public static void main(String[] args) { FlagStop flagStop = new FlagStop(); new Thread(()-&gt;flagStop.m1(),\"t1\").start(); flagStop.setFlag(true); }} 验证volatile没有原子性 还用之前的例子，只不过将共享变量设为volatile，会发现还是会有线程不安全问题 1234567891011121314151617181920212223242526272829303132333435363738package com.company.MultiThread;import java.util.ArrayList;import java.util.List;import java.util.concurrent.atomic.AtomicInteger;public class MyRunnable { //AtomicInteger count = new AtomicInteger(0); private volatile int count = 0; public void increase(){ for (int i = 0; i &lt; 1000; i++) { //count.incrementAndGet(); count++; } } public static void main(String[] args) { MyRunnable myRunnable = new MyRunnable(); List&lt;Thread&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { list.add(new Thread(()-&gt;myRunnable.increase(),\"Thread\"+i)); } list.forEach((o)-&gt;o.start()); list.forEach((o)-&gt;{ try { o.join(); } catch (InterruptedException e) { e.printStackTrace(); } }); System.out.println(myRunnable.count); }} 19787 多运行几次，可以看到每次结果不一样，偶尔才有可能正好是10000。出现问题的原因在于比如上面每个线程1加到100后由于可见性线程2得到的也是100这是没问题的，但是这个时候线程1在自己内存拷贝加1之后写入共享内存，同时线程2在内存拷贝(这时候内存拷贝的是100)加1，这时候再写入共享内存时，不会检查是不是101，而是直接写入共享内存。所以经常得到的结果是小于10000的。 volatile有序性 有序性是指程序的执行严格按照我们写的代码的顺序进行执行。 一般情况下，CPU和编译器为了提升程序执行的效率，会按照一定的规则允许对指令进行优化，即调整实际指令运行的顺序。指令重排不会对单线程的程序造成任何不利的影响，但是多线程环境下将会产生一些影响。指令重排的前提条件是指令调整后不会影响单线程程序的执行： 1234int i = 2; //statement 1int j = 1;//statement 2int k = i*j;//statement 3 在上面的代码中对于语句1和语句2相互之间没有任何依赖，所以可能发生指令重排，但是语句3和语句1,2都有关系，所以语句3一定是在语句1和语句2之后执行的。所以单线程情况下是绝对不会出现问题的。但是对于多线程可能就发生只是初始化了语句1或者语句2就执行语句3了。 综合volatile的性质，一般来说，volatile大多用于标志位上(判断操作),满足下面的条件才应该使用volatile修饰变量： 修改变量时不依赖变量的当前值(因为volatile是不保证原子性的) 该变量是可变的 在访问变量的时候不需要加锁(加锁就没必要使用volatile这种轻量级同步机制了) 当然对于上面的count++的非原子性问题，除了加锁之外，还可以使用J.U.C包下的atomic包，这些包中提供的类保证了原子性操作。上面的代码中的注释部分就是用的atomic包中的AtomicInteger。 另外，volatile的有序性可以解决单例双重检查对象初始化代码执行乱序问题 volatile有序性的底层实现原理. JMM层面:在volatile的操作前后需要加入内存屏障 LoadLoad 屏障，读操作与读操作不可重排序。LoadStore 屏障，读操作与写操作不可重排序。StoreLoad 屏障，写操作与读操作不可重排序。StoreStore 屏障，写操作与写操作不可重排序。 具体在 volatile 这里，内存屏障是这样加入的： 在每一个volatile写操作前面插入一个StoreStore屏障。这确保了在进行volatile写之前前面的所有普通的写操作都已经刷新到了内存。 在每一个volatile写操作后面插入一个StoreLoad屏障。这样可以避免volatile写操作与后面可能存在的volatile读写操作发生重排序。 在每一个volatile读操作后面插入一个LoadLoad屏障。这样可以避免volatile读操作和后面普通的读操作进行重排序。 在每一个volatile读操作后面插入一个LoadStore屏障。这样可以避免volatile读操作和后面普通的写操作进行重排序。 字节码层面：增加了一个ACC_VOLATILE标志 JVM/汇编层面：依赖于汇编指令lock;addl 1234// 写操作0x01a3de1d: movb $0×0,0×1104800(%esi);// 内存屏障0x01a3de24: lock addl $0×0,(%esp); 详细参考文章:https://www.pianshen.com/article/81043901/ 简单的说lock addl的作用就是： 锁总线： LOCK有效时表示CPU不允许其它总线主控者占用总线而CPU与内存等硬件之前的通信需要经过总线。因此会阻止其它主控者使用总线。说白了就是LOCK前缀只保证对当前指令要访问的内存互斥。 锁缓存： 由于锁总线开销较大，因此在之后的处理器中，LOCK#用来锁缓存而不是总线。在所有的 X86 CPU 上都具有锁定一个特定内存地址的能力，当这个特定内存地址被锁定后，它就可以阻止其它的系统总线读取或修改这个内存地址。这种能力是通过 LOCK 指令前缀再加上具体操作（如ADD）的汇编指令来实现的。 显式Lock锁显式Lock锁是JDK1.5之后才有的位于java.util.concurrent（J.U.C）包中。 读一下源码注释，简单了解下Lock锁： 实现Lock接口的类提供比synchronized更灵活和更多的锁操作功能。 可以绑定多个Condition对象 一般来说一个锁能防止多个线程共享资源，但是有些锁可以允许多个线程并发的访问共享资源如ReadWriteLock读写锁。 与synchronized相比，Lock锁的实现类可以实现synchronized不具备的功能，如尝试非阻塞的获取锁、超时获取锁和能被中断的获取锁 源码注释中也提供给了我们如何使用Lock锁 1234567Lock l = ...;l.lock();try { // access the resource protected by this lock} finally { l.unlock(); }} 注意两点： lock.lock()在try语句块外面，否则如果在获取锁(自定义锁的实现)时发生了异常，会导致锁无故释放。 lock锁一定要手动释放，一般是写在finally语句块中 用lock锁改写下我们之前的例子 12345678910111213141516171819202122232425public class T { private int count = 0; Lock lock = new ReentrantLock(); //等价于private synchronized void m1() public void m1(){ lock.lock(); //加锁 try { count++; Thread.sleep(500); System.out.println(count); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); //注意一定不要忘了手动释放锁 } } public static void main(String[] args) { T t = new T(); new Thread(()-&gt;t.m1(),\"t1\").start(); new Thread(()-&gt;t.m1(),\"t2\").start(); }} Lock接口中的方法： lock和unlock上面已经用过了，Conditon是在线程通信中使用，下篇文章讲，这里讲下tryLock()。 tryLock是尝试获取锁，如果能拿到锁就执行，否则就不执行。方法可以设置尝试获取锁的时间。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class T { Lock lock = new ReentrantLock(); public void m1(){ lock.lock(); try { for (int i = 0; i &lt; 10; i++) { TimeUnit.SECONDS.sleep(1); System.out.println(i); } } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } public void m2(){ boolean locked = false; try { System.out.println(\"m2开始尝试拿锁\"); locked = lock.tryLock(5,TimeUnit.SECONDS); if(locked){ System.out.println(locked+\"拿到锁了，开始执行需要同步的代码\"); }else{ System.out.println(locked+\"没有拿到锁，执行不需要同步的代码\"); } } catch (InterruptedException e) { e.printStackTrace(); } finally { if(locked) lock.unlock(); } } public static void main(String[] args) { T t = new T(); new Thread(()-&gt;t.m1(),\"t1\").start(); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } new Thread(()-&gt;t.m2(),\"t2\").start(); }} 123456789101112m2开始尝试拿锁01234false没有拿到锁，执行不需要同步的代码56789 这里对于lock锁只是简单介绍，之后会详细讲解同步器AQS以及Lock接口常用的实现类ReentrantLock。 synchronized和显式Lock的对比和选择synchronized和显式Lock锁的区别原始构成 synchronized是关键字属于JVM层面，底层通过monitor对象来完成 Lock是J.U.C包中的类，是API层面的锁 使用方法 synchronized不需要用户去手动释放锁，当synchronized代码执行完后系统会自动让线程释放对锁的占用。 ReentrantLock需要用户去手动释放锁，若没有主动释放锁，则有可能产生死锁现象。 等待是否可中断 synchronizd不可中断，除非抛出异常或者正常执行完成。 ReentrantLock可中断，1.设置超时方法tryLock(long timeout,TimeUnit unit)2. lockInterruptibly()放代码块中，调用interrupt()方法可中断 加锁是否公平 synchronized是非公平锁 ReentrantLock是公平锁和非公平锁都支持。默认是非公平锁 锁绑定多个condition synchronized没有，只能随机唤醒一个线程或者唤醒全部线程 ReentrantLock可以绑定多个condition对象，可以实现精准唤醒 如何选择synchronized和显式Lock锁我觉得最大的区别在于前者是JDK层面的，后者是JVM层面的。如果我们需要使用像tryLock或则lockInterruptly等方法时则需要使用我们的ReentrantLock锁。其他则可以使用synchronized锁。 另外我们还需要结合我们的具体场景:比如我现在是滴滴，我早上有打车高峰，我代码使用了大量的synchronized，有什么问题？锁升级过程是不可逆的，过了高峰我们还是重量级的锁，那效率是不是大打折扣了？这个时候你用Lock是不是很好？","link":"/2020/03/18/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8Bsynchronized%E9%94%81%E5%92%8C%E6%98%BE%E5%BC%8FLock%E9%94%81/"},{"title":"Java多线程之线程间通信","text":"主要内容 等待/通知机制 生产者、消费者模式 ThreadLocal 线程间通信包括： volatile和synchronized同步机制(见synchronized锁和显式Lock锁]) 等待/通知机制 生产者、消费者模式 Thread.join()的使用(见Thread类源码解析) ThreadLocal类 等待/通知机制还是把这张图放在这里，方便下面介绍 等待/通知机制使用的方法就是Object类上的wait()/notify()、notifyAll() 等待/通知机制最经典的范式就是生产者/消费者模式，但此模式在使用上还有点需要注意的，原理都是基于wait()/notify()的。 生产者/消费者模式注意： 使用wait()/notify() notifyAll()时要先调用对象加锁否则会报错IllegalMonitorStateException 使用while而不是if的原因是防止虚假唤醒。(以生产者为例，如果是if的话，可能多个线程在wait状态被唤醒后，其中一个线程生产完成，资源不为0，而这时候因为是if，剩余被唤醒的线程则会直接生产，造成超生。 同样，如果是消费者线程会造成超卖) 一定要使用notifyAll而不是notify。notify和notifyAll的区别是notify将等待队列中的一个线程移到同步队列，而notifyAll是将所有等待队列中的线程移到同步队列。比如生产者使用notify唤醒的仍然可能是生产者，这时候只有被唤醒的生产者线程，判断资源不为0则直接wait，导致程序无法继续往下运行。 synchronized方式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Resource { private int number; public synchronized void increse(){ //1.判断 while(number!=0){ try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } //2.干活 number++; System.out.println(Thread.currentThread().getName()+\"--\"+number); //3. 通知 this.notifyAll(); } public synchronized void decrese(){ //1.判断 while(number == 0){ try { this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } //2.干活 number--; System.out.println(Thread.currentThread().getName()+\"--\"+number); //3. 通知 this.notifyAll(); } public static void main(String[] args) { Resource resource = new Resource(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { resource.increse(); } },\"t1\").start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { resource.decrese(); } },\"t2\").start(); }} 显式Lock锁式 java.util.concurrent 类库中提供了 Condition类来实现线程之间的协调，可以在 Condition 上调用 await()方法使线程等待，其它线程调用 signal() 或 signalAll() 方法唤醒等待的线程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.company.MultiThread;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class Resource { private int number; private Lock lock = new ReentrantLock(); Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); public void increase() { lock.lock(); try { while(number!=0){ try { condition1.await(); } catch (InterruptedException e) { e.printStackTrace(); } } number++; System.out.println(Thread.currentThread().getName()+\"--\"+number); condition2.signalAll(); }finally { lock.unlock(); } } public void decrese(){ lock.lock(); try { while(number==0){ try { condition2.await(); } catch (InterruptedException e) { e.printStackTrace(); } } number--; System.out.println(Thread.currentThread().getName()+\"--\"+number); condition1.signalAll(); }finally { lock.unlock(); } } public static void main(String[] args) { Resource resource = new Resource(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { resource.increase(); } },\"t1\").start(); new Thread(()-&gt;{ for (int i = 0; i &lt; 10; i++) { resource.decrese(); } },\"t2\").start(); }} 上面的这个只是两个线程间进行通信，如果有多于2个线程，则可以通过设置标志位和多个condition对象进行准确控制 ThreadLocal类 ThreadLocal类是什么？ ThreadLoal 变量，就是我们的线程局部变量，每一个线程中的变量对于其他线程而言是隔离的。而且在使用ThreadLocal维护变量的时候，会为每一个使用该变量的线程提供一个独立的变量副本。这样同时多个线程访问该变量的时候不会彼此影响，也就不存在线程安全问题。 适用场景 总的来说，ThreadLocal 适用于每个线程需要自己独立的实例变量且该实例变量需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景。 在进行对象跨层传递的时候，使用ThreadLocal可以避免多次传递，打破层次间的约束。 线程间数据隔离 进行事务操作，用于存储线程事务信息。 数据库连接，Session会话管理。 ThreadLocal的demo 1234567891011121314151617181920212223242526272829303132333435import java.util.concurrent.TimeUnit;public class ThreadLocalDemo { //private static volatile Person person = new Person(); private static ThreadLocal&lt;Person&gt; threadLocal = new ThreadLocal&lt;&gt;(); public static void main(String[] args) { new Thread(()-&gt;{ try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } //System.out.println(Thread.currentThread().getName()+\"--\"+person.name); System.out.println(Thread.currentThread().getName()+\"--\"+threadLocal.get()); },\"t1\").start(); new Thread(()-&gt;{ try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } threadLocal.set(new Person()); //person.name = \"t2Name\"; //System.out.println(Thread.currentThread().getName()+\"--\"+person.name); System.out.println(Thread.currentThread().getName()+\"--\"+threadLocal.get().name); },\"t2\").start(); }}class Person{ String name = \"haha\";} 从结果可以看到,我们之前使用共享变量person时线程t2修改值后，线程t1中得到的值是t2修改过的值。 而使用ThreadLocal是t2修改name后只在t2修改，相当于在t2线程中添加了一个person对象，而t1线程中没有添加person对象，所以为null 12345t2--t2Namet1--t2Name------------------t2--hahat1--null 以数据库管理为例 1234567891011121314public class ConnectionManager { private static Connection connection = null; public static Connection openConnection(){ if(connect==null){ connnect = DriverManager.getConnection(); } return connnect; } public static void closeConnection(){ if(connect!=null){ connnect.close(); } }} 上面是一个数据库连接的管理类，我们使用数据库的时候首先就是建立数据库连接，然后用完了之后关闭就好了，这样做有一个很严重的问题，如果有1个客户端频繁的使用数据库，那么就需要建立多次连接和关闭，我们的服务器可能会吃不消，怎么办呢？如果有一万个客户端，那么服务器压力更大。 这时候最好使用ThreadLocal，因为ThreadLocal在每个线程中对连接会创建一个副本，且在线程内部任何地方都可以使用，线程之间互不影响，这样一来就不存在线程安全问题，也不会严重影响程序执行性能。是不是很好用。 源码分析首先我们先看一下它的最常用的set和get方法，我们就知道了它的工作原理。 set方法 1234567891011121314151617181920public void set(T value) { //拿到当前线程 Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);}ThreadLocalMap getMap(Thread t) { return t.threadLocals;}void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue);} get方法 12345678910111213public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; } } return setInitialValue();} 通过上面这些内容，我们足以通过猜测得出结论：最终的变量是放在了当前线程的ThreadLocalMap中，取值时，先拿到但前线程的ThreadLocalMap，在从map中进行取值。ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 我们去源码中验证下，可以看到ThreadLocalMap确实是ThreadLocal的内部类 具体ThreadLocalMap的内部源码这里就不细讲了，看过了HashMap源码的话，这个估计不难。我们这里就直接挑重点讲。 我们上面知道，如果是第一次set值的时候，会新建一个ThreadLocalMap，我们看看它的构造函数，看它是如何存储我们set的值的。 可以看到，实际上它存储的键就是我们的ThreadLocal对象，值就是我们Object类型的值(我们set进去的value)。比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，Thread内部都是使用仅有那个ThreadLocalMap 存放数据的。ThreadLocal 是map结构是为了让每个线程可以关联多个 ThreadLocal变量。这也就解释了 ThreadLocal声明的变量为什么在每一个线程都有自己的专属本地变量。 内存泄露问题 ThreadLocalMap 中使用的key为 ThreadLocal的弱引用,而 value是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value (生命周期和Thread相同)不会被清理掉。这样一来，ThreadLocalMap 中就会出现key为null的Entry。假如我们不做任何措施的话，value 永远无法被GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。 解决方法:使用完 ThreadLocal方法后 最好手动调用remove()方法 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; }} 总结 ThreadLocal底层实际是封装了一个ThreadLocalMap，ThreadLocalMap中存放的key是ThreadLocal对象，值就是我们通过set方法设置的值 ThreadLocal中由于key为ThreadLocal的弱引用，value为强引用，因此如果ThreadLocal在没有被外部引用的情况下，进行垃圾回收时，key会被清理掉而value不会。因此value无法被回收，产生内存泄露问题。 ThreadLocal考虑到了上面提到的内存泄露问题，在调用set、get、remove方法的时候，会清理掉key为null的标记，但我们最好在使用完ThreadLocal方法后，手动调用remove方法。","link":"/2020/03/19/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/"},{"title":"深入理解JVM之四大垃圾收集算法和七大垃圾收集器","text":"主要内容 判断对象是否存活 四大垃圾收集算法 七大垃圾收集器 上一节我们在介绍Java堆中简单提了下说Java堆是垃圾收集管理的主要区域，那么 如何判断Java堆内存中的对象是否是垃圾？(判断对象是否存活) 如果是垃圾，如何对垃圾进行收集？(四大垃圾收集算法) 收集时使用的垃圾收集器一共有几种？(七大垃圾收集器) 判断对象是否存活引用计数算法 为对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。 优点：实现简单、判定效率高 缺点：无法解决循环引用的问题。 可达性分析算法 以一系列GC Roots 为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收。 Java 虚拟机使用该算法来判断对象是否可被回收，GC Roots 一般包含以下内容： 虚拟机栈中局部变量表中引用的对象 方法区中的常量引用的对象 方法区中类静态属性引用的对象 本地方法栈中 JNI 中引用的对象 下图中的Object4就处于可被回收状态 引用类型 上面两种算法中判定对象是否可被回收都与引用有关。Java 提供了四种强度不同的引用类型。下面四种类型的引用强度依次减弱 强引用 我们平时用的new 对象创建的就是一个强引用。被强引用关联的对象不会被回收。 12Object obj = new Object();obj = null //通过手动将将栈中obj置为null，就可以被回收 软引用 软引用用来描述一些有用但非必须的对象。被软引用关联的对象只有在内存不够的情况下才会被回收，如果这次回收内存仍然不够，则会出现OOM异常。这种特性常常被用来实现缓存技术 在 JDK1.2 之后，用java.lang.ref.SoftReference类来表示软引用。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用 被弱引用关联的对象一定会被回收，也就是说弱引用只能存活到下一次垃圾回收发生之前。 使用 WeakReference 类来创建弱引用。 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null;// 使对象只被弱引用关联 虚引用 虚引用并不会决定对象的生命周期。在任何时候都可能被垃圾回收器回收。虚引用主要用来跟踪对象被垃圾回收器回收的活动。 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj, null);obj = null; 四大垃圾收集算法1. 标记 - 清除算法 首先标记出所有需要回收的对象，在标记后统一回收所有被标记的对象。 回收对象就是把对象作为分块，连接到被称为 “空闲链表” 的单向链表，之后进行分配时只需要遍历这个空闲链表，就可以找到分块。 在分配时，程序会搜索空闲链表寻找空间大于等于新对象大小 size 的块 block。如果它找到的块等于 size，会直接返回这个分块；如果找到的块大于 size，会将块分割成大小为 size 与 (block - size) 的两部分，返回大小为 size 的分块，并把大小为 (block - size) 的块返回给空闲链表。 不足： 标记和清除过程效率都不高； 会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2.复制算法 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 下面两段话注意，之后在说明这个垃圾回收过程中会用到。现在的商业虚拟机都采用复制算法回收新生代。经研究发现98%的对象都是“朝生夕死”的，所以并不需要划分为大小相等的两块，而是一块较大的 Eden 空间和两块较小的 Survivor 空间。在回收时，将 Eden 和 Survivor 中还存活着的对象全部复制到另一块 Survivor 上，最后清理 Eden 和使用过的那一块 Survivor。 HotSpot 虚拟机的 Eden 和 Survivor0 和 Survivor1大小比例默认为 8:1:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 就不够用了，此时需要依赖于老年代进行空间分配担保，也就是借用老年代的空间存储放不下的对象。 3. 标记 - 整理算法 与标记-清除算法类似，只不过不是直接对对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 4. 分代收集算法 现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 算法 七大垃圾收集器上面我们介绍的是垃圾收集的算法，具体实现则是通过垃圾收集器来实现的。 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。这里我们重点关注CMS垃圾收集器和G1垃圾收集器，其他的简单了解即可。 Serial 收集器 单线程收集器、Client场景下默认新生代收集器 ParNew 收集器 Serial 收集器的多线程版本、 Server 场景下默认的新生代收集器 Parallel Scavenge 收集器 多线程收集器、“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户程序的时间占总时间的比值。 Serial Old 收集器 Serial 收集器的老年代版本、给 Client 场景下的虚拟机使用 Parallel Old 收集器 Parallel Scavenge 收集器的老年代版本。在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器 接下来重点解释下CMS收集器和G1收集器 CMS收集器 第一款并发的垃圾收集器(用户线程和收集器线程线程同时进行) CMS收集器是一种老年代收集器，以获得最短回收停顿时间为目标。 CMS收集器是基于标记-清除算法实现的。 工作过程分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 G1收集器 我们知道堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。 G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。从而将原来的一整块内存空间划分成多个的小空间，使得每个小Region可以单独进行垃圾回收。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描 G1垃圾收集器的优点 并发 通过并发方式让Java程序在垃圾收集时期继续执行 分代收集 虽然可以将新生代和老年代一同收集，但是分代的概念依然在G1中保留。仍然能够采用不同的方式去处理新创建的对象和已经存活一段时间的旧对象 空间整合 G1从整体来看是基于标记-整理算法，局部(两个Region间)是基于复制算法实现的。两种算法都意味着G1运行期间不会产生内存空间碎片 可预测停顿 能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 如果不计算维护 Remembered Set 的操作，G1 收集器的 运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。","link":"/2020/03/16/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E4%B9%8B%E5%9B%9B%E5%A4%A7%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%83%E5%A4%A7%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/"},{"title":"多线程开篇","text":"主要内容 程序、进程、线程、并行、并发、同步、异步、阻塞、非阻塞 创建线程的三种基本方式(线程池的方式后面再加) 基本概念程序、进程、线程 程序：是为完成特定任务，用某种语言编写的一组指令的集合，即指一段静态的代码，静态对象。 进程：是程序的一次执行过程，或是正在运行的一个程序，是一个动态的过程，有它自身的产生，存在和消亡的过程。 线程：进程可进一步细化为线程，是一个程序内部的一条执行路径 进程和线程的对比： 进程是资源分配的基本单位,而线程不拥有资源（也有一点儿必不可少的资源），但同一进程内多个线程共享进程内的资源 线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，则会引起进程切换。 创建或撤销进程时的开销远大于创建或撤销线程时的开销。(创建或撤销进程时，系统都要为之分配或回收资源) 线程不能单独执行，必须组成进程，一个进程至少有一个主线程。简而言之，一个程序至少有一个进程，一个进程至少有一个线程。 并行、并发 并行：一个时间点多个程序可以同时执行。(**多个人同时做多件事) 多核多CPU或多机器处理同一段处理逻辑的时候，同一时刻多个执行流共同执行 并发：一段时间内多个程序可以运行。一个CPU，通过CPU的调度算法，使用户感觉像是同时处理多个任务，但同一时刻只有一个执行流占用CPU执行。 同步、异步同步和异步强调的是消息通信机制。 同步:就是调用某个方法时，调用方得等待这个调用返回结果才能继续往后执行。 异步 ：调用方不会立即得到结果，而是在调用发出后继续执行后续操作，被调用者通过状态来通知调用者。 阻塞、非阻塞阻塞和非阻塞 强调的是程序在等待调用结果（消息，返回值）时的状态. 阻塞：调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞：指在不能立刻得到结果之前，该调用不会阻塞当前线程。 创建线程的三种基本方式创建线程的3中基本方式： 继承 Thread类 实现 Runnable接口 实现 Callable接口 继承Thread类继承Thread类，重写run方法 12345678public class MyThread extends Thread { @Override public void run() { for (int i = 0; i &lt; 100; i++) { System.out.println(); } }} 调用时直接使用线程的start()方法。 12345678910public class test { public static void main(String[] args) { //创建两个线程 MyThread t1 = new MyThread1(); MyThread t2 = new MyThread1(); //启动线程 t1.start(); t2.start(); }} 实现Runnable接口实现Runnable接口，实现run方法 12345678public class MyRunnable implements Runnable { @Override public void run() { for (int i = 0; i &lt;100 ; i++) { System.out.println(i); } }} 注意调用线程时，还是使用Thread的start()方法。 123456789101112public class test { public static void main(String[] args) { //创建实现Runnable接口的类对象 Runnable myRunnable = new MyRunnable(); //将接口类对象作为参数传递到Thread类创建线程 Thread t1 = new Thread(myRunnable); Thread t2 = new Thread(myRunnable); //启动线程 t1.start(); t2.start(); }} 实现Callable接口基于java.util.concurrent.Callable工具类的实现 123456public class MyCallable implements Callable { @Override public Object call() throws Exception { return 1; }} 12345678910111213141516171819public class test { public static void main(String[] args){ //创建实现Callable接口的类对象 Callable myCallable = new MyCallable(); //构建FutureTask对象 FutureTask&lt;Integer&gt; integerFutureTask = new FutureTask&lt;&gt;(myCallable); //创建线程 Thread t1 = new Thread(integerFutureTask); //启动线程 t1.start(); try { System.out.println(integerFutureTask.get()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } }} 当然更常见的也是更加经常书写的是使用匿名内部类的方式或者lambda表达式的方式 匿名内部类 1234567891011121314151617181920212223public class test { public static void main(String[] args){ //实现Runnable接口方法 new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &lt; 100; i++) { System.out.println(i); } } }).start(); //继承Thread类方法 new Thread(){ @Override public void run() { for (int i = 0; i &lt; 100; i++) { System.out.println(i); } } }.start(); }} 当然还可以使用Java8新特性lambda表达式的形式 123456789public class test { public static void main(String[] args){ new Thread(()-&gt;{ for (int i = 0; i &lt; 100; i++) { System.out.println(i); } }).start(); }} 注意事项实现接口 VS 继承 Thread 实现接口会更好一些，因为： Java 不支持多重继承，因此继承了 Thread 类就无法继承其它类，但是可以实现多个接口； 实现Runnable接口更适合用来处理多个线程共享数据的情况。 类可能只要求可执行就行，继承整个 Thread 类开销过大。 start()方法和run()方法 start()方法的作用：1.启动当前线程 2.调用当前线程的run()方法(在主线程中生成子线程，有两条线程) run()方法的作用：在主线程中调用后，只有主线程一条线程中执行了该线程方法。(调用线程run方法，只调用run方法，并不新开线程) Runnable接口和Callable接口 使用方法相似，只不过Callable接口功能更丰富些： call方法可以有泛型返回值(获取返回结果需要借助FutureTask类) call方法可以抛出异常 可以使用工具类Executors实现Runnable接口和Callable 接口之间的相互转换。（Executors.callable（Runnable task）或 Executors.callable（Runnable task，Object resule））。 Runnable接口源代码 123public interface Runnable { public abstract void run();} Callable接口源代码 1234@FunctionalInterface //支持函数式接口public interface Callable&lt;V&gt; { V call() throws Exception;}","link":"/2020/03/10/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"title":"Java多线程之并发容器","text":"主要内容 集合容器的安全性问题 并发容器分类 集合容器的安全问题Java的集合容器框架中，主要有四大类别：List、Set、Queue、Map，大家熟知的这些集合类ArrayList、LinkedList、HashMap这些容器都是非线程安全的。如果有多个线程并发地访问这些容器时，就会出现问题。比如下面这个例子： 1234567891011121314151617package com.company.MultiThread;import java.util.ArrayList;import java.util.List;import java.util.UUID;public class ListUnsafeDemo { public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { new Thread(()-&gt; { list.add(UUID.randomUUID().toString().substring(0,6)); System.out.println(list); },\"t\"+i).start(); } }} 输出结果中会抛出并发修改异常 1java.util.ConcurrentModificationException set、Map、Queue同理,这里就不演示了。 如何解决并发修改异常以ArrayList为例(set、Map、Queue同理，具体参照下面的表格),解决方法如下 1.使用同步容器Vector List&lt;String&gt; list = new Vector&lt;&gt;(); 2.使用Collections.synchronizedArrayList List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); 3.使用JUC包中的并发容器CopyOnWriteArrayList List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); 我们先看下Vector和Collections.synchronizedArrayList的源码： Vector中的方法 12public synchronized boolean add(E e) public synchronized E get(int index) Collections.synchronizedArrayList去翻一下源码，也可以知道： 1234567public boolean add(E e) { synchronized (mutex) {return c.add(e);}}public E get(int index) { synchronized (mutex) {return list.get(index);}} 可以看到无论是Vector还是Collections.synchronizedArrayList都是通过synchronized关键字加锁来保证多线程下的集合安全。但是这样做的代价是降低了并发性，当多个线程共同竞争容器级的锁时，吞吐量就会降低 并发容器的分类为了解决同步容器的性能问题，所以在JDK1.5之后有了并发容器，这些并发容器位于J.U.C包中。 我们对上面的并发容器做个简单的说明： CopyOnWriteArrayList 底层是一个volatile修饰的数组 1private transient volatile Object[] array; add操作 1234567891011121314public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); } } get操作 123public E get(int index) { return get(getArray(), index);} 总结： 底层是volatile修饰的数组 写操作加锁，读操作不加锁。 写操作过程：先复制一份新的数组，在新的集合上面修改，然后将新数组赋值给旧的引用。 CopyOnWriteArraySet 底层基于CopyOnWriteArrayList实现 1private final CopyOnWriteArrayList&lt;E&gt; al; add操作 12345678public boolean add(E e) { return al.addIfAbsent(e);}public boolean addIfAbsent(E e) { Object[] snapshot = getArray(); return indexOf(e, snapshot, 0, snapshot.length) &gt;= 0 ? false : addIfAbsent(e, snapshot);} add时调用的是CopyOnWriteArrayList的addIfAbsent方法，其遍历当前Object数组，如Object数组中已有了当前元素，则直接返回，如果没有则放入Object数组的尾部，并返回。 ConcurrentHashMap、ConcurrentSkipListMap、ConcurrentSkipListSet 因为比较重要，所以放在下篇文章讲。 ConcurrentLinkedQueue和ConcurrentLinkedDeque ConcurrentLinkedQueue底层是基于链表实现的FIFO队列ConcurrentLinkedDeque底层是基于链表实现的双端队列 BlockingQueue 因为线程池中用到了，比较重要，也会新开一篇","link":"/2020/03/26/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8/"},{"title":"Java多线程之CAS和AQS源码解析","text":"主要内容 CAS思想 AQS实现锁 AQS源码分析 本来这篇文章是打算讲下Lock的子类实现的，但是Lock的子类实现都是基于AQS(AbstractQueuedSynchronizer)的，而AQS中很多地方用到了CAS操作来提高并发效率。所以这篇文章直接撸这两个东东。 CAS什么是CAS CAS(Compare And Swap)：比较并交换，它是乐观锁的一种实现方式。主要有三个操作数：内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做 线程在读取数据时不进行加锁，在准备写回数据时，先去查询原值，操作的时候比较原值是否修改，若未被其他线程修改则写回，若已被修改，则重新执行读取流程。 CAS在Java中的应用 JAVA1.5开始引入了CAS，主要代码都放在J.U.C的atomic包下 我们以AtomicInteger的getAndIncrement为例，看下它的底层源码，其他的方法和这个方法底层是类似的。了解了这个，其他的方法也就了解了。 我们从源码中可以看到getAndIncrement的底层中调用了unsafe.getAndAddInt()方法。我们从AtomicInteger上面的源码可以看到unsafe是Unsafe类的一个对象。 Unsafe类是位于sun.misc包中,其内部方法(都是native修饰的方法,就不贴图了)操作可以像C指针一样，直接操作内存。 知道了Unsafe类，我们回过头来再看下getAndIncrement方法，它实际上是调用了Unsafe类中的getAndAddInt方法，并且传递了三个参数： this 就是当前类的对象 valueOffset 变量的内存偏移地址 1 就是我们每次+1操作的1 在getAndAddInt中var1 var2 var4对应于上面传递过来的三个参数 通过本地方法getIntVolatile，通过当前对象(var1)和对象中变量的内存偏移(var2)得到我们的变量值。(将主内存变量拷贝到工作内存) 进行do-while循环判断。 判断条件中通过compareAndSwapInt进行原子更新操作，将主内存变量和工作内存变量var5进行比较，相同则执行+1操作，并写入主内存，跳出循环。不同则comepareAndSwapInt方法返回false，重新进行循环操作。 注意：上面只是为了解释方便分了1.2.3步，实际上整个过程是原子操作的，即不允许被中断。 这时候我们也就理解了为什么可以用AtomicInteger可以解决volatile的原子性问题。因为AtomicInteger和volatile相比多了比较的步骤，即CAS CAS的ABA问题 CAS有很多优点，比如： 和synchronized相比较，CAS并没有进行加锁，所以并发性得到提高 和volatile相比，则是保证了原子性 但是CAS同时也存在一些缺点: 循环时间长，开销大。(因为如果CAS失败，会一直进行do-while循环，长时间不成功，会给CPU带来很大的开销)。 只能保证一个变量的原子操作。(对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候只能用锁来保证原子性) ABA问题 什么是ABA问题 线程1从内存X中取出A，这时候另一个线程2也从内存X中取出A，并且线程2进行了一些操作将内存X中的值变成了B，然后线程2又将内存X中的数据变成A，这时候线程1进行CAS操作发现内存X中仍然是A，然后线程1操作成功。虽然线程1的CAS操作成功，但是整个过程就是有问题的。比如链表的头在变化了两次后恢复了原值，但是不代表链表就没有变化。 下面的代码中演示了ABA问题，可以看到线程t1将100变为101后又变回100发生ABA问题。而线程t2仍然比较成功，并重新设置为了102. 123456789101112131415161718192021222324252627282930package com.company.MultiThread;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicReference;public class ABADemo { private AtomicReference&lt;Integer&gt; atomicReference = new AtomicReference&lt;&gt;(100); public static void main(String[] args) { ABADemo abaDemo = new ABADemo(); new Thread(()-&gt;{ //ABA t1 100--&gt;101--&gt;100 abaDemo.atomicReference.compareAndSet(100,101); abaDemo.atomicReference.compareAndSet(101,100); },\"t1\").start(); new Thread(()-&gt;{ //睡2秒钟，保证线程1发生ABA问题 try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(abaDemo.atomicReference.compareAndSet(100,102) +\"----\"+abaDemo.atomicReference.get()); },\"t2\").start(); }} ABA之后线程t2仍然比较成功，并重新设置为了102。 1true----102 ABA问题的解决JAVA中提供了AtomicStampedReference/AtomicMarkableReference来处理会发生ABA问题的场景，主要思路就是在对象中额外再增加一个标记来标识对象是否有过变更。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.company.MultiThread;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicStampedReference;/** * Created by chen on 2020/3/22 */public class ABADemo { private AtomicStampedReference&lt;Integer&gt; atomicStampedReference= new AtomicStampedReference&lt;&gt;(100,1); //1为标记号 public static void main(String[] args) { ABADemo abaDemo = new ABADemo(); new Thread(()-&gt;{ int stamp = abaDemo.atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName()+\"第一次版本号为\"+stamp); //暂停1秒钟让t2线程拿到第一次版本号 try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } //ABA t1 100--&gt;101--&gt;100 // t1版本号 1 2 3 abaDemo.atomicStampedReference.compareAndSet(100,101, abaDemo.atomicStampedReference.getStamp(),abaDemo.atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName()+\"第二次版本号为\"+abaDemo.atomicStampedReference.getStamp()); abaDemo.atomicStampedReference.compareAndSet(101,100, abaDemo.atomicStampedReference.getStamp(),abaDemo.atomicStampedReference.getStamp()+1); System.out.println(Thread.currentThread().getName()+\"第三次版本号为\"+abaDemo.atomicStampedReference.getStamp()); },\"t1\").start(); new Thread(()-&gt;{ int stamp = abaDemo.atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName()+\"第一次版本号为\"+stamp); //睡3秒钟，保证线程1发生ABA问题 try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } boolean flag = abaDemo.atomicStampedReference.compareAndSet(100,102, stamp,stamp+1); int stamp2 = abaDemo.atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName()+\"是否修改成功:\"+flag+\"当前实际版本号为\"+stamp2+\"实际变量值为\"+ abaDemo.atomicStampedReference.getReference()); },\"t2\").start(); }} 12345t1第一次版本号为1t2第一次版本号为1t1第二次版本号为2t1第三次版本号为3t2是否修改成功:false当前实际版本号为3实际变量值为100 AQS讲完了CAS我们再来看下这篇文章的主角AQS，之所以先讲CAS就是因为AQS中用到了CAS。 1. 什么是AQS AQS是位于J.U.C包中的locks子包下面的抽象类AbstractQueuedSynchronizer的简写，顾名思义就是一个抽象的队列同步器。 我们看下它的API文档和注释就可以知道它的作用： 提供了一个框架来实现锁和相关的同步器。如常用的ReentrantLock和同步器Semaphore/CountDownLatch等。 核心是维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列） 支持两种线程模式独占模式和共享模式。在不同模式中的等待线程共享FIFO队列。通常子类实现只支持一种模式，但是ReadWriteLock支持两种模式。子类实现只需要重写对应模式的方法即可，具体线程等待队列的维护AQS已经帮我们实现好了。 定义了一个内部类ConditionObject用来支持独占模式的子类实现Conditon 2. 如何使用AQS 上面我们从源码注释中知道了AQS是一个框架来实现锁和相关的同步器。那怎么使用AQS来实现一把锁呢？ 我们先把实现锁的方法提一下，然后直接看下注释中的独占锁是怎么实现的。 锁和同步器的设计是基于模板方法模式的。我们需要继承同步器并重写相应模式的指定方法，然后将同步器组合在自定义同步组件的实现中，并调用同步器的模板方法，而这些模板方法将会调用我们重写的方法。 重写同步器的指定方法时，需要使用同步器提供的三个方法来访问或修改同步状态： getState() 获取当前同步状态 setState(int newState) 设置当前同步状态 compareAndSetState(int expect,int update) 使用CAS设置当前状态，该方法能够保证状态设置的原子性。 需要重写的方法主要包括： 方法名称 描述 tryAcquire(int) 独占方式。尝试获取资源，成功则返回true，失败则返回false。实现该方法需要查询当前状态并判断同步状态是否符合预期，然后再进行CAS设置同步状态 tryRelease(int) 独占方式。尝试释放资源，成功则返回true，失败则返回false。 等待获取同步状态的线程将有机会获取同步状态 tryAcquireShared(int) 共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int) 共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 isHeldExclusively() 该线程是否正在独占资源。只有用到condition才需要去实现它。 下面是Java注释中提供的一个不可重入的互斥锁的实现。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041class Mutex implements Lock, java.io.Serializable { // 静态内部类，自定义同步器 private static class Sync extends AbstractQueuedSynchronizer { // 判断是否处于锁定状态 protected boolean isHeldExclusively() { return getState() == 1; } // 当状态为0的时候尝试获取锁 public boolean tryAcquire(int acquires) { assert acquires == 1; //如果传进来的不是1就不执行下面的代码，是1则继续执行 if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } // 尝试释放锁，将状态设置为0 protected boolean tryRelease(int releases) { assert releases == 1; // Otherwise unused if (getState() == 0) //既然来释放肯定是已经占有状态，这里只是为了保险，多层判断 throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); //释放资源，放弃占有状态 return true; } //只需要将操作代理到内部类Sync上，其余困难的操作AQS已经帮我们实现好了 private final Sync sync = new Sync(); //lock&lt;--&gt;acquire。两者语义一样：获取资源，即便等待，直到成功才返回。 public void lock() { sync.acquire(1); } //tryLock&lt;--&gt;tryAcquire。两者语义一样：尝试获取资源，要求立即返回。 public boolean tryLock() { return sync.tryAcquire(1); } //unlock&lt;--&gt;release。两者语文一样：释放资源。 public void unlock() { sync.release(1); } //判断是否占有锁 public boolean isLocked() { return sync.isHeldExclusively(); } }} 3.AQS底层源码解析 3.0Node结点 这里我们说下Node。Node结点是对每一个等待获取资源的线程的封装，其包含了需要同步的线程本身(thread)和其等待状态(waitStatus)以及前驱和后继节点 我们看下Node节点中都定义了些什么东东： 注意: 负值表示结点处于有效等待状态，而正值表示结点已被取消。所以源码中很多地方用&gt;0、&lt;0来判断结点的状态是否正常。 nextWaiter 是等待在conditon上的线程所构成的等待队列中的后继节点。 如果当前节点共享，将会是一个SHARED常量。 下面的这幅图中更加详细的描述了AQS的FIFO队列 3.1独占式同步状态获取 acquire(int) 此方法是独占模式下线程获取共享资源的顶层入口。如果获取到资源，线程直接返回，否则进入等待队列，直到获取到资源为止，且整个过程忽略中断的影响。这也正是lock()的语义，当然不仅仅只限于lock()。获取到资源后，线程就可以去执行其临界区代码了。我们看下acquire方法的源码 12345public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} 可以看到上面的acquire方法中调用了tryAcquire()、addWaiter()、acquireQueued()以及selfInterrupt()方法，接下来我们一一看看这些方法。 3.1.1tryAcquire()方法 此方法尝试去获取独占资源。如果获取成功，则直接返回true，否则直接返回false。源码如下： 123protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();} 还记得我们上面实现自定义锁要重写的方法和java给我们的注释中Mutex锁的实现吗？ 这个tryAcquire()方法，就是我们**实现自定义独占锁需要重写的方法**。 3.1.2addWaiter()方法 此方法用于将当前线程加入到等待队列的队尾，并返回当前线程所在的结点。 1234567891011121314151617181920private Node addWaiter(Node mode) { //以给定模式构造节点。mode有两种：EXCLUSIVE（独占）和SHARED（共享） Node node = new Node(Thread.currentThread(), mode); //快速尝试在尾部添加 Node pred = tail; //如果队列不为空，则用CAS方式将当前节点设为尾结点 if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } //代码执行到这里，只有两种情况 //1. 队列为空 //2. CAS失败 // 注意, 这里是并发条件下, 所以什么都有可能发生, 尤其注意CAS失败后也会来到这里 enq(node); //将节点插入队列 return node; } 在这个方法中，我们首先会尝试直接入队，但是因为目前是在并发条件下，所以有可能同一时刻，有多个线程都在尝试入队，导致compareAndSetTail(pred, node)操作失败——因为有可能其他线程已经成为了新的尾节点，导致尾节点不再是我们之前看到的那个pred了。 如果入队失败了，接下来我们就需要调用enq(node)方法，在该方法中我们将通过自旋+CAS的方式，确保当前节点入队。enq方法的源码： 1234567891011121314151617181920private Node enq(final Node node) { //CAS\"自旋\"，直到成功加入队尾 for (;;) { Node t = tail; // 队列为空，首先进行初始化(可以看出队列是延迟加载的，只有用到的时候再加载，而不是在构造的时候直接初始化) //创建一个空的节点作为head节点，并将tail也指向它。 if (t == null) { //注意，初始化时使用new Node()新建了一个dummy节点 if (compareAndSetHead(new Node())) tail = head; //将尾节点指向dummy节点 } else { //队列不为空，正常入队列 node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 这里尤其要注意的是，当队列为空时，我们初始化队列并没有使用当前传进来的节点，而是：新建了一个空节点！！！ 在新建完空的头节点之后，我们并没有立即返回，而是将尾节点指向当前的头节点，然后进入下一轮循环。在下一轮循环中，尾节点已经不为null了，此时再将我们包装了当前线程的Node加到这个空节点后面。这就意味着，在这个等待队列中，头结点是一个“dummy节点”，它不代表任何等待的线程。head节点不代表任何线程，它就是一个空节点！！！ 尾分叉继续往下分析 12345678} else {// 到这里说明队列已经不是空的了, 这个时候再继续尝试将节点加到队尾 node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; }} 这里将node节点添加到队列中需要三步： 设置node的前驱节点为当前的尾节点：node.prev = t 修改tail属性，使它指向当前节点 compareAndSetTail(t, node) 修改原来的尾节点，使它的next指向当前节点 t.next = node 但是需要注意的，这里的三步并不是一个原子操作，第一步很容易成功；而第二步由于是一个CAS操作，在并发条件下有可能失败，第三步只有在第二步成功的条件下才执行。这里的CAS保证了同一时刻只有一个节点能成为尾节点，其他节点将失败，失败后将回到for循环中继续重试。 所以，当有大量的线程在同时入队的时候，同一时刻，只有一个线程能完整地完成这三步，而其他线程只能完成第一步，于是就出现了尾分叉： 注意，这里第三步是在第二步执行成功后才执行的，这就意味着，有可能即使我们已经完成了第二步，将新的节点设置成了尾节点，此时原来旧的尾节点的next值可能还是null(因为还没有来的及执行第三步)，所以如果此时有线程恰巧从头节点开始向后遍历整个链表，则它是遍历不到新加进来的尾节点的，但是这显然是不合理的，因为现在的tail已经指向了新的尾节点。 另一方面，当我们完成了第二步之后，第一步一定是完成了的，所以如果我们从尾节点开始向前遍历，已经可以遍历到所有的节点。这也就是为什么我们在AQS相关的源码中，有时候常常会出现从尾节点开始逆向遍历链表——因为一个节点要能入队，则它的prev属性一定是有值的，但是它的next属性可能暂时还没有值。 至于那些“分叉”的入队失败的其他节点，在下一轮的循环中，它们的prev属性会重新指向新的尾节点，继续尝试新的CAS操作，最终，所有节点都会通过自旋不断的尝试入队，直到成功为止。 3.1.3acquireQueued()方法 首先看这个方法前，需要先明确几个概念 能执行到该方法, 说明addWaiter 方法已经成功将包装了当前Thread的节点添加到了等待队列的队尾 该方法中将再次尝试去获取锁 在再次尝试获取锁失败后, 判断是否需要把当前线程挂起 acquireQueued的源码如下： 123456789101112131415161718192021222324252627final boolean acquireQueued(final Node node, int arg) { boolean failed = true; //没有拿到资源 try { boolean interrupted = false; //没有被中断过 //CAS自旋 for (;;) { //拿到node节点(因为上面入队，所以此时node节点是尾结点)的前驱节点 final Node p = node.predecessor(); //如果前驱节点是头结点则再次尝试获取同步状态 if (p == head &amp;&amp; tryAcquire(arg)) { //获取成功，则将当前节点设置为头结点(上面已经拿到同步状态，所以不需要进行CAS，普通的set方法即可) setHead(node); p.next = null; //GC failed = false; //标记成功获取资源 return interrupted; //返回等待过程中是否被中断 } //node的前驱节点不是头结点或者获取资源失败 判断是否需要使当前线程挂起(进入WAITING状态) if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { //如果等待过程中没有成功获取资源（如timeout，或者可中断的情况下被中断了），那么取消结点在队列中的等待。 if (failed) cancelAcquire(node); } } 这里我们要注意setHead这个方法 12345private void setHead(Node node) { head = node; node.thread = null; node.prev = null;} 可以看出，这个方法的实际上是丢弃原来的head，将head指向已经获得了锁的node。但是接着又将该node的thread属性置为null了，这某种意义上导致了这个新的head节点又成为了一个dummy节点。某种程度上就是将当前线程从等待队列里面拿出来了，是一个变相的出队操作。 接下来我们再来看看另一种情况，即p == head &amp;&amp; tryAcquire(arg)返回了false，此时我们需要判断是否需要将当前线程挂起： 先看看shouldParkAfterFailedAcquire()和parkAndCheckInterrupt()具体干些什么。 shouldParkAfterFailedAcquire从函数名也可以看出, 该方法用于决定在获取锁失败后, 是否将线程挂起.决定的依据就是前驱节点的waitStatus值。 12345678910111213141516171819202122232425private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; //拿到前驱节点的状态 if (ws == Node.SIGNAL) /* * 如果前驱节点状态为SIGNAL,则表示后继节点即node节点处于WAITING状态 * 这就好比排队时，你告诉前面的人我先去那边歇会，等你进去了release或者不相等了cancel，麻烦你叫醒我 */ return true; if (ws &gt; 0) { //只有CANCELLED状态&gt;0 /* * 前驱节点处于CANCELLED状态即前驱节点不想等了，就一直往前找，直到找到正常等待的状态，并排在它后面。 * 注意：处于CANCELLED的节点形成一个无引用链，稍后就会被GC */ do { node.prev = pred = pred.prev; } while (pred.waitStatus &gt; 0); pred.next = node; } else { /* * 前驱节点状态既不是SIGNAL,也不是CANCELLED,则用CAS设置为SIGNAL */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 这里需要注意，只有这个函数在当前节点的前驱节点的waitStatus状态本身就是SIGNAL的时候才会返回true, 其他时候都会返回false。 返回false后会回到循环中再次尝试获取锁。获取成功则直接结束，否则继续判断是否需要挂起，因为上次已经将状态改为SIGNAL,所以这次会直接挂起 当shouldParkAfterFailedAcquire返回true，即当前节点的前驱节点的waitStatus状态已经设为SIGNAL后，我们就可以安心的将当前线程挂起了，此时我们将调用parkAndCheckInterrupt： 1234private final boolean parkAndCheckInterrupt() { LockSupport.park(this);//调用park()使线程进入waiting状态 return Thread.interrupted();//如果被唤醒，查看自己是不是被中断的。} 3.1.4小结 acquireQueued()的源码终于分析完了，我们再贴下acquire(int)的源码，总结下流程： 12345public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} 函数流程如下： 调用我们自定义同步器时重写的tryAcquire()方法尝试直接去获取资源，如果成功则直接返回； 没成功，则调用addWaiter()方法生成节点加入队列尾部(先快速添加，快速添加失败通过enq函数进行CAS自旋添加)，并标记为独占模式。 acquireQueued()方法使得每个节点(线程)进行CAS自旋，当前驱节点为头结点且能够获取到同步状态，则获取到资源进行返回；否则线程进入WAITING状态。 如果线程在等待过程中被中断，不做响应。只有在获取资源后才进行自我中断。 最后节点的状态如下： 除了头节点，其余节点全部为WAITING状态 除了尾节点，其余节点都满足waitStatus=SIGNAL，表示释放后需要唤醒后继节点 3.2独占式同步状态释放 release(int) 上一小节已经把acquire()说完了，这一小节就来讲讲它的反操作release()吧。此方法是独占模式下线程释放共享资源的顶层入口。它会释放指定量的资源，如果彻底释放了（即state=0）,它会唤醒等待队列里的其他线程来获取资源。这也正是unlock()的语义。下面是release()的源码： 12345678910public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; //找到头结点 //有头结点，并且头结点后面有挂起等待的后继节点(h.waitStatus = 0表示刚初始化，没有后继节点) if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); //唤醒等待队列里的下一个线程 return true; } return false; } 3.2.1tryRelease() 此方法尝试去释放指定量的资源,源码如下： 123protected boolean tryRelease(int arg) { throw new UnsupportedOperationException();} 跟tryAcquire()一样，这个方法是需要独占模式的自定义同步器去实现的。正常来说，tryRelease()都会成功的，因为这是独占模式，该线程来释放资源，那么它肯定已经拿到独占资源了，直接减掉相应量的资源即可(state-=arg)，也不需要考虑线程安全的问题。但要注意它的返回值，release()是根据tryRelease()的返回值来判断该线程是否已经完成释放掉资源了！所以自义定同步器在实现时，如果已经彻底释放资源(state=0)，要返回true，否则返回false。 3.2.2unparkSuccessor() 1234567891011121314151617181920212223private void unparkSuccessor(Node node) { int ws = node.waitStatus; // 如果当前线程节点状态&lt;0 则直接将其置为0 if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /** * 当前node是释放锁的node，则在队列中后面的节点，一般是需要唤醒的节点 * 如果后继节点存在且也在等待锁，就直接唤醒它 * 但是有可能存在 后继节点取消等待锁的情况 * 此时从尾结点开始向前找，直到找到有效节点 */ Node s = node.next; //找到下一个需要唤醒的有效节点 if (s == null || s.waitStatus &gt; 0) { s = null;//如果为空或者已取消 进行GC for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) //从后往前找 if (t.waitStatus &lt;= 0) //&lt;=0为有效节点 s = t; } if (s != null) LockSupport.unpark(s.thread); //唤醒 } 这里有一个小问题就是为什么找有效节点时是从尾结点往前面找，而不是直接找head的下一个结点？ 首先我们要看到，从后往前找是基于一定条件的： 1if (s == null || s.waitStatus &gt; 0) 即后继节点不存在，或者后继节点取消了排队，这一条件大多数条件下是不满足的。因为虽然后继节点取消排队很正常，但是通过上面我们介绍的shouldParkAfterFailedAcquire方法可知，节点在挂起前，都会给自己找一个waitStatus状态为SIGNAL的前驱节点，而跳过那些已经cancel掉的节点。 所以，这个从后往前找的目的其实是为了照顾刚刚加入到队列中的节点，这就涉及到我们上面讲的尾分叉了。刚刚加入的节点的prev一定是指向了前面的节点。而由于CAS失败或者虽然CAS成功但unparkSuccessor方法就开始执行，此时pred.next还没有设置值就会造成从前往后遍历遍历不到尾结点。 最后, 在调用了 LockSupport.unpark(s.thread) 也就是唤醒了线程之后, 会发生什么呢? 我们上面讲获取资源的时候，不是到了获取失败被挂起了吗 1234private final boolean parkAndCheckInterrupt() { LockSupport.park(this); // 喏, 就是在这里被挂起了, 唤醒之后就能继续往下执行了 return Thread.interrupted();} 我们现在唤醒s线程后，s线程会返回当前线程的中断状态，并清除它。接着，我们再返回到acquire()函数中调用parkAndCheckInterrupt的地方: 12if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; 可见，如果Thread.interrupted()返回true，则 parkAndCheckInterrupt()就返回true, if条件成立，interrupted状态将设为true;如果Thread.interrupted()返回false, 则 interrupted 仍为false。 再接下来我们重新进入for(;;)循环，进行新一轮的抢锁。 假设这次我们抢到了，我们将从return interrupted处返回，返回到哪里呢？ 当然是acquireQueued的调用处啦: 1234public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} 我们看到，如果acquireQueued的返回值为true, 我们将执行 selfInterrupt(): 123static void selfInterrupt() { Thread.currentThread().interrupt();} 而它的作用，就是中断当前线程。 为什么要这样做呢？ 从上面的代码中我们知道，即使线程在等待资源的过程中被中断唤醒，它还是会不依不饶的再抢锁，直到它抢到锁为止。也就是说，它是不响应这个中断的，仅仅是记录下自己被人中断过。 最后，当它抢到锁返回了，如果它发现自己曾经被中断过，它就再中断自己一次，将这个中断补上。 终于写完了AQS独占锁的获取与释放，接下来我们看看共享锁。其实只要真正看懂了独占锁，共享锁其实也是很容易看懂的。 在独占锁模式中，我们只有在获取了独占锁的节点释放锁时，才会唤醒后继节点。然而，在共享锁模式下，当一个节点获取到了共享锁，我们在获取成功后就可以唤醒后继节点了，而不需要等到该节点释放锁的时候。因此，在共享锁模式下，在获取锁和释放锁结束时，都会唤醒后继节点。 3.3共享式同步状态获取 acquireShared(int) 此方法是共享模式下线程获取共享资源的顶层入口。它会获取指定量的资源，获取成功则直接返回，获取失败则进入等待队列，直到获取到资源为止，整个过程忽略中断。下面是acquireShared()的源码： 1234public final void acquireShared(int arg) { if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);} 3.3.1tryAcquireShared() 123protected int tryAcquireShared(int arg) { throw new UnsupportedOperationException();} 这个方法和tryAcquire方法一样，也是由自定义同步器来实现，只是需要注意它提到的返回值： 如果该值小于0，则代表当前线程获取共享锁失败 如果该值大于0，则代表当前线程获取共享锁成功，并且接下来其他线程尝试获取共享锁的行为很可能成功 如果该值等于0，则代表当前线程获取共享锁成功，但是接下来其他线程尝试获取共享锁的行为会失败 从上面可以看到只要返回值大于0，就表示获取锁成功 接下来我们看看doAcquireShared方法，它对应于独占锁的acquireQueued()方法，两者其实很类似，我们这里只关注不同的部分 3.3.2doAcquireShared() 123456789101112131415161718192021222324252627282930private void doAcquireShared(int arg) { //对应于acquireQueued参数的addWaiter方法 final Node node = addWaiter(Node.SHARED); //对应于acquireQueued方法 boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); //成功拿到资源 if (r &gt;= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 这里第一点不同就是独占锁的acquireQueued调用的是addWaiter(Node.EXCLUSIVE)，而共享锁调用的是addWaiter(Node.SHARED)，表明了该节点处于共享模式，这两种模式的定义为： 1234/** Marker to indicate a node is waiting in shared mode */static final Node SHARED = new Node();/** Marker to indicate a node is waiting in exclusive mode */static final Node EXCLUSIVE = null; 该模式被赋值给了节点的nextWaiter属性： 1234Node(Thread thread, Node mode) { // Used by addWaiter this.nextWaiter = mode; this.thread = thread;} 我们知道在条件队列中，nextWaiter是指向条件队列中的下一个节点的，它将条件队列中的节点串起来，构成了单链表。但是在同步队列中，我们只用prev,next属性来串联节点，形成双向链表，nextWaiter属性在这里只起到一个标记作用，不会串联节点，这里不要被Node SHARED = new Node()所指向的空节点迷惑，这个空节点仅仅用作判断节点是否处于共享模式的依据 1234// Node#isShard()final boolean isShared() { return nextWaiter == SHARED;} 这里的第二点不同就在于获取锁成功后的行为，对于独占锁而言，是直接调用了setHead(node)方法，而共享锁调用的是setHeadAndPropagate(node, r)： 1234567891011private void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below setHead(node); if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); }} 在该方法内部我们不仅调用了setHead(node)，还在一定条件下调用了doReleaseShared()来唤醒后继的节点。这是因为在共享锁模式下，锁可以被多个线程所共同持有，既然当前线程已经拿到共享锁了，那么就可以直接通知后继节点来拿锁，而不必等待锁被释放的时候再通知。 这个doReleaseShared方法，我们到下面分析锁释放的时候再看。 3.4共享式同步状态释放 releaseShared(int) 上一小节已经把acquireShared()说完了，这一小节就来讲讲它的反操作releaseShared()吧。此方法是共享模式下线程释放共享资源的顶层入口。它会释放指定量的资源，如果成功释放且允许唤醒等待线程，它会唤醒等待队列里的其他线程来获取资源。 1234567public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false;} 3.4.1tryReleaseShared() 123protected boolean tryReleaseShared(int arg) { throw new UnsupportedOperationException();} 和之前的tryRelease()方法一样，需要自定义同步器实现，这里就不多提了。主要看下doReleaseShared()这个方法。 3.4.2doReleaseShared() 在共享锁模式下，头节点就是持有共享锁的节点，在它释放共享锁后，它也应该唤醒它的后继节点，但是值得注意的是，我们在之前doAcquiredShared()方法中的setHeadAndPropagate方法中可能已经调用过该方法了，也就是说它可能会被同一个头节点调用两次，也有可能在我们从releaseShared方法中调用它时，当前的头节点已经易主了，下面我们就来详细看看这个方法： 123456789101112131415161718 private void doReleaseShared() { for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; }} 该方法可能是共享锁模式最难理解的方法了，在看该方法时，我们需要明确以下几个问题： (1) 该方法有几处调用？ 该方法有两处调用，一处在acquireShared方法的末尾，当线程成功获取到共享锁后，在一定条件下调用该方法；一处在releaseShared方法中，当线程释放共享锁的时候调用。 (2) 调用该方法的线程是谁？ 在独占锁中，只有获取了锁的线程才能调用release释放锁，因此调用unparkSuccessor(h)唤醒后继节点的必然是持有锁的线程，该线程可看做是当前的头节点(虽然在setHead方法中已经将头节点的thread属性设为了null，但是这个头节点曾经代表的就是这个线程) 在共享锁中，持有共享锁的线程可以有多个，这些线程都可以调用releaseShared方法释放锁；而这些线程想要获得共享锁，则它们必然曾经成为过头节点，或者就是现在的头节点。因此，如果是在releaseShared方法中调用的doReleaseShared，可能此时调用方法的线程已经不是头节点所代表的线程了，头节点可能已经被易主好几次了。 (3) 调用该方法的目的是什么？ 无论是在acquireShared中调用，还是在releaseShared方法中调用，该方法的目的都是在当前共享锁是可获取的状态时，唤醒head节点的下一个节点。这一点看上去和独占锁似乎一样，但是它们的一个重要的差别是——在共享锁中，当头节点发生变化时，是会回到循环中再立即唤醒head节点的下一个节点的。也就是说，在当前节点完成唤醒后继节点的任务之后将要退出时，如果发现被唤醒后继节点已经成为了新的头节点，则会立即触发唤醒head节点的下一个节点的操作，如此周而复始。 (4) 退出该方法的条件是什么 该方法是一个自旋操作for(;; )，退出该方法的唯一办法是走最后的break语句： 12if (h == head) // loop if head changed break; 即，只有在当前head没有易主时，才会退出，否则继续循环。 这个怎么理解呢？为了说明问题，这里我们假设目前sync queue队列中依次排列有 dummy node -&gt; A -&gt; B -&gt; C -&gt; D 现在假设A已经拿到了共享锁，则它将成为新的dummy node， dummy node (A) -&gt; B -&gt; C -&gt; D 此时，A线程会调用doReleaseShared，我们写做doReleaseShared[A]，在该方法中将唤醒后继的节点B，它很快获得了共享锁，成为了新的头节点： dummy node (B) -&gt; C -&gt; D 此时，B线程也会调用doReleaseShared，我们写做doReleaseShared[B]，在该方法中将唤醒后继的节点C，但是别忘了，在doReleaseShared[B]调用的时候，doReleaseShared[A]还没运行结束呢，当它运行到if(h == head)时，发现头节点现在已经变了，所以它将继续回到for循环中，与此同时，doReleaseShared[B]也没闲着，它在执行过程中也进入到了for循环中。。。 由此可见，我们这里形成了一个doReleaseShared的”调用风暴“，大量的线程在同时执行doReleaseShared，这极大地加速了唤醒后继节点的速度，提升了效率，同时该方法内部的CAS操作又保证了多个线程同时唤醒一个节点时，只有一个线程能操作成功。 那如果这里doReleaseShared[A]执行结束时，节点B还没有成为新的头节点时，doReleaseShared[A]方法不就退出了吗？是的，但即使这样也没有关系，因为它已经成功唤醒了线程B，即使doReleaseShared[A]退出了，当B线程成为新的头节点时，doReleaseShared[B]就开始执行了，它也会负责唤醒后继节点的，这样即使变成这种每个节点只唤醒自己后继节点的模式，从功能上讲，最终也可以实现唤醒所有等待共享锁的节点的目的，只是效率上没有之前的“调用风暴”快。 由此我们知道，这里的“调用风暴”事实上是一个优化操作，因为在我们执行到该方法的末尾的时候，unparkSuccessor基本上已经被调用过了，而由于现在是共享锁模式，所以被唤醒的后继节点极有可能已经获取到了共享锁，成为了新的head节点，当它成为新的head节点后，它可能还是要在setHeadAndPropagate方法中调用doReleaseShared唤醒它的后继节点。 详细请参考这篇文章https://segmentfault.com/a/1190000016447307 总结 共享锁的调用框架和独占锁很相似，它们最大的不同在于获取锁的逻辑——共享锁可以被多个线程同时持有，而独占锁同一时刻只能被一个线程持有。 由于共享锁同一时刻可以被多个线程持有，因此当头节点获取到共享锁时，可以立即唤醒后继节点来争锁，而不必等到释放锁的时候。因此，共享锁触发唤醒后继节点的行为可能有两处，一处在当前节点成功获得共享锁后，一处在当前节点释放共享锁后。","link":"/2020/03/21/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8BAQS%E5%88%9D%E6%8E%A2/"},{"title":"ConcurrentHashMap源码解析","text":"主要内容 逐行分析下面5部分的源码解析 Node节点 hash方法spread() 构造方法 put方法 rehash方法tryPresize() Node结点JDK1.8中，和HashMap一样，也是使用Node节点存储键值对。不过与HashMap不同的是由于是并发容器，所以value值和next指针使用volatile修饰，保证线程之间的可见性。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.val = val; this.next = next; } ...} hash方法123static final int spread(int h) { return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;} 1static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash 这里和HashMap的hash方法相同都是hashcode的高16位和低16位进行异或运算，防止扰动，只不过这里又和HASH_BITS进行相与，这里相与最后的结果是保证了最高位的1个bit位总是0。 这里，我并没有明白它的意图，仅仅是保证计算出来的hash值不超过 Integer 最大值，且不为负数吗。 同 HashMap 的hash 方法对比一下，会发现连源码注释都是相同的，并没有多说明其它的。我个人认为意义不大，因为最后 hash 是为了和 capacity -1 做与运算，而 capacity 最大值为 1&lt;&lt;30，即 0100 0000 0000 0000 0000 0000 0000 0000 ，减1为 0011 1111 1111 1111 1111 1111 1111 1111。即使 hash 最高位为 1(无所谓0)，也不影响最后的结果，最高位也总会是0. 构造方法了解了上面的基本的方法，我们先来看下ConcurrentHashMap的构造方法和一些主要参数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class ConcurrentHashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements ConcurrentMap&lt;K,V&gt;, Serializable { private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; private static final int DEFAULT_CAPACITY = 16; private static final float LOAD_FACTOR = 0.75f; static final int TREEIFY_THRESHOLD = 8; static final int UNTREEIFY_THRESHOLD = 6; static final int MIN_TREEIFY_CAPACITY = 64; //上面这些变量和HashMap的完全一样，这里就不说了 //主要看看下面这些和并发有关的变量 //数组初始化大小和扩容时的大小控制 //-1表示数组将被初始化 //-(1+活动线程数n)表示n个线程在调整大小 private transient volatile int sizeCtl; //默认并发级别，未使用，是为了兼容之前的版本 private static final int DEFAULT_CONCURRENCY_LEVEL = 16; //定义的数组 transient volatile Node&lt;K,V&gt;[] table; //定义的扩容之后的数组 private transient volatile Node&lt;K,V&gt;[] nextTable; //空构造函数 public ConcurrentHashMap() { } //指定初始容量的构造函数 //同样用tableSizeFor去计算大于初始容量的最小的2的整数次幂 //注意这里最终是将容量赋值给了sizeCtl public ConcurrentHashMap(int initialCapacity) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap; } //指定初始容量和负载因子 //调用的是下面的构造函数 public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, 1); } //指定初始容量、负载因子和并发级别 public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (initialCapacity &lt; concurrencyLevel) // Use at least as many bins initialCapacity = concurrencyLevel; // as estimated threads long size = (long)(1.0 + (long)initialCapacity / loadFactor); int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); //同样是赋值给了sizeCtl this.sizeCtl = cap; } //通过已有Map构建ConcurrentHashMap public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) { this.sizeCtl = DEFAULT_CAPACITY; putAll(m); } 总结：和HashMap一样，ConcurrentHashMap的5个构造函数初始化时并没有初始化table。并且也只是初始化了sizeCtl和loadFactor这两个变量。 put方法put方法是ConcurrentHashMap的核心方法，我们接下来看看它是如何实现的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public V put(K key, V value) { return putVal(key, value, false); //false表示覆盖原有值}/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; //实际初始化table是放在了put操作中，用时再初始化类似于懒加载 if (tab == null || (n = tab.length) == 0) tab = initTable(); //通过Unsafe类硬件安全的判断主内存目标桶内是否为null //null则新建节点并通过CAS放到目标位置 //这里没有用锁 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) break; } //如果检测到数组某个节点的hash值是MOVED，则表示正在进行数组扩容的数据复制阶段 //则当前线程也会参与去复制，通过允许多线程复制，来减少数组的复制所带来的性能损失 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //到这里说明是目标节点有值，且不是数组扩容 else { V oldVal = null; //这里用的锁为f这个桶的第一个节点，而不是锁定整张表，提高了并发性 synchronized (f) { if (tabAt(tab, i) == f) { //是链表的话 if (fh &gt;= 0) { binCount = 1; //循环链表 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; //找到hash和equals都相同的则替换 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } //否则则加入链表尾部 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } //到这里说明是树节点 //通过putTreeVal添加到红黑树 else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } //这里已经没有加锁了 //判断链表是否需要树化 //并且返回oldValue if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount);//计数 return null;} 这里我们简单看下initTable()的源码: 1234567891011121314151617181920212223242526private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { //sizeCtl小于0，说明别的线程正在进行初始化让出执行权 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin //到这里说明sizeCtl&gt;0,则直接初始化 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { //sizeCtl&gt;0则初始化一个sizeCtl的数组，否则初始化默认长度(16)数组 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; //sizeCtl设置为数组长度的3/4 sc = n - (n &gt;&gt;&gt; 2); } } finally { sizeCtl = sc; } break; } } return tab;} 可以看到初始化数组table的流程如下： 如果sizeCtl小于0，说明别的线程正在进行初始化，则让出执行权 如果sizeCtl大于0的话，则通过CAS初始化一个大小为sc的数组或者一个默认大小(16)的数组 然后设置sizeCtl的值为数组长度的3/4 到这里我们总结下put方法的流程： 当添加一对键值对的时候，首先会去判断保存这些键值对的数组是不是初始化了，如果没有的话就初始化数组 然后通过计算hash值来确定放在数组的哪个位置，如果这个位置为空则通过CAS直接添加。 如果不为空的话，则判断该节点的hash值是不是MOVED(-1)，是的话则表示当前正在对这个数组进行扩容，复制到新的数组，则当前线程也去帮助复制。 最后一种情况就是，如果这个节点，不为空，也不在扩容，则通过synchronized来对目标位置的节点进行加锁（注意这里只是对节点加锁），进行添加操作 然后判断当前取出的节点位置存放的是链表还是树 如果是链表的话，则遍历整个链表，直到取出来的节点的key来个要放的key进行比较，如果key相等，并且key的hash值也相等的话，则说明是同一个key，则覆盖掉value，否则的话则添加到链表的末尾 如果是树的话，则调用putTreeVal方法把这个元素添加到树中去,在添加完成之后，会判断在该节点处共有多少个节点（注意是添加前的个数），如果达到8个以上了的话，则调用treeifyBin方法来尝试将处的链表转为树(同样使用synchronized加锁，并且和添加时使用的是同时一把锁)，或者扩容数组为原大小的一倍 最后整个数量+1,判断是否需要进行扩容 数组扩容从上面的putVal方法中我们可以知道,数组扩容发生在两个位置: 在往map中添加元素的时候，在某一个节点的数目已经超过了8个，同时数组的长度又小于64的时候，才会触发数组的扩容。即上面的treefyBin函数中。 当数组中元素达到了sizeCtl的数量的时候，则会调用transfer方法来进行扩容。我们最后的addCount函数中。 这两个函数中的扩容方法就不贴了，只需要知道最终会扩容成原来数组的一倍。然后会调用我们的transfer方法进行数据迁移。这里主要看下数据迁移的过程: 这里还是先看下迁移的示意图，帮助理解。 为了方便，上边以原数组长度 8 为例。在元素迁移的时候，所有线程都遵循从后向前推进的规则，即如图A线程是第一个进来的线程，会从下标为7的位置，开始迁移数据。 而且当前线程迁移时会确定一个范围，限定它此次迁移的数据范围，如图 A 线程只能迁移 bound=6到 i=7 这两个数据。 此时，其它线程就不能迁移这部分数据了，只能继续向前推进，寻找其它可以迁移的数据范围，且每次推进的步长为固定值 stride（此处假设为2）。如图中 B线程发现 A 线程正在迁移6,7的数据，因此只能向前寻找，然后迁移 bound=4 到 i=5 的这两个数据。 当每个线程迁移完成它的范围内数据时，都会继续向前推进。那什么时候是个头呢？ 这就需要维护一个全局的变量 transferIndex，来表示所有线程总共推进到的元素下标位置。如图，线程 A 第一次迁移成功后又向前推进，然后迁移2,3 的数据。此时，若没有其他线程在帮助迁移，则 transferIndex 即为2。 剩余部分等待下一个线程来迁移，或者有任何的 A 和B线程已经迁移完成，也可以推进到这里帮助迁移。直到 transferIndex=0 。（会做一些其他校验来判断是否迁移全部完成，看代码）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171/*** Moves and/or copies the nodes in each bin to new table. See* above for explanation.*/private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { int n = tab.length, stride; //根据当前CPU核心数，确定每次推进的步长，最小值为16.（为了方便我们以2为例） if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range //初始化新数组长度为原数组的两倍 if (nextTab == null) { // initiating try { @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } //用nextTable指代新数组 nextTable = nextTab; //推进标志位初始化为原数组长度(以16为例) transferIndex = n; } //新数组长度 int nextn = nextTab.length; //创建一个标志类用来表示当前桶中的元素已经全部迁移完成 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); //是否向前推进标志 boolean advance = true; //是否所有线程都完成迁移标志 boolean finishing = false; // to ensure sweep before committing nextTab //i 代表当前线程正在迁移的桶的下标，bound代表它本次可以迁移的范围下限 for (int i = 0, bound = 0;;) { Node&lt;K,V&gt; f; int fh; while (advance) { int nextIndex, nextBound; //(1) 先看 (3) 。i每次自减 1，直到 bound。若超过bound范围，或者finishing标志为true，则不用向前推进。 //若未全部完成迁移，且 i 并未走到 bound，则跳转到 (7)，处理当前桶的元素迁移。 if (--i &gt;= bound || finishing) advance = false; //(2) 每次执行，都会把 transferIndex 最新的值同步给 nextIndex //若 transferIndex小于等于0，则说明原数组中的每个桶位置，都有线程在处理迁移了， //于是，需要跳出while循环，并把 i设为 -1，以跳转到④判断在处理的线程是否已经全部完成。 else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } //(3) 第一个线程会先走到这里，确定它的数据迁移范围。(2)处会更新 nextIndex为 transferIndex 的最新值 //因此第一次 nextIndex=n=16，nextBound代表当次迁移的数据范围下限，减去步长即可， //所以，第一次时，nextIndex=16，nextBound=16-2=14。后续，每次都会间隔一个步长。 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } //(4) if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; //若全部线程迁移完成 //更新table为新表 //扩容阈值改为原来数组长度的 3/2 ，即新长度的 3/4，也就是新数组长度的0.75倍 if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; } //到这，说明当前线程已经完成了自己的所有迁移（无论参与了几次迁移）， //则把 sc 减1，表明参与扩容的线程数减少 1。 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; //这里非常有意思，会把 i 从 -1 修改为16， //目的就是，让 i 再从后向前扫描一遍数组，检查是否所有的桶都已被迁移完成，参看 (6) i = n; // recheck before commit } } //(5) 若i的位置元素为空，则说明当前桶的元素已经被迁移完成，就把头结点设置为fwd标志 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //(6) 若当前桶的头结点是 ForwardingNode ，说明迁移完成，则向前推进 else if ((fh = f.hash) == MOVED) advance = true; // already processed //(7) 处理当前桶的数据迁移。 else { //同样需要给头结点加锁 synchronized (f) { if (tabAt(tab, i) == f) { Node&lt;K,V&gt; ln, hn; //两个链表头结点 //说明是链表节点 if (fh &gt;= 0) { //hash和原数组长度相与 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; //找到第一个节点后面b全部相同的节点并赋值给lastRun for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } //lastRun之后的节点直接迁移 if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } //之前的节点一个个的迁移，注意迁移过去后是头插法 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } //到这里说明是树节点 else if (f instanceof TreeBin) { TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } }} 链表迁移过程： 会分成两条链表，一条链表和原来的下标相同，另一条链表是原来的下标加数组长度的位置 然后找到 lastRun 节点，从它到尾结点整体迁移。lastRun前边的节点则单个迁移，但是需要注意的是，这里是头插法。 最后会有两条链表，一条链表从 lastRun到尾结点是正序的，而lastRun之前的元素是倒序的，另外一条链表，从头结点开始就是倒叙的。看下图。 对于红黑树的迁移，这里先放一下。 get方法看完了put方法，get方法就非常简单了。 123456789101112131415161718192021222324public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //首先先得到key的hash值，来确定桶的位置 int h = spread(key.hashCode()); //桶中有元素 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { //桶中第一个节点就是，则直接返回 if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } //桶中第一个节点是树节点，则搜索红黑树返回 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //否则，则遍历链表返回 while ((e = e.next) != null) { if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null;} 总结 JDK1.7中ConcurrentHashMap使用分段锁技术，1.8中使用 CAS+synchronized 关键字保证线程安全。 数组的初始化放在put操作时，初始化tab时，通过CAS使得sc为-1，保证只有一个线程进行初始化。 之后开始添加数据 如果桶位置为空，则直接通过CAS进行添加。 如果桶位置的sc=-1，则表示正在进行扩容，则当前线程也去帮助进行扩容。 桶位置不为空且没有在扩容，使用synchronized关键字锁住桶位置的第一个节点,然后判断是链表还是红黑树，链表则添加到尾部，红黑树则直接添加。 添加完成后，有两处位置会发生数组扩容 在往map中添加元素的时候，在某一个节点的数目已经超过了8个，同时数组的长度又小于64的时候，才会触发数组的扩容。即上面的treefyBin函数中。 当数组中元素达到了sizeCtl的数量的时候，则会调用transfer方法来进行扩容。我们最后的addCount函数中。 扩容主要是通过transfer()函数进行，最终数组长度是原来数组长度的一倍。并且整个数据迁移过程下: 首先会有多个线程进行数据迁移，第一个线程从数组末尾开始，每一个线程都会从后往前推进一个固定的步长(最小为16)。 迁移过程中需要使用synchronized关键字锁住桶位置第一个节点。 对于链表迁移同样是拆分成两个链表，每个位置元素迁移后要么是在原位置，要么是原索引+原数组长度位置处。首先从桶中第一个元素开始遍历，找到lastRun节点，它之后所有的节点的hash&amp;原数组长度都相同。然后 lastRun 节点和之后的元素整体迁移，(根据是否与runBit相同，确定是在ln还是hn)。lastRun之前的节点则根据是否与runBit相同，添加到响应位置，注意这里是头插法。","link":"/2020/03/26/ConcurrentHashMap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"Java多线程之线程池","text":"主要内容 线程池的概念 线程池的基本使用 线程池的7大参数 线程池工作原理 使用自定义线程池 线程池基本概念什么是线程池 线程池可以看做是一个线程的集合。它的工作主要是控制运行的线程数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量超出数量的线程排队等候，等其它线程执行完毕，再从队列中取出任务来执行。 主要特点：线程复用、控制最大并发数、管理线程 为什么使用线程池 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。 线程池的基本使用这里，我们先对这张图有点印象，只需要知道ThreadPoolExecutor是线程池的核心即可。剩下的放到Executor框架来讲。 使用线程池基本上有3步: 新建线程池 向线程池提交任务 关闭线程池 这里我们先写个demo感受一下： 1234567891011121314151617181920212223242526package com.company.MultiThread;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadPoolDemo { public static void main(String[] args) { //新建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(5); //ExecutorService threadPool = Executors.newSingleThreadExecutor(); //ExecutorService threadPool = Executors.newCachedThreadPool(); try { for (int i = 0; i &lt; 10; i++) { //向线程池提交任务 threadPool.execute(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"办理业务\"); }); } } catch (Exception e) { e.printStackTrace(); } finally { //关闭线程池 threadPool.shutdown(); } }} 1. 新建线程池 我们可以通过两种方式新建线程池： JDK提供的线程池 使用自定义线程池 JDK提供的线程池主要有以下3个： newFixedThreadPool() 固定线程数线程数 newSingleThreadExecutor() 单个线程的线程池 newCachedThreadPool() 根据需求创建线程的线程池 我们用上面的代码验证下这3个线程池有什么不同： 从左到右依次为newFixedThreadPool、newSingleThreadExecutor、newCachedThreadPool()。所有线程池都是添加了10个任务。其中newFixedThreadPool()中设置线程数为5,从最左边的图可以看到5个线程处理完10个任务;newSingleThreadExecutor()只有1个线程处理10个任务;而newCachedThreadPool()中则是直接创建了10个线程完成了10个任务 注意 注意注意 注意注意 注意 虽然上面使用的是JDK给我们提供的线程池，但是阿里巴巴Java开发手册中明确要求实际使用时我们应该使用自定义线程池。原因和示例代码参考下面的使用自定义线程池章节。 2. 向线程池提交任务 可以使用两个方法向线程池提交任务： execute() 用于提交不需要返回值的任务 submit() 用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象的get()方法来获取返回值。 3. 关闭线程池 关闭线程池有两种方法： shutdown shutdownNow 区别:shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表。shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。 它们的原理都是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。 线程池七大参数上面我们讲到了JDK提供的3种线程池，我们看下他们的源码： 从上面的源码中我们知道，创建3种线程池时，实际上返回的是一个ThreadPoolExecutor,另外我们也看到，虽然3种不同类型线程池传递的参数值不同，但都是传递了5个参数。 那为什么我们会说7大参数呢？我们看下上面使用的ThreadPool的构造方法： 从源码可以看到实际上是通过this调用了另一个构造方法，并且传递了7个参数。 接下来我们就好好聊聊这7个参数的意义： 参数 意义 corePoolSize 线程池中的常驻核心数 maximumPoolSize 线程池允许的最大线程数。此值必须大于等于1 keepAliveTime 多余的空闲线程的存活时间。当前线程池数量超过corePoolSize时，并且空闲时间达到keepAliveTime时，多余空闲线程会被销毁直至只剩下corePoolSize个线程为止 unit keepAliveTime的单位 workQueue 被提交但尚未被执行的任务会被分配到任务队列任务队列，就是我们前面学习的阻塞队列。 threadFactory 生成线程池中工作线程的线程工厂，用于创建线程，一般用默认的即可 RejectedExecutionHandler 拒绝策略(共4种)，当队列满了并且工作线程大于等于线程池的最大线程数时, 四种拒绝策略： AbortPolicy：(系统默认拒绝策略)。直接抛出RejectedExecutionException异常阻止系统正常运行。 CallerRunsPolicy：”调用者运行”一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新的任务流量。 DiscardOldestPolicy：丢弃队列里等待最久的一个任务，然后把当前任务加入到队列中尝试再次提交当前任务。 DiscardPolicy：直接丢弃任务，不处理也不抛出异常。如果允许任务丢失，这是最好的一种方案 代码验证见下面的自定义线程池部分。 线程池的工作原理只是这几个参数可能还不能很好的理解，我们接下来结合线程池的工作原理来看看这几个参数的意义。 当提交一个新任务到线程池时，线程池的处理流程如下 再结合上面的参数，ThreadPoolExecutor执行execute方法可以分下面4种情况 1）如果当前运行的线程少于corePoolSize，则创建新线程来执行任务（注意，执行这一步骤需要获取全局锁）2）如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue。3）如果无法将任务加入BlockingQueue（队列已满），则扩展新的线程来处理任务（注意，执行这一步骤需要获取全局锁）4）如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将通过拒绝策略被拒绝 另外对于keepAliveTime这时候我们也就清楚了，就是我们的核心线程池和阻塞队列都已经满了之后，线程池会新建线程执行任务，这些新建的线程执行完任务后处于空闲状态，经过keepAliveTime后会被销毁，直到只剩下corePoolSize个线程。 使用自定义线程池注意 注意注意 注意注意 注意 阿里巴巴Java开发手册中明确规定：不能使用Executors方法创建线程池，应该使用自定义线程池 图片来自《阿里巴巴Java开发手册1.4.0》 自定义线程池原因 原因上面也提到了，我们再贴一下JDK提供的3种线程的代码 可以看到newFixedThreadPool和newSingleThreadExecutor中使用的是没有指明参数的LinkedBlockingQueue&lt;&gt;()，它是一个无界阻塞队列，即最大可以存放Integer.Max_VALUE的任务队列，导致产生OOM异常。这也是为什么我们使用自定义线程池时不能使用无界阻塞队列的原因。而newCachedThreadPool中，虽然使用的是SynchronousQueue(),但是它的最大线程数设置的同样是Integer.MAX_VALUE,所以当线程过多时同样会产生OOM异常。 使用自定义线程池 接下来我们使用自定义线程池，并且验证下上面提到的4种拒绝策略 1234567891011121314151617181920212223242526272829303132333435package com.company.MultiThread;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class ThreadPoolDemo { public static void main(String[] args) { //新建线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor( 3, //核心线程数3 6, //最大线程数6 2L, //扩展线程保持时间2ms TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;&gt;(3), //任务队列容量3 Executors.defaultThreadFactory(), // new ThreadPoolExecutor.AbortPolicy()); //AbortPolicy拒绝策略抛异常 //new ThreadPoolExecutor.CallerRunsPolicy() //new ThreadPoolExecutor.DiscardOldestPolicy() //new ThreadPoolExecutor.DiscardPolicy() try { for (int i = 0; i &lt; 10; i++) { //i&lt;8 i&lt;9 i&lt;10 threadPool.execute(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"办理业务\"); }); } } catch (Exception e) { e.printStackTrace(); } finally { threadPool.shutdown(); } }} 从上面设置的参数我们知道，我们这个自定义的线程池最大容量为6+3=9。 AbortPolocy策略 当提交8个任务时，线程池正常处理完8个任务。当提交9个任务时，线程池也正常处理完9个任务。当提交10个任务，超过最大容量9时AbortPolocy拒绝策略则会直接抛出RejectedExecutionException异常 剩下的3个我们只看拒绝策略的效果(即使用10个线程i&lt;10)时的效果 CallerRunsPolicy 任务回退拒绝策略可以看到当提交10个任务超过最大容量9时CallerRunsPolicy拒绝策略则会自己执行9个任务，多出的任务回退给调用者，我们上面的代码中是main线程，所以是main线程执行多出的任务。 DiscardOldestPolicy 丢弃最久拒绝策略 可以看到当提交10个任务超过最大容量9时DiscardOldestPolicy拒绝策略则会执行9个任务，丢弃掉等待最久的任务。 DiscardPolicy 直接丢弃拒绝策略 可以看到当提交10个任务超过最大容量9时DiscardPolicy拒绝策略则会执行9个任务，直接丢弃新的任务。 自定义线程池参数设置 上面我们写的自定义线程池中的参数都是自己随便设置的，实际生产中对于参数的设置是有要求的，那么如何设置一个合理的参数呢？(这里最重要的也是最大线程数即maximumPoolSize)，这里就先讲下这个参数怎么配 首先通过Runtime.getRuntime().availableProcessors()获得设备的CPU数。 其次分析任务特性，可以从以下几个角度来分析。 任务的性质：CPU密集型任务、IO密集型任务和混合型任务。 任务的优先级：高、中和低。 任务的执行时间：长、中和短。 任务的依赖性：是否依赖其他系统资源，如数据库连接。 CPU密集型任务:指的是该任务需要大量的运算，而没有阻塞，CPU一直全速运行 IO密集型任务:该任务需要大量IO，即大量的阻塞。IO密集型任务并不是一直在执行任务，则应配置尽可能多的线程 混合密集型任务：可以拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大。如果这两个任务执行时间相差太大，则没必要进行分解。 配置合理参数 CPU密集型任务配置为CPU核数+1，比如8核CPU则应配置最大线程数为8+1=9个 IO密集型任务配置为2*CPU核数 或 CPU核数/(1-阻塞因子) 阻塞因子为0.8~0.9。 比如对于8核CPU则应配置最大线程数为2*8=16个或者8/(1-0.9)=80个。","link":"/2020/03/31/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"Lock锁的子类ReentrantLock和ReadWriteLock源码解析","text":"主要内容 ReentrantLock概述 ReentrantLock源码解析 ReentrantReadWriteLock读写锁使用 ReentrantReadWriteLock源码解析 上一篇已经学过AQS了，因此本篇主要是讲解Lock锁主要的两个子类是如何使用AQS实现相应的特性： ReentrantLock ReentrantReadWriteLock ReentrantLock概述我们这里还是看下它的顶部注释，总结下它的特点： 和synchronized一样都是互斥锁和可重入锁，但更加灵活。注意可重入锁也称为递归锁 可以调用isHeldByCurrentThread和getHoldCount方法判断当前线程是否拿到锁 可以在构造方法中使用true来让锁设置为公平锁,默认为非公平锁。但公平锁一般没有非公平锁效率高 同时也告诉了我们使用ReentrantLock的最标准方法 123456789101112131415class X { private final ReentrantLock lock = new ReentrantLock(); // ... public void m() { //try之前调用lock方法 lock.lock(); // block until condition holds try { // ... method body } finally { //finally中释放锁 lock.unlock() } }} 互斥锁和可重入锁之前已经提到过，这里介绍下公平锁和非公平锁： 公平锁 多个线程按照申请锁的顺序来获取锁 非公平锁 多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程有线获取锁。可能会造成优先级反转或者饥饿现象 ReentrantLock的用法已经在我的这篇文章中写了，这里就不提了。主要看看他底层是如何实现的。 ReentrantLock源码解析上面我们已经提到了，ReentrantLock实际上是由AQS实现的，我们接下来就看看ReentrantLock的源码。 Sync、NonfairSync、FairSync 因为ReentrantLock支持公平锁和非公平锁，所以它抽出了一个抽象类来继承AQS。 1private final Sync sync; 12345abstract static class Sync extends AbstractQueuedSynchronizer { abstract void lock(); //...} 可以看到，在Sync类中定义了一个抽象方法lock，该方法应当由继承它的子类来实现。它的子类实现则包括了我们上面提到的非公平锁和公平锁。其中，FairSync和NonfairSync的定义如下： 123static final class NonfairSync extends Sync { //实现方法先省略，下面讲} 123static final class FairSync extends Sync { //实现方法先省略，下面讲} 我们看一下ReentrantLock的构造方法，验证一下是不是这样。 123456789//默认为非公平锁public ReentrantLock() { sync = new NonfairSync();}//传入true可以使用公平锁public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync();} 可以看到ReerantLock中提供了两种构造方法，默认构造的是非公平锁，通过传入true可以使用公平锁。 非公平锁的lock()方法实现 首先我们先看下ReentrantLock的lock方法： 123public void lock() { sync.lock();} 可以看到lock()方法是调用了sync.lock()，从上面我们知道sync中的lock方法是抽象方法，具体实现是由公平锁和非公平锁自己实现的。 接下来我们就看看非公平锁的实现： 123456final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1);} 可以看到实际上ReentrantLock的非公平锁就是调用了AQS的acquire(1)。acquire()方法我们在上一篇已经详细讲过了，这里就不再重复。我们知道AQS是基于模板方法模式的，acquire方法中需要调用tryAcquire方法，使用AQS实现锁只需要重写tryAcquire方法即可，我们看看ReentrantLock的非公平锁是如何重写tryAcquire方法的。 123protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires);} 从上面可以看到tryAcquire方法实际上是调用的nonfairTryAcquire方法： 1234567891011121314151617181920212223242526272829final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); //没有线程占有锁 if (c == 0) { //直接尝试占有锁，体现非公平锁 if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } //上面体现了是互斥锁 //下面体现了可重入锁 //到这里说明有线程占有锁 //判断占有锁的线程是不是就是自己 else if (current == getExclusiveOwnerThread()) { //是自己则可以再次获取锁 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); //将state+1 setState(nextc); return true; } return false;} 公平锁的lock()方法实现 知道了非公平锁，公平锁也就十分简单了。 123456789101112131415161718192021222324final void lock() { acquire(1);}protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { //和非公平锁唯一的不同在于需要先去判断是否有线程排在自己前面 体现公平锁 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false;} 可以看到非公平锁和公平锁的实现基本相同，唯一的不同在于公平锁在抢锁时需要调用hasQueuedPredecessors方法先去判断是否有线程排在自己前面，而非公平锁是直接争锁，其他则完全一样。 unlock()方法 ReentrantLock的unlock()方法如下： 123public void unlock() { sync.release(1);} 可以看到非公平锁和公平锁使用的是都是AQS的release方法,同样我们看看ReentrantLock中tryRelease方法是如何重写的。 123456789101112131415protected final boolean tryRelease(int releases) { //state-1释放一次锁 int c = getState() - releases; //不是当前线程释放锁，则直接抛异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; //锁全部释放 if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free;} 可以看到tryRelease方法的实现也很简单, 总结 ReentrantLock的公平锁和非公平锁的lock方法实际上调用的是AQS中的acquire()方法，并且两种锁的tryAcquire方法的唯一不同在于公平锁需要判断前面是否有线程等待。 ReentrantLock的unlock()方法,实际上调用的是AQS的release()方法。 从上面可以看到AQS在并发编程中的地位。只要弄懂了它，我们在学习其他并发编程工具的时候就会容易很多。 ReentrantReadWriteLock读写锁概述 ReentrantReadWriteLock同样是位于J.U.C的子包locks包中，我们看下它的类结构图。 可以看到ReentrantReadWriteLock实现了ReadWriteLock接口。我们先看一下ReadWriteLock结口顶部的注释，总结下重点： ReadWriteLock维护一对关联的锁，一个用于只读操作(读锁)，一个用于写入操作(写锁)。 读锁是共享锁，写锁是独占锁。(读读高效 | 读写 写读 写写互斥) 获取到读锁的线程将看到在先前释放写锁时所做的所有更新。 与互斥锁相比，读写锁在对于共享数据的读操作时并发性更好。所以适用于读多于写的情况。 ReadWriteLock的接口的源码也非常简单: 1234public interface ReadWriteLock { Lock readLock(); Lock writeLock();} 知道了ReadWriteLock接口,我们再看下它的具体实现类ReentrantReadWriteLock，同样我们还是看下它的顶部注释，除了上面的特性外，ReentrantReadWriteLock还支持如下特性： 公平锁和非公平锁。和ReentrantLock一样，默认是非公平锁，同样可以通过加上true，使用公平锁。 可重入锁。支持重进入，以读写线程为例：读线程获取了读锁后能再次获取读锁。写线程在获取了写锁之后能再次获取写锁，同样也能再次获取读锁。 锁降级。写锁能够降级为读锁，读锁不能升级为写锁。写锁降级为读锁的方式： 1) 获得写锁 2) 获得读锁 3) 释放写锁 读写锁使用 最核心的用法就是在读操作时使用读锁，写操作时使用写锁。下面简单写一个缓存，用读写锁控制线程安全。缓存最基本的三个方法是get、put和clear 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.company.MultiThread;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantReadWriteLock;public class Cache { //HashMap非线程安全 private Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); //使用读写锁保证Cache是线程安全的 private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock(); //获取key对应的value 正常应该返回Object，这里只打印演示效果 public void get(String key){ //读操作使用读锁,并发访问该方法不会被阻塞 rwLock.readLock().lock(); try { System.out.println(Thread.currentThread().getName()+\"正在读取\"); Object res = map.get(key); TimeUnit.SECONDS.sleep(1); System.out.println(Thread.currentThread().getName()+\"读取到了\"+res); } catch (InterruptedException e) { e.printStackTrace(); } finally { rwLock.readLock().unlock(); } } //设置key对应的value,正常应该返回返回旧的value，这里只打印演示效果 public void put(String key,Object value){ //写操作使用写锁，其他线程均被阻塞 rwLock.writeLock().lock(); try { System.out.println(Thread.currentThread().getName()+\"正在写入\"+key); map.put(key,value); System.out.println(Thread.currentThread().getName()+\"写操作完成\"); } finally { rwLock.writeLock().unlock(); } }// //清空缓存中的所有内容// public void clear(){// wLock.lock();// try {// map.clear();// } finally {// wLock.unlock();// }// } public static void main(String[] args) { Cache cache = new Cache(); for (int i = 0; i &lt; 5; i++) { final int tempI = i; new Thread(()-&gt;{ cache.put(tempI+\"\",tempI); },String.valueOf(i)).start(); } for (int i = 0; i &lt; 5; i++) { final int tempI = i; new Thread(()-&gt;{ cache.get(tempI+\"\"); },String.valueOf(i)).start(); } }} 运行结果如下所示: 12345678910111213141516171819202122//可以看到写操作是独占锁，每次只能有一个线程进行写2正在写入22写操作完成1正在写入11写操作完成0正在写入00写操作完成4正在写入44写操作完成3正在写入33写操作完成//读操作是共享锁，5个线程都可以去读2正在读取3正在读取0正在读取4正在读取1正在读取1读取到了13读取到了30读取到了02读取到了24读取到了4 ReentrantReadWriteLock源码解析从下面的图中，可以看到ReentrantReadWriteLock中比ReentrantLock多了两个子类，就是我们ReentrantReadWriteLock中所维护的一对读锁和写锁。 Sync和ReentrantLock一样，也是继承了AQS,然后NonefairSync和FairSync分别继承Sync实现公平锁和非公平锁。这里就不再展开。我们主要看下我们的读锁ReadLock和写锁WriteLock是如何实现的。 锁的状态表示 之前的ReentrantLock是独占锁。所以state直接表示锁的状态即可。但是读写锁中有两把锁，如何只用state表示两种锁的状态呢？ReentrantReadWriteLock的源码中已经告诉了我们: 从上面可以看到是用锁的状态state(这里是c)的高16位表示读锁，低16位表示写锁。sharedCount返回的就是共享锁的数量，即读锁的数量，exclusiveCount返回的是互斥锁的数量，即写锁的数量。 写锁的lock方法 首先我们先看下写锁的源码部分： 12345678public static class WriteLock implements Lock, java.io.Serializable { private final Sync sync; protected WriteLock(ReentrantReadWriteLock lock) { sync = lock.sync; } public void lock() { sync.acquire(1);} 可以看到实际上WriteLock写锁的lock方法，实际上还是调用的AQS的acquire(1),同样调用acquire方法也说明了写锁是独占锁。接着我们看看它是如何实现tryAcquire()方法的。 12345678910111213141516171819202122232425262728protected final boolean tryAcquire(int acquires) { Thread current = Thread.currentThread(); //从上面锁的状态分析知道这里 //c为读锁和写锁的总状态 //w为写锁的状态 int c = getState(); int w = exclusiveCount(c); if (c != 0) { //读锁非0即其他线程正在读(写读互斥) //或者不是当前线程则失败 if (w == 0 || current != getExclusiveOwnerThread()) return false; //超过锁次数的最大限制2^16-1 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); //否则，拿到锁 setState(c + acquires); return true; } //到这里说明c=0,即没有锁 //直接通过CAS尝试占有锁 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;} 写锁的unLock方法 123public void unlock() { sync.release(1);} 同样WriteLock写锁的unLock方法，实际上还是调用的AQS的release(1),接着我们看看它是如何实现tryRelease()方法的。 12345678910111213protected final boolean tryRelease(int releases) { if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; //看写锁是否释放完(因为是可重入锁) boolean free = exclusiveCount(nextc) == 0; //释放完，则设置为无独占锁占用 if (free) setExclusiveOwnerThread(null); //否则则写锁状态-1 setState(nextc); return free;} 可以看到写锁的lock和unlock方法实际上还是比较简单的，我们接下来看下共享锁读锁的实现。 读锁的lock方法 123public void lock() { sync.acquireShared(1);} 这里可以看到实际上ReadLock读锁的lock方法，实际上是调用的AQS的acquireShared(1),同样调用acquireShared方法也说明了写锁是独占锁。接着我们看看它是如何实现tryAcquireShared()方法的。 12345678910111213141516171819202122232425262728293031323334protected final int tryAcquireShared(int unused) { Thread current = Thread.currentThread(); int c = getState(); //另一个线程持有写锁 则失败返回-1 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; int r = sharedCount(c); //获取读锁 if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; compareAndSetState(c, c + SHARED_UNIT)) { //下面这些代码先不用管 //这里是为了方法getReadHoldCount方法写的，目的是返回当前线程获取读锁的次数 //因为读状态是所有线程的state，而每个线程各自获取读锁的次数是能保存在ThreadLocal中 if (r == 0) { firstReader = current; firstReaderHoldCount = 1; } else if (firstReader == current) { firstReaderHoldCount++; } else { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; } //获取锁成功返回1 return 1; } //首次获取失败则重新尝试 return fullTryAcquireShared(current);} 这里我们只需要知道： 如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。 如果当前线程获取了写锁或者写锁未被获取，则当前线程通过CAS增加读状态，成功获取读锁。 读锁的unLock方法 123public void unlock() { sync.releaseShared(1);} 123456789101112131415161718192021222324252627282930protected final boolean tryReleaseShared(int unused) { Thread current = Thread.currentThread(); if (firstReader == current) { // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; } else { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) { readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); } --rh.count; } //上面不用管 //CAS自旋 for (;;) { int c = getState(); //c-1 int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) return nextc == 0; }} 读写锁目前我也只能理解这么多了，不过最终还是要把AQS搞懂啊。。不说了，我再去看两遍AQS的源码。","link":"/2020/03/26/Lock%E9%94%81%E7%9A%84%E5%AD%90%E7%B1%BBReentrantLock%E5%92%8CReadWriteLock/"},{"title":"SQL基本语法(二) 高级特性","text":"主要内容 索引、视图、存储过程、游标、触发器 MySQL事务管理 MySQL权限管理 索引、视图、存储过程、游标、触发器索引首先声明，索引这个东西十分复杂也十分重要，尤其是对于查询性能优化，写一本书都不过分，所以这里只写索引最最基本的使用。之后会详细介绍索引和查询性能优化。 索引是存储引擎用于快速找到记录的一种数据结构。主要作用就是在数据量比较大时，可以提高我们的查询效率。可以简单的把索引类比成我们书的目录。 索引主要包括： 单值索引：即一个索引只包含单个列，一个表可以有多个单值索引 唯一索引： 索引列的值必须唯一，但允许有空值。 复合索引： 即一个索引包含多个列。 索引的基本使用： 1234567891011# 创建索引-- 加UNIQUE则为唯一索引，不加就是单值索引CREATE [UNIQUE] INDEX indexName ON mytable(columname);-- 复合索引CREATE INDEX indexName ON mytable(col1,col2);# 删除索引DROP INDEX indexName ON mytable;# 查看索引SHOW INDEX FROM mytable; 视图视图是一个虚表，即视图包含的不是数据而是根据需要检索数据的SQL语句。视图提供了一种封装SELECT语句的层次，可用来简化数据处理，重新格式化和保护基础数据。 使用视图的好处： 重用SQL语句，对于复杂点的SQL语句将它创建为视图后可以方便的重用 因为视图可以使用表的一部分而不是整个表，所以可以保护原来表中的一些重要数据。 视图可以更改数据格式和表示。 视图的基本使用 12345678910111213141516171819202122232425# 基本模板CREATE VIEW myview AS-- SELECT语句# 使用视图简化复杂联结CREATE VIEW productcustomers ASSELECT c.cust_name,c.cust_contact,oi.prod_idFROM customers AS cINNER JOIN orders AS oON c.cust_id = o.cust_idINNER JOIN orderitems AS oiON o.order_num = oi.order_num;-- 这时候就可以直接查询不用再写上面复杂的联结SELECT * FROM productcustomers;# 使用视图保护表中数据-- 只给出3列数据，products表中其他数据不给CREATE VIEW productsview ASSELECT prod_id,prod_name,vend_idFROM products;-- 这时候只有上面的3列数据SELECT * FROM productsview;-- 可以看到每个VIEW的作用取决于SELECT语句是如何定义的 更新视图上面的都是对视图进行查询，我们之前提到过可以把视图可以看成表来进行操作，那么如果对视图进行增删改，原来的表中的数据会发生变化吗？ 通常，视图是可更新的（即，可以对它们使用 INSERT、UPDATE 和 DELETE ），更新一个视图将更新其基表。 并非所有视图都是可更新的。如果MySQL不能正确地确定被更新的基数据，则不允许更新（包括插入和删除） 比如如果SELECT语句中使用了下面这些条件就不能进行视图的更新： 分组（使用 GROUP BY 和 HAVING ) 联结； 子查询； 并； 聚集函数（ Min() 、 Count() 、 Sum() 等）； DISTINCT； 导出（计算）列。 绝大部分情况下,视图只用于检索（ SELECT 语句）而不用于更新（ INSERT 、 UPDATE 和 DELETE ） 存储过程之前我们都是使用的是1条SQL语句，但有的操作需要针对许多表的多条MySQL语句这时就需要用到存储过程。存储过程可以看成是对一系列 SQL 操作的批处理。（可以理解为我们封装了一个方法） 使用存储过程的好处： 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 存储过程基本使用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 创建存储过程CREATE PROCEDURE productpricing() -- 没有接收参数BEGIN SELECT AVG(prod_price) AS priceaverage FROM products; -- 分号;不可省END;-- 调用存储过程CALL productpricing();-- 删除存储过程DROP PROCEDURE productpricing;# 创建存储过程使用参数-- 此存储过程接收3个参数-- OUT表示从存储过程传出 还有 IN传入存储过程 INOUT对存储过程传入和传出-- pmin存储产品最低价格 类型为DECIMAL类型（创建表的类型都能用）其他同理-- 通过INTO关键字保存到响应变量中CREATE PROCEDURE productpricing( OUT pmin DECIMAL(8,2), OUT pmax DECIMAL(8,2), OUT pavg DECIMAL(8,2))BEGIN SELECT MIN(prod_price) INTO pmin FROM products; SELECT MAX(prod_price) INTO pmax FROM products; SELECT AVG(prod_price) INTO pavg FROM products;END;-- 调用存储过程-- 需要传递3个参数，使用@CALL productpricing( @pricelow, @pricehigh, @priceaverage);-- 查询SELECT @pricelow,@pricehigh,@priceaverage-- # 再看另外一个例子使用INCREATE PROCEDURE ordertotal( IN onumber INT, OUT ototal DECIMAL(8,2))BEGIN SELECT SUM(item_price*quantity) FROM orderitems WHERE order_num = onumber INTO ototal;END;-- 和调用函数相同，可以传递不同的值CALL ordertotal(20005,@total);SELECT @total;CALL ordertotal(20009,@total);SELECT @total; 智能存储过程使用 12345678910111213141516171819202122232425262728293031323334353637# 获得订单合计，但需要对合计增加营业税-- 1. 获得合计-- 2. 把营业税有条件的添加到合计-- 3. 返回合计(带或不带税)CREATE PROCEDURE ordertotal( IN onumber INT, IN taxable BOOLEAN, OUT ototal DECIMAL(8,2))BEGIN -- 声明total变量 DECLARE total DECIMAL(8,2); -- 声明上税百分比 DECLARE taxrate INT DEFAULT 6; -- 获得订单合计 SELECT SUM(item_price*quantity) FROM orderitems WHERE order_num = onumber INTO total; -- 判断是否要上税 IF taxable THEN -- true 需要上税 SELECT total+(total/100*taxrate) INTO total; END IF; -- 直接将total赋给ototal SELECT total INTO ototal;END;CALL ordertotal(20005,FALSE,@total);SELECT @total;CALL ordertotal(20005,TRUE,@total);SELECT @total; 游标游标的主要作用是对一个结果集进行移动遍历（可以理解为是一个指针，指向查出的结果的每一行）。主要用于交互式应用，其中用户需要对数据集中的任意一行或多行进行浏览和修改。 注意：游标只能在存储过程中使用 游标的基本使用： 声明游标 2. 打开游标 3. 取出数据 4. 关闭游标123456789101112131415CREATE PROCEDURE processorders()BEGIN -- 声明游标为ordernumbers DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- 打开游标 OPEN ordernumbers; -- ...具体操作 -- 关闭游标 CLOSE ordernumbers;END; 接下来看个具体的例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 每个订单加上上税后的金额CREATE PROCEDURE processorders()BEGIN -- 声明局部变量 DECLARE done BOOLEAN DEFAULT 0;-- 声明循环终止符 DECLARE o INT; -- 声明局部变量o 用于存储每行数据 DECLARE t DECIMAL(8,2); -- 存储每个订单合计 -- 声明游标为ordernumbers DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; -- 声明句柄用于条件出现时(即SQLSTATE=02000时)设置done为1，结束循环） DECLARE CONTINUE HANDLER FOR SQLSTATE '02000' SET done=1; -- 创建一个表来存储结果 CREATE TABLE IF NOT EXISTS ordertotals(order_num INT,total DECIMAL(8,2)); -- 打开游标 OPEN ordernumbers; -- 循环所有行 REPEAT -- FETCH 用来检索当前行的 order_num 列 -- 自动从第一行开始,并存储到一个名为 o 的局部声明的变量中 FETCH ordernumbers INTO o; -- 得到订单总量 CALL ordertotal(o,1,t); -- （之前创建的存储过程）计算每个订单上税合计 INSERT INTO ordertotals (order_num,total) VALUES(o,t); -- 结束循环 UNTIL done END REPEAT; -- 关闭游标 CLOSE ordernumbers;END;CALL processorders();SHOW TABLES;SELECT * FROM ordertotals; 触发器触发器：监视某种情况，并触发某种操作，它是提供给程序员来保证数据完整性的一种方法。 触发器是与表事件相关的一种特殊的存储过程，它的执行不是由程序调用，也不是手工启动，而是由事件来触发。触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。 触发器创建语法四要素： 监视地点(table) 监视事件(INSERT/UPDATE/DELELTE) 触发时间(BEFORE/AFTER) 触发事件(INSERT/UPDATE/DELELTE) 这里对BEFORE和AFTER进行一下说明 BEFORE 在监视事件之前执行触发事件。则主要用于数据校验和净化。 AFTER 在监视事件之后执行触发事件。主要用于审计跟踪，将修改记录到另外一张表中。 触发器的基本使用 123456789101112131415161718# 基本语法CREATE TRIGGER 触发器名BEFORE/AFTER INSERT/UPDATE/DELETE ON 表名FOR EACH ROW -- 这句话在MySQL中是固定(指的是操作的每一行)BEGIN sql语句;END;# 简单的例子-- 在插入vendors表中一条记录后，在products表中插入一条记录CREATE TRIGGER newvendorsAFTER INSERT ON vendorsFOR EACH ROWBEGIN INSERT INTO products VALUES ('ANNNNNN',1009,'jdfks',9.99,'插入时的描述');END;-- 测试一下INSERT INTO vendors VALUES (1009,'haha','haha','haha','aa','dfsfs','USA'); INSERT触发器INSERT触发器内可以引用一个名为NEW的虚拟表，访问被插入的行。 12345678910111213141516171819202122232425262728-- 比如还是上面的例子，vend_id就可以使用NEW.vend_id-- 这里NEW就表示vendors表中插入的一条记录DROP TRIGGER IF EXISTS `newvendors`;CREATE TRIGGER newvendorsAFTER INSERT ON vendorsFOR EACH ROWBEGIN INSERT INTO products VALUES ('ANNNNNN',NEW.vend_id,'jdfks',9.99,'插入时的描述');END;-- 测试一下INSERT INTO vendors VALUES (1009,'haha','haha','haha','aa','dfsfs','USA');# 也可以使用IF判断-- 判断供应商国家是不是USA，是的话执行，否则不执行CREATE TRIGGER newvendorsAFTER INSERT ON vendorsFOR EACH ROWBEGIN IF NEW.vend_country = 'USA' THEN INSERT INTO products VALUES ('ANNNNNN',NEW.vend_id,'jdfks',9.99,'插入时的描述'); END IF;END;-- 测试一下-- 插入成功，两张表都更新INSERT INTO vendors VALUES (1009,'haha','haha','haha','aa','dfsfs','USA'); -- 插入失败,因为是AFTER,所以只有vendors表中更新INSERT INTO vendors VALUES (1010,'haha','haha','haha','aa','dfsfs','France'); DELETE触发器在 DELETE触发器代码内，你可以引用一个名为OLD的虚拟表，访问被删除的行 1234567891011-- 将删除的数据保存到另一张表中CREATE TRIGGER deletesaveorderBEFORE DELETE ON ordersFOR EACH ROWBEGIN INSERT INTO deleteorder (order_num,order_date,cust_id) VALUES(OLD.order_num,OLD.order_date,OLD.cust_id);END;-- 测试DELETE FROM orders WHERE order_num = 20010; UPDATE触发器在 UPDATE触发器代码内可以引用一个名为OLD的虚拟表，访问被删除的行也可以引用一个名为 NEW 的虚拟表访问新更新的值 12345678910DROP TRIGGER updatevendor;CREATE TRIGGER updatevendorBEFORE UPDATE ON vendorsFOR EACH ROWBEGIN -- 一种简便的写法因为是BEFORE,所以可以直接SET SET NEW.vend_state = UPPER(NEW.vend_state);END;-- 测试一下UPDATE vendors SET vend_state = 'ab' WHERE vend_id = 1006 MySQL事务管理事务处理是一种机制，通过确保成批的 SQL 操作要么完全执行，要么完全不执行，来维护数据库的完整性，保证数据库不包含不完整的操作结果。 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。 注意： 事务不能回退 SELECT 语句，回退 SELECT 语句也没意义 事务也不能回退 CREATE 和 DROP 语句。 MySQL中事务是默认自动提交的，MySQL会把每一条INSERT、UPDATE、DELETE语句都看做是一个事务自动提交。 123456-- 查看是否是自动提交 ON表示开启自动提交 OFF表示关闭SHOW VARIABLES LIKE 'autocommit' -- ON-- 使用autocommit = 0关闭自动提交，-- autocommit = 1开启自动提交SET autocommit = 0;SHOW VARIABLES LIKE 'autocommit' -- OFF 事务使用的基本方法： 如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处； 如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。 保留点在事务处理完成（执行一条 ROLLBACK 或 COMMIT）后自动释放。1234567START TRANSACTION-- ...SAVEPOINT delete1 -- ...ROLLBACK TO delete1-- ...COMMIT 我们先按照上面关闭自动提交，再来看下面的实验结果 12345678910111213141516171819# ROLLBACK 和SAVEPOINT测试-- SELECT * FROM ordertotals;START TRANSACTION;DELETE FROM ordertotals WHERE order_num = 20005;SAVEPOINT delete1;-- SELECT * FROM ordertotals;DELETE FROM ordertotals;-- SELECT * FROM ordertotals;ROLLBACK TO delete1; --回滚到delete1点，只是删除了order_num = 20005的列-- SELECT * FROM ordertotals;ROLLBACK;-- SELECT * FROM ordertotals; --回滚到开始处，20005行也没有删除# COMMIT测试-- 彻底删除了20005行，无法回滚了START TRANSACTIONDELETE FROM ordertotals WHERE order_num = 20005;SAVEPOINT delete1;COMMIT; MySQL权限管理到目前为止我们使用的都是MySQL的root账户，它对整个MySQL服务器具有完全的控制。不过实际使用时决不能使用root,应该创建一系列的账号，有的用于管理，有的供用户使用，有的供开发人员使用。 MySQL的权限管理就是管理用户以及用户访问数据的权限管理。比如哪些用户可以创建表，哪些用户只能读和写表;哪些用户只能添加数据不能删除数据等等。 MySQL用户账号和信息存储在名为 mysql 的MySQL数据库中,可以使用下面的命令查看用户： 12USE mysql;SELECT user FROM user; 用户管理12345678910111213# 创建账户-- 新创建的账户没有任何权限。-- Vic用户名 mypassword密码CREATE USER Vic IDENTIFIED BY 'mypassword'; # 修改账户名RENAME USER Vic TO coderchen;# 删除账户DROP USER coderchen;# 修改密码SET PASSWORD FOR coderchen = Password('chenpassword'); 权限管理新创建的用户账号没有访问权限。它们能登录MySQL，但不能看到数据，不能执行任何数据库操作。比如上面的新建用户coderchen。 因此新创建用户后需要给用户赋予权限。 授予权限使用 GRANT 撤销权限使用 REVOKE 123456789101112131415161718192021222324252627# 查看用户权限-- 又重新创建了coderchen账号SHOW GRANTS FOR myuser;-- 运行结果如下所示：表示coderchen没有任何权限-- GRANT USAGE ON *.* TO 'coderchen'@'%'# 设置访问权限-- 授予coderchen对reviewmysql中的所有表具有SELECT和INSERT权限GRANT SELECT, INSERT ON reviewmysql.* TO coderchen;-- 验证一下SHOW GRANTS FOR coderchen;-- 运行结果如下：可以看到多了一行我们设置的权限-- GRANT USAGE ON *.* TO 'coderchen'@'%'-- GRANT SELECT, INSERT ON `reviewmysql`.* TO 'coderchen'@'%'-- 这时候也可以用coderchen连接数据库进行测试-- 查询和插入都没问题-- 更新会出现UPDATE command denied to user 'coderchen'@'localhost' for table 'orders'错误# 撤销访问权限-- 撤销插入权限REVOKE INSERT ON reviewmysql.* FROM coderchen;-- 验证一下SHOW GRANTS FOR coderchen;-- 运行结果如下：可以看到第二行中已经少了INSERT权限-- GRANT USAGE ON *.* TO 'coderchen'@'%'-- GRANT SELECT ON `reviewmysql`.* TO 'coderchen'@'%' GRANT 和 REVOKE 可在几个层次上控制访问权限： 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 详细的权限说明参考下表","link":"/2020/04/06/SQL%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E4%B9%8B%E7%B4%A2%E5%BC%95%E3%80%81%E8%A7%86%E5%9B%BE%E3%80%81%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E3%80%81%E8%A7%A6%E5%8F%91%E5%99%A8%E3%80%81%E6%B8%B8%E6%A0%87%E3%80%81%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E3%80%81%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E3%80%81%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BB%B4%E6%8A%A4/"},{"title":"Java并发工具类CountDownLatch、CyclicBarrier、Semaphore","text":"主要内容 CountDownLatch(闩锁) CyclicBarrier(屏障) Semaphore(信号量) CountDownLatch源码分析 CyclicBarrier源码分析 Semaphore源码分析 JDK1.5之后Java为我们提供了上面三个同步工具类。这几个工具类的目的就是为了能够更好的控制线程间的通信问题 CountDownLatch(闩锁) A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes. 作用:允许一个或多个线程等待其他线程完成操作。 常用的方法就两个await和countDown() 使用说明： 初始化CoutDownLatch的初始值count 需要等待的线程调用await()方法，await方法会一直阻塞直到count=0 其他的线程执行自己的操作，操作完成后调用countDown()方法使得count值-1。 123456789101112131415161718192021222324package com.company.MultiThread;import java.util.concurrent.CountDownLatch;public class CountDownLatchDemo { public static void main(String[] args) { CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 1; i &lt;= 6; i++) { new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"号同学离开教室\"); countDownLatch.countDown(); },String.valueOf(i)).start(); } new Thread(()-&gt;{ try { countDownLatch.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\"最后离开教室锁门\"); },\"coderchen33\").start(); }} 运行结果： 12345671号同学离开教室6号同学离开教室5号同学离开教室4号同学离开教室2号同学离开教室3号同学离开教室coderchen33最后离开教室锁门 反过来，让多个线程等待一个线程也是一样的： 123456789101112131415161718192021222324package com.company.MultiThread;import java.util.concurrent.CountDownLatch;public class CountDownLatchDemo { public static void main(String[] args) { CountDownLatch countDownLatch = new CountDownLatch(1); for (int i = 1; i &lt;= 6; i++) { new Thread(()-&gt;{ try { countDownLatch.await(); System.out.println(Thread.currentThread().getName()+\"出去玩\"); } catch (InterruptedException e) { e.printStackTrace(); } },String.valueOf(i)).start(); } new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"离开教室了\"); countDownLatch.countDown(); },\"老师\").start(); }} 1234567老师离开教室了2出去玩3出去玩1出去玩6出去玩5出去玩4出去玩 CyclicBarrier(栅栏) A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point. CyclicBarriers are useful in programs involving a fixed sized party of threads that must occasionally wait for each other. The barrier is called cyclic because it can be re-used after the waiting threads are released. 作用：让一组线程相互等待以达到一个公共的障碍点(屏障)时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被平常拦截的线程才会继续运行。 1234567891011121314151617181920212223242526package com.company.MultiThread;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;public class CyclicBarrierDemo { public static void main(String[] args) { CyclicBarrier cyclicBarrier = new CyclicBarrier(7,()-&gt;{ System.out.println(\"召唤神龙\"); }); for (int i = 1; i &lt;= 7; i++) { final int tempI = i; new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"收集到了第\"+tempI+\"颗龙珠\"); try { cyclicBarrier.await(); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } },String.valueOf(i)).start(); } }} 只有7个线程都到了屏障点，才打开屏障，召唤神龙。 123456781收集到了第1颗龙珠5收集到了第5颗龙珠3收集到了第3颗龙珠4收集到了第4颗龙珠2收集到了第2颗龙珠7收集到了第7颗龙珠6收集到了第6颗龙珠召唤神龙 Semaphore(信号量) A counting semaphore. Conceptually, a semaphore maintains a set of permits. Each acquire() blocks if necessary until a permit is available, and then takes it. Each release() adds a permit, potentially releasing a blocking acquirer. However, no actual permit objects are used; the Semaphore just keeps a count of the number available and acts accordingly. 作用：控制同时访问特定资源的线程数，它维护了一组许可证 当调用acquire()时会消费一个许可证。如果没有许可证了，线程会阻塞起来 当调用release()时会添加一个许可证。 这些许可证就是一个count变量 12345678910111213141516171819202122232425262728package com.company.MultiThread;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;public class SemaphoreDemo { public static void main(String[] args) { Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; 7; i++) { new Thread(()-&gt;{ try { //抢许可证 semaphore.acquire(); System.out.println(Thread.currentThread().getName()+\"抢到许可证了\"); //抢到了许可证,干3秒自己的活 TimeUnit.SECONDS.sleep(3); System.out.println(Thread.currentThread().getName()+\"干完活了，释放许可证\"); //释放许可证 semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } },String.valueOf(i)).start(); } }} 12345678910111213140抢到许可证了1抢到许可证了2抢到许可证了1干完活了，释放许可证0干完活了，释放许可证2干完活了，释放许可证4抢到许可证了3抢到许可证了5抢到许可证了5干完活了，释放许可证3干完活了，释放许可证4干完活了，释放许可证6抢到许可证了6干完活了，释放许可证 CountDownLatch源码分析上面我们看过了CountdownLatch的使用方法，现在我们来看看它的源码。 CountDownLatch主要是通过 AQS的共享锁 机制实现的，因此它的核心属性只有一个sync，它继承自AQS，同时覆写了tryAcquireShared和tryReleaseShared，以完成具体的实现共享锁的获取与释放的逻辑。 1private final Sync sync; 1234567891011121314151617181920212223242526private static final class Sync extends AbstractQueuedSynchronizer { Sync(int count) { setState(count); } int getCount() { return getState(); } protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1; } protected boolean tryReleaseShared(int releases) { // Decrement count; signal when transition to zero for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; } }} 构造函数 1234public CountDownLatch(int count) { if (count &lt; 0) throw new IllegalArgumentException(\"count &lt; 0\"); this.sync = new Sync(count);} 可以看到构造函数非常简单，就是将我们传入的count值传递给了AQS中的state countDown方法 123public void countDown() { sync.releaseShared(1);} 1234567891011protected boolean tryReleaseShared(int releases) { // Decrement count; signal when transition to zero for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; }} 可以看到countDown方法其实也非常简单，调用的是AQS的releaseShared(1)的方法，并且在countDown的tryReleaseShared方法中，通过自旋+CAS对state进行减1操作，只有在state=0的时候才返回true。之后会调用doReleaseShared方法唤醒所有等待中的线程，该方法我们AQS文章中已经详细分析过了，这里就不再赘述了。 await方法 与Condition的await()方法的语义相同，该方法是阻塞式地等待，并且是响应中断的，只不过它不是在等待signal操作，而是在等待count值为0 123public void await() throws InterruptedException { sync.acquireSharedInterruptibly(1);} 可见，await方法内部调用的是acquireSharedInterruptibly方法，相当于借用了获取共享锁的“壳”： 1234567public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg); //AQS中很简单，不在赘述} 我们看下它自己实现的tryAcquireShared方法 123protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1;} 该方法似乎有点挂羊头卖狗肉的感觉——所谓的获取共享锁，事实上并不是什么抢锁的行为，没有任何CAS操作，它就是判断当前的state值是不是0，是就返回1，不是就返回-1。 总结 CountdownLatch底层是基于AQS的共享锁进行实现的，实际上就是对state的操作。 await()方法中是AQS响应中断的acquire()方法。而countDown则是调用的AQS的releaseShared(1)方法，在重写的tryRelease方法中通过自旋+CAS进行State的减1操作。 这里可能会有一个问题在于为什么是共享锁而不是独占锁?其实上面已经说到了，CountDownLatch可以让一个或者多个线程等待，这里如果其他线程执行完了，那么等待的多个线程可以共同执行，因此需要用共享锁。 如果只能让一个线程等待，则可以用独占锁 CyclicBarrier源码分析 12345678910111213141516171819202122232425private static class Generation { boolean broken = false;}/*为了方便理解，调整了下变量的顺序*///参与线程的总数，即需要一起通过barrier的线程数private final int parties;//还需要等待的线程数，初始值为parties//每当一个线程到达，就减1，如果减为0，则所有线程一起通过barrierprivate int count;//使用ReentrantLock独占锁+Condition对象实现private final ReentrantLock lock = new ReentrantLock();//Condition对象//相互等待的线程都会在同样的条件队列trip上挂起//被唤醒后添加到sync queue中取争夺独占锁lockprivate final Condition trip = lock.newCondition(); //只有一个boolean类型的broken属性//下面分析的时候再讲private Generation generation = new Generation(); //Runnable对象，类似于钩子方法//在它们一同通过barrier之前，就会执行这个对象的run方法(我们上面的召唤神龙就是这个参数)private final Runnable barrierCommand; 构造函数 1234567891011public CyclicBarrier(int parties) { this(parties, null);}public CyclicBarrier(int parties, Runnable barrierAction) { if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction;} 构造函数很简单，就是初始化了parties、count和runnable三个属性。 辅助方法 要理解CyclicBarrier，首先我们需要弄明白它的几个辅助方法。 首先需要理解的是“代”（Generation）的概念，由于CyclicBarrier是可重复使用的，我们把每一个新的barrier称为一“代”。这个怎么理解呢，打个比方：一个过山车有10个座位，景区常常需要等够10个人了，才会去开动过山车。于是我们常常在栏杆（barrier）外面等，等凑够了10个人，工作人员就把栏杆打开，让10个人通过；然后再将栏杆归位，后面新来的人还是要在栏杆外等待。这里，前面已经通过的人就是一“代”，后面再继续等待的一波人就是另外一“代”，栏杆每打开关闭一次，就产生新一的“代”。 nextGeneration方法：开启新的一代 1234567private void nextGeneration() { // 唤醒当前这一代中所有等待在条件队列里的线程 trip.signalAll(); // 恢复count值，开启新的一代 count = parties; generation = new Generation();} 该方法用于开启新的“一代”，通常是被最后一个调用await方法的线程调用。在该方法中，我们的主要工作就是唤醒当前这一代中所有等待在条件队列里的线程，将count的值恢复为parties，以及开启新的一代。 breakBarrier()方法:即打破现有的栅栏，让所有线程通过 12345678private void breakBarrier() { // 标记broken状态 generation.broken = true; // 恢复count值 count = parties; // 唤醒当前这一代中所有等待在条件队列里的线程（因为栅栏已经打破了） trip.signalAll();} 这个breakBarrier怎么理解呢，继续拿上面过上车的例子打比方，有时候某个时间段，景区的人比较少，等待过山车的人数凑不够10个人，眼看后面迟迟没有人再来，这个时候有的工作人员也会打开栅栏，让正在等待的人进来坐过山车。这里工作人员的行为就是breakBarrier，由于并不是在凑够10个人的情况下就开启了栅栏，我们就把这一代的broken状态标记为true。 reset()方法: 用于将barrier恢复成初始的状态。它的内部就是简单地调用了breakBarrier方法和nextGeneration方法。 12345678910public void reset() { final ReentrantLock lock = this.lock; lock.lock(); try { breakBarrier(); // break the current generation nextGeneration(); // start a new generation } finally { lock.unlock(); }} 这里要注意的是，如果在我们执行该方法时有线程正等待在barrier上，则它将立即返回并抛出BrokenBarrierException异常。另外一点值得注意的是，该方法执行前需要先获得锁。 await方法 看完了辅助方法，我们看看它的核心方法await()方法。它也是一个集“countDown”和“阻塞等待”于一体的方法。 1234567891011121314public int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L); } catch (TimeoutException toe) { throw new Error(toe); // cannot happen }}public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException { return dowait(true, unit.toNanos(timeout));} 可以看到，无论是否带超时机制，最终调用的还是dowait()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 所有执行await方法的线程必须是已经持有了锁，所以这里必须先获取锁 lock.lock(); try { final Generation g = generation; // 前面说过，调用breakBarrier会将当前“代”的broken属性设为true // 如果一个正在await的线程发现barrier已经被break了，则将直接抛出BrokenBarrierException异常 if (g.broken) throw new BrokenBarrierException(); // 如果当前线程被中断了，则先将栅栏打破，再抛出InterruptedException // 这么做的原因是，所以等待在barrier的线程都是相互等待的，如果其中一个被中断了，那其他的就不用等了。 if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // 当前线程已经来到了栅栏前，先将等待的线程数减一 int index = --count; // 如果等待的线程数为0了，说明所有的parties都到齐了 // 则可以唤醒所有等待的线程，让大家一起通过栅栏，并重置栅栏 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) // 如果创建CyclicBarrier时传入了barrierCommand // 说明通过栅栏前有一些额外的工作要做 command.run(); ranAction = true; // 唤醒所有线程，开启新一代 nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // 如果count数不为0，就将当前线程挂起，直到所有的线程到齐，或者超时，或者中断发生 for (;;) { try { // 如果没有设定超时机制，则直接调用condition的await方法 if (!timed) trip.await(); // 当前线程在这里被挂起 else if (nanos &gt; 0L) // 如果设了超时，则等待指定的时间 nanos = trip.awaitNanos(nanos); // 当前线程在这里被挂起，超时时间到了就会自动唤醒 } catch (InterruptedException ie) { // 执行到这里说明线程被中断了 // 如果线程被中断时还处于当前这一“代”，并且当前这一代还没有被broken,则先打破栅栏 if (g == generation &amp;&amp; ! g.broken) { breakBarrier(); throw ie; } else { // 注意来到这里有两种情况 // 一种是g!=generation，说明新的一代已经产生了，所以我们没有必要处理这个中断，只要再自我中断一下就好，交给后续的人处理 // 一种是g.broken = true, 说明中断前栅栏已经被打破了，既然中断发生时栅栏已经被打破了，也没有必要再处理这个中断了 Thread.currentThread().interrupt(); } } // 注意，执行到这里是对应于线程从await状态被唤醒了 // 这里先检测broken状态，能使broken状态变为true的，只有breakBarrier()方法，到这里对应的场景是 // 1. 其他执行await方法的线程在挂起前就被中断了 // 2. 其他执行await方法的线程在还处于等待中时被中断了 // 2. 最后一个到达的线程在执行barrierCommand的时候发生了错误 // 4. reset()方法被调用 if (g.broken) throw new BrokenBarrierException(); // 如果线程被唤醒时，新一代已经被开启了，说明一切正常，直接返回 if (g != generation) return index; // 如果是因为超时时间到了被唤醒，则打破栅栏，返回TimeoutException if (timed &amp;&amp; nanos &lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); }} 总结 CyclicBarrier基于ReentrantLock独占锁和condition条件队列来实现，而非共享锁。 CyclicBarrier可重复使用，在所有线程都到齐了一起通过后，将会开启新的一代。 CyclicBarrier使用了“all-or-none breakage model”，所有互相等待的线程，要么一起通过barrier，要么一个都不要通过，如果有一个线程因为中断，失败或者超时而过早的离开了barrier，则该barrier会被broken掉，所有等待在该barrier上的线程都会抛出BrokenBarrierException（或者InterruptedException）。 Semaphore源码分析与CountDownLatch类似，Semaphore主要是通过 AQS的共享锁 机制实现的，因此它的核心属性只有一个sync，它继承自AQS： 1234567891011121314151617181920212223242526abstract static class Sync extends AbstractQueuedSynchronizer { Sync(int permits) { setState(permits); } final int getPermits() { return getState(); } final int nonfairTryAcquireShared(int acquires) { //省略 } protected final boolean tryReleaseShared(int releases) { //省略 } final void reducePermits(int reductions) { //省略 } final int drainPermits() { //省略 } } 这里的permits和CountDownLatch的count很像，它们最终都将成为AQS中的state属性的初始值。 构造函数另外，它还是支持公平锁和非公平锁的，我们可以从它的构造函数中看出来: 12345678//默认是非公平锁 public Semaphore(int permits) { sync = new NonfairSync(permits); }public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits);} 公平锁和非公平锁公平锁和非公平锁的定义如下: 12345678910111213141516171819202122232425262728293031static final class NonfairSync extends Sync { NonfairSync(int permits) { super(permits); } protected int tryAcquireShared(int acquires) { return nonfairTryAcquireShared(acquires); } } /** * Fair version */ static final class FairSync extends Sync { FairSync(int permits) { super(permits); } protected int tryAcquireShared(int acquires) { for (;;) { if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; } } } acquire()方法 123public void acquire() throws InterruptedException { sync.acquireSharedInterruptibly(1);} 1234567public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);} 可以看到和CountDownLatch一样，底层还是调用的acquireSharedInterruptibly(1) ，我们上面也提到过了acquireSharedInterruptibly()和acquireShared一样，都是AQS中的函数，只不过前者响应中断，后者不响应中断。 这里我们就直接看看公平锁和非公平锁是如何重写tryAcquireShared()方法的： 非公平锁的tryAcquireShared方法: 12345678910111213protected int tryAcquireShared(int acquires) { return nonfairTryAcquireShared(acquires);}final int nonfairTryAcquireShared(int acquires) { for (;;) { int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; } } 这里基本上和CountDownLatch一样，都是用自旋+CAS来进行state的减操作。 该方法退出的唯一条件是成功的修改了state值，并返回state的剩余值。如果剩下的信号量不够了，则就不需要进行CAS操作，直接返回剩余值。所以其实tryAcquireShared返回的不是当前剩余的信号量的值，而是如果扣去acquires之后，当前将要剩余的信号量的值，如果这个“将要”剩余的值比0小，则是不会发生扣除操作的。(之前我们提到过，如果tryAcquiredShared的返回值小于0，则代表当前线程获取共享锁失败) 公平锁的实现： 123456789101112protected int tryAcquireShared(int acquires) { for (;;) { //和非公平锁的唯一区别在于需要先判断下之前是否有等待的节点 if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; }} release方法 123public void release() { sync.releaseShared(1);} 可以看到releaseShared最终还是使用的AQS的releaseShared(1)方法，这里就不赘述了。 总结 Semaphore和CountDownLatch非常相似，都是基于AQS的共享锁实现的,并且最终都是对state进行操作。 Semaphore还支持公平锁和非公平锁模式。 acquire中使用自旋+CAS实现，release中直接调用的是AQS的releaseShared方法","link":"/2020/03/31/Java%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BBCountDownLatch%E3%80%81CyclicBarrier%E3%80%81Semaphore/"},{"title":"EXPLAIN关键字详解","text":"主要内容 explain介绍 explain各个列的作用 参考MySQL官网 https://dev.mysql.com/doc/refman/5.7/en/explain-output.html 示例SQL EXPLAIN之前我们学习MySQL架构的时候知道，在第二层服务层中有一个查询优化器(Query Optimizer)用来对我们的SELECT语句进行优化，优化器选择执行最有效查询的一组操作称为查询执行计划。当使用explain时，MySQL就能告诉我们优化器对相关语句的执行计划的信息。 通过explain命令我们可以知道以下信息：表的读取顺序，数据读取操作的类型，哪些索引可以使用，哪些索引实际使用了，表之间的引用，每张表有多少行被优化器查询等信息。 简单来说就是:explain可以让我们知道select语句是如何执行的，是否用到了索引等信息，从而为我们之后优化select语句提供信息。 EXPLAIN的用法非常简单 EXPLAIN+SELECT语句 即可。 1explain select * from film where id = 1; 紧随其后，可以通过show warnings得到优化后的select语句(5.7之前需要使用explain extended + show warnings) 我们下面一一解释下上面explain结果中各个列的含义。 EXPLAIN中的列1. idSELECT的标识符，包含一组数字，有几个 select 就有几个id，用来表示查询中SELECT子句或操作表的顺序。 id相同，执行顺序从上之下 id不同，执行顺序从大到小 2. select_type1）simple：简单查询。不包含子查询和union 2）primary：复杂查询中最外层的 select 3）subquery：子查询中的第一个select 4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义）奇怪的是在MySQL5.7中竟然只有一个SIMPLE，应该是mysql内部进行了优化。 5）union：在 union 中的第二个和随后的 select 6）union result : 从 union 临时表检索结果的 select 其他还有一些不常见的，这里就不解释了。想要详细看其他的可以参考MySQL官网。 3. table 这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 &lt;derivenN&gt; 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查询。 当有 union 时，UNION RESULT 的 table 列的值为 &lt;union1,2&gt;，1和2表示参与 union 的 select 行id。 4. partitions显示分区表中的命中情况。非分区表，则该值为NULL 5. type该列称为关联类型或者访问类型，它指明了MySQL决定如何查找表中符合条件的行。这个字段直接反映我们的SQL性能是否高效，是我们优化要重点关注的字段。 依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 最常见的是：const &gt; eq_ref &gt; ref&gt; range &gt; index &gt; ALL NULL:MySQL能在优化阶段分解查询语句，在执行阶段不用再访问表或索引。 比如下面的在索引中选取最小值，可以单独查找索引来完成，不需要在执行时访问表 system和const 1.system：表只有一行记录，这个是const的特例，一般不会出现，可以忽略2.const：const用于比较主键索引或者唯一索引。因为只匹配一行数据，所以很快。 1234SELECT * FROM tbl_name WHERE primary_key=1;SELECT * FROM tbl_name WHERE primary_key_part1=1 AND primary_key_part2=2; eq_ref唯一性索引扫描，对于每个索引键，表中只有1条记录与之匹配。一般是两表关联，关联条件中的字段是主键或唯一索引。 123456SELECT * FROM ref_table tab1,other_table tab2ON tab1.key_column=tab2.column;SELECT * FROM ref_table tab1,other_table tab2ON tab1.key_column_part1=tab2.columnWHERE tab1.key_column_part2=1; ref非唯一性索引扫描，表中有多条记录与之匹配。本质上是一种索引访问，然而可能会找到多个符合条件的行，所以是索引和扫描的混合体。此类型通常出现在使用了非唯一或非主键索引或者使用了最左前缀规则索引的查询。 123456789SELECT * FROM ref_table WHERE key_column=expr;-- 这里和ref的不同在于联结会有多行SELECT * FROM ref_table tab1,other_table tab2ON tab1.key_column=tab2.column;SELECT * FROM ref_table tab1,other_table tab2ON tab1.key_column_part1=tab2.columnWHERE tab1.key_column_part2=1; 非主键索引和非唯一索引 关联表查询仅使用了film_actor的左前缀 ref_or_null类似ref，但是可以搜索值为NULL的行。 index_merge 表示使用了索引合并的优化方法。 range有检索范围的索引扫描范围扫描通常出现在 &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL,BETWEEN, LIKE,IN() 操作中。 index全索引扫描。与ALL差不多都是读全表，但通常比ALL快，因为索引文件通常比数据文件小。主要优点是避免了排序，但是开销仍然非常大。如果在 Extra 列看到 Using index，说明正在使用覆盖索引，只扫描索引的数据，它比按索引次序全表扫描的开销要少很多。 ALL 全表扫描以找到匹配的行。建议进行优化。 6. possible_keys这一列显示MySQL查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有值，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果。 7. key这一列显示mysql实际采用哪个索引来优化对该表的访问。如果想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 8. key_len列这一列显示了MySQL在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 9. ref列显示了在key列记录的索引中，表查找值时哪一列被使用了。常见的有：const（常量），func，NULL，字段名（例:film.id），如果可能的话，是一个常数。 从下面可以看出是使用了reviewmysql数据库中的film表的id列来查找film_actor中的记录。 10. rows列这一列是MySQL认为查询要扫描的行数，注意这个不是结果集里的行数。 11. filtered列该列表示根据条件过滤的表行的估计百分比，和 rows 相乘，表示和查询计划里下一个表关联的行数。 12. Extra列这一列展示的是额外信息。常见的重要值如下： Using index：使用覆盖索引，表示查询索引就可查到所需数据，不用扫描表数据文件，往往说明性能不错。 Using Where：在存储引擎检索行后再进行过滤，使用了where从句来限制哪些行将与下一张表匹配或者是返回给用户。 Using filesort：对结果使用一个外部索引排序，而不是按索引次序从表里读取行，一般有出现该值，都建议优化去掉，因为这样的查询 CPU 资源消耗大。 因为actor中id建了索引，所以是using index，而name中没有建，则需要进行文件排序。 Using temporary：在查询结果排序时会使用一个临时表，一般出现于排序、分组和多表 join 的情况，查询效率不高，建议优化。 EXPLAIN总结从上面我们知道了explain各个列的作用，其中比较重要的有: id列 (id不同从大到小执行，id相同则顺序执行) type列 (效率：const &gt; eq_ref &gt; ref &gt; range &gt; index &gt;all) key列 (实际用到的索引) rows列 (MySQL的扫描行数) Extra列 (Using index、Using filesort、Using temporary、Using Where) 以上sql使用的表和数据： 123456789101112131415161718192021222324252627282930DROP TABLE IF EXISTS `actor`;CREATE TABLE `actor` ( `id` int(11) NOT NULL, `name` varchar(45) DEFAULT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,'a','2017-12-22 15:27:18'), (2,'b','2017-12-22 15:27:18'), (3,'c','2017-12-22 15:27:18');DROP TABLE IF EXISTS `film`;CREATE TABLE `film` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `film` (`id`, `name`) VALUES (3,'film0'),(1,'film1'),(2,'film2');DROP TABLE IF EXISTS `film_actor`;CREATE TABLE `film_actor` ( `id` int(11) NOT NULL, `film_id` int(11) NOT NULL, `actor_id` int(11) NOT NULL, PRIMARY KEY (`id`), KEY `idx_film_actor_id` (`film_id`,`actor_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1);","link":"/2020/04/11/EXPLAIN%E5%85%B3%E9%94%AE%E5%AD%97%E8%AF%A6%E8%A7%A3/"},{"title":"索引和查询性能优化简介","text":"主要内容 查询性能优化简介 MySQL索引结构 创建高性能的索引策略 查询性能优化简介这里先整体提一下查询性能优化的结构，好让我们知道之后学习的内容是属于哪个层面的优化。 查询性能优化主要分为以下4个层面: 硬件优化 (硬件层面的优化，比如说增加内存等) 库表结构的优化 (库表结构设计的是否合理,比如是否符合范式等) 索引优化 (索引的建立是否合理高效) 查询优化 (SELECT语句如何写的准确高效) 上面提到的每一层设计或使用的不合理都会影响最终查询的性能，并且这几层从上到下都是层层递进的。 没有良好的硬件，即便我们把表结构、索引和SELECT语句都设计的十分完美都是无济于事的;同样有了良好的硬件，但是库表结构设计的不合理,就会影响我们索引和查询语句的建立和性能;硬件和库表结构设计的合理高效，但是索引建的很烂，最终效果也不会好;上面都建好了，但是我们最终写的SELECT语句很烂,比如在不必要的时候扫描全表等，也会使得查询性能下降。 硬件优化不是我们需要考虑的，库表结构优化和查询优化之后会详细讲。这里我们就看看对查询性能优化最有效的索引优化。 MySQL索引结构在MySQL中索引是在存储引擎层实现，而不是在服务器层。所以，不同的存储引擎的索引工作方式并不一样，也不是所有的存储引擎都支持所有类型的索引。接下来我们先看看MySQL支持的索引类型。 B+树索引 如上图，是一颗B+树，关于B+树的定义可以参考我数据结构里面树的文章，这里只说一些重点。浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。 B+树的查找过程如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的B+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。 B+树性质1.通过上面的分析，我们知道IO次数取决于B+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么B+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。 2.当B+树的数据项是复合的数据结构，比如(name,age,sex)的时候，B+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，B+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，B+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，B+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最佳左前缀法则，这个在之后索引优会详细介绍。 哈希索引哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 哈希索引结构如下： 从上面可以看到 哈希索引只包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。 虽然无法避免读取行，但由于使用了hash，所以在hash冲突很少的情况下，仍然可以以O(1)的时间复杂度读取数据。 哈希索引不是按照索引值顺序存储的，所以无法用于排序(ORDER BY)与分组(GROUP BY) 因为哈希索引是使用索引列的全部内容来计算哈希值，索引不支持部分索引列匹配查找。例如（A,B）列建立索引，如果查询只用数据列A，则无法使用该索引。 只支持精确查找(= IN &lt;=&gt;)，无法用于部分查找和范围查找。 InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。这个是完全自动的、内部的行为，用户无法控制或者配置，不过如果有必要，完全可以关闭这个功能。 全文索引全文索引用于查找文本中的关键词，而不是直接比较是否相等，类似于搜索引擎做的事情。 MyISAM引擎支持全文索引，InnoDB 引擎在5.6.4版本后也开始支持全文索引。 实际上全文索引很少用，我们知道全文索引类似于搜索引擎做的事情，而目前搜索引擎的方案又十分成熟，所以大部分情况下我们使用存储引擎如elasticsearch等而不是全文索引。 空间数据索引(R-Tree)MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 这个索引也很少使用 InnoDB引擎和MyISAM引擎使用的索引InnoDB引擎使用的索引 以下内容主要来自一篇文章，如果看我解释的不清楚的话，可以去看原文 聚簇索引 聚簇索引并不是一种单独的索引类型，而是一种数据存储的方式。它是指将实际的数据行和索引放到了一块，并且索引结构的叶子节点保存了实际的数据行。因为无法把数据行同时存放在两个地方，所以一个表只能有一个聚簇索引。 上图即为InnoDB引擎中的聚簇索引，它是在同一个结构中保存了B+树索引和实际数据行，体现在两个方面： 1.使用记录 主键 值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的主键大小顺序排成一个双向链表。 各个存放目录项的页也是根据页中记录的主键大小顺序排成一个双向链表。 2.B+树的叶子节点存储的是实际的完整的数据行。 从上面知道，InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键，如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 二级索引(辅助索引)上边介绍的聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件该咋办呢？只能从头到尾沿着链表依次遍历记录么? 答案是否定的，不然如果不能快速查找和排序我们还建索引干什么玩意儿。 实际上，我们每新建一个索引就会新建一棵B+树，不同的B+树中的数据采用不同的排序规则。比方说我们在非主键列c2列新建一个索引，则会新建一棵B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 1.使用记录 c2列 的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 各个存放目录项的页也是根据页中记录的c2列大小顺序排成一个双向链表。 2.B+树的叶子节点存储的并不是完整的用户记录，而只是 c2列+主键 这两个列的值。 3.目录项记录中不再是 主键+页号 的搭配，而变成了 c2列+页号 的搭配。 所以如果我们现在想通过c2列的值查找某些记录的话就可以使用我们刚刚建好的这个B+树了。和上面的步骤一样，从根页面根据c2列的值找到叶子页面后，叶子页面中存储的数据是c2和c1（也就是主键）两个列，所以 我们必须再根据主键值去聚簇索引中再查找一遍完整的用户记录 。 我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程也被称为回表。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！！！ 上面介绍的是单一索引，实际上更常用的是复合索引。我们知道建立复合索引(c2,c3)后，对记录先是根据c2进行排序，c2相同时再根据c3排序，反映到B+树上，则如下图所示: 如图所示，我们需要注意一下几点： 每条目录项记录都由c2、c3、页号这三个部分组成，各条记录先按照c2列的值进行排序，如果记录的c2列相同，则按照c3列的值进行排序。 B+树叶子节点处的用户记录由c2、c3和主键c1列组成。 MyISAM引擎使用的索引MyISAM的索引方案虽然也使用B+树，但是和InnoDB不同的是在MyISAM中是将索引和数据分开存储。 将表中的记录按照插入时间 顺序 的存储在一块存储空间上，我们可以通过 行号 而快速访问到一条记录。如图所示： 由于在插入数据的时候并没有刻意按照主键大小排序，所以我们并不能在这些数据上使用二分法进行查找。 1.MyISAM会单独为表的主键创建一个B+树索引，只不过在B+树的叶子节点中存储的不是完整的用户记录，而是 主键值 + 行号 的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！ 这一点和InnoDB是完全不相同的，在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着 MyISAM中建立的索引全部都是二级索引！ 2.如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB中的索引是一样的，只不过在叶子节点处存储的是相应的 列 + 行号 而已。这些索引也全部都是二级索引。 由于索引树是独立的，通过辅助键检索无需访问主键的索引树，即不用回表。 InnoDB引擎和MyISAM引擎的对比相同点: MyISAM和InnoDB引擎使用的都是B+树索引。 不同点: InnoDB引擎是聚簇索引+二级索引，而MyISAM引擎使用的都是二级索引。具体而言体现在： InnoDB引擎将B+树索引和实际的数据行存放在一起，实现索引即数据。 而MyISAM引擎中B+树索引和实际的数据行分开。 InnoDB引擎数据的物理存放顺序与索引顺序是一致的，即有序。而MyISAM表中的记录按照插入时间顺序的存储在一块存储空间上，是无序的。 InnoDB引擎中页节点存放的是主键+实际数据行，而MyISAM引擎中存放的是主键+行号。 如果根据主键查询数据InnoDB引擎可以直接从索引中取到，如果从二级索引中查询时需要进行回表。 而MyISAM引擎中不需要进行回表。所以如果表是只读表，则MyISAM的效率会比InnoDB高 索引的优点和缺点因为我们平时绝大部分时候使用的是InnoDB引擎的B+树结构的索引，并且我们上面知道了B+树的结构，这里也就能更好的的理解索引的优点和缺点了： 索引的优点： 索引大大减小了服务器需要扫描的数据行数 (索引中存储了实际的数据) 索引可以帮助服务器避免排序和分组，以及创建临时表(B+树索引是有序的，所以可以使用ORDER BY和GROUP BY进行排序和分组操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表) 将随机I/O变为顺序I/O (B+树索引是有序的，会将相邻的数据存储在一起) 索引的缺点 索引保存了主键和索引字段的真实数据，所以索引列也是要占用空间的。 虽然索引大大提高了查询速度，但同时会降低更新表的速度，如对表的INSERT、UPDATE和DELETE。因为更新表时,MySQL不仅要保存数据，还要保存索引文件每次更新添加了索引列的字段。 如果MySQL中有大数据量的表，需要花费一定的时间建立优秀的索引。","link":"/2020/04/10/%E7%B4%A2%E5%BC%95%E5%92%8C%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AE%80%E4%BB%8B/"},{"title":"leetcode-database题解","text":"主要内容leetcode中关于sql的语法练习题。leetcode SQL练习地址 175.Combine Two Tables12345# 注意下面要求没有地址的个人信息也要显示所以要使用左外联结SELECT p.FirstName,p.LastName,a.City,a.StateFROM Person AS pLEFT JOIN Address AS aON p.PersonId = a.PersonId; 176. Second Highest Salary1234567# 为了没有数据时返回null，则多加一层SELECT子查询SELECT ( SELECT DISTINCT Salary FROM Employee Order BY Salary DESC LIMIT 1,1) AS SecondHighestSalary; 177. Nth Highest Salary上面176题的扩展 12345678CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGINSET N = N-1; -- 使用SET设置参数值 RETURN ( SELECT (SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N,1) );END 178. Rank Scores123456SELECT s1.Score, COUNT(DISTINCT s2.Score) RankFROM Scores s1INNER JOIN Scores s2ON s1.Score &lt;= s2.ScoreGROUP BY s1.IdORDER BY s1.Score DESC 180. Consecutive Numbers1234567SELECT DISTINCT l1.Num ConsecutiveNumsFROM Logs l1INNER JOIN Logs l2ON l1.Id+1 = l2.IdINNER JOIN Logs l3ON l2.Id +1 = l3.IdWHERE l1.Num = l2.Num AND l2.Num = l3.Num; 181. Employees Earning More Than Their Managers对于筛选条件是放在on后面还是使用where子句：先执行 ON，后执行 WHERE；ON 是建立关联关系，WHERE 是对关联关系的筛选。参考链接 https://www.jianshu.com/p/d923cf8ae25f 12345SELECT e1.Name AS EmployeeFROM Employee e1INNER JOIN Employee e2ON e1.ManagerId = e2.idWHERE e1.Salary &gt; e2.Salary; 182. Duplicate Emails12345-- GROUP BY之后使用HAVINGSELECT EmailFROM PersonGROUP BY EmailHAVING COUNT(Email) &gt; 1; 183. Customers Who Never Order12345SELECT c.Name AS CustomersFROM Customers AS cLEFT JOIN Orders AS oON c.Id = o.CustomerIdWHERE o.CustomerId IS NULL; 184. Department Highest Salary123456789101112131415161718# 开始是这样写的，问题在于最大值有两个或两个以上时，只会显示一个，因为使用了聚合函数。-- SELECT d.Name AS Department,e.Name AS Employee,MAX(e.Salary)-- FROM Employee AS e-- INNER JOIN Department AS d-- ON e.DepartmentId = d.Id-- GROUP BY e.DepartmentId;# 正确做法SELECT d.Name AS Department,e.Name AS Employee,e.SalaryFROM Department AS dINNER JOIN Employee AS eON d.Id = e.DepartmentIdINNER JOIN ( SELECT DepartmentId,MAX(Salary) AS Salary FROM Employee GROUP BY DepartmentId) AS m ON e.Salary = m.SalaryWHERE e.DepartmentId = m.DepartmentId; 196. Delete Duplicate Emails12345-- delete也可以使用联结DELETE p1 FROM Person p1 INNER JOIN Person p2ON p1.Email = p2.Email WHERE p1.Id &gt; p2.Id; 197. Rising Temperature123456-- TO_DAYS()函数返回一个从年份0开始的天数SELECT w2.IdFROM Weather w2INNER JOIN Weather w1ON TO_DAYS(w2.RecordDate) - TO_DAYS(w1.RecordDate) = 1WHERE w2.Temperature &gt; w1.Temperature; 595. Big Countries123SELECT name,population,areaFROM WorldWHERE area &gt; 3000000 OR population &gt; 25000000; 596. Classes More Than 5 Students12345-- DISTINCT去重SELECT DISTINCT classFROM coursesGROUP BY classHAVING count(DISTINCT student)&gt;=5; 620. Not Boring Movies1234SELECT *FROM cinemaWHERE id%2 != 0 AND description != 'boring'ORDER BY rating DESC; 626. Exchange Seats123456789-- 完整case函数SELECTCASEWHEN id = (SELECT MAX(id) FROM seat) AND id %2=1 THEN idWHEN id%2=0 THEN id -1WHEN id%2 = 1 THEN id +1END AS id, studentFROM seatORDER BY id; 627. Swap Salary1234567-- 简单case函数UPDATE salarySET sex = CASEWHEN sex = 'm' THEN 'f' ELSE 'm'END;","link":"/2020/04/08/leetcode-database%E9%A2%98%E8%A7%A3/"},{"title":"SQL基本语法(一) 增删改查","text":"主要内容 查询数据SELECT 插入数据INSERT 修改数据UPDATE 删除数据DELET 二维表创建与维护 查询数据SELECT基本查询1234# select基本使用SELECT prod_name FROM products -- 查询单列SELECT prod_id,prod_name,prod_price FROM products -- 查询多列SELECT * FROM products -- 查询所有列，由于速度较慢一般不使用 DISTINCT去重和LIMIT分页123456789######### DISTINCT去重SELECT vend_id FROM products SELECT DISTINCT vend_id FROM products######### LIMIT分页SELECT prod_name FROM products LIMIT 5 -- 查询表的前5行(注意开始是0行)SELECT prod_name FROM products LIMIT 5,5 -- 第5行开始,返回5行数据SELECT prod_name FROM products LIMIT 3,4 -- 第3行开始,返回4行数据SELECT prod_name FROM products LIMIT 4 OFFSET 3 -- 第3行开始,返回4行数据 排序数据ORDER BYASC 升序 DESC降序 不指明默认升序 123456789# 按单列排序SELECT prod_name FROM products ORDER BY prod_name# 按多列排序(先按第一个字段名，再按第二个字段名)SELECT * FROM products ORDER BY prod_price DESC,prod_name -- 价格降序，名字升序SELECT prod_id,prod_name,prod_price FROM products ORDER BY 3,2; --先按第三列排序后按第二列排序# 可以根据不显示的列排序SELECT prod_name FROM products ORDER BY prod_price 过滤数据WHERE 单条件过滤= &lt;&gt;或!= &lt; &lt;= &gt; &gt;= BETWEEN 多条件过滤AND OR IN NOT 通配符(% _)过滤数据LIKE 正则表达式过滤数据REGEXP 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 单条件过滤 = &lt;&gt;或!= &lt; &lt;= &gt; &gt;= BETWEENSELECT * FROM products WHERE prod_price &gt; 10SELECT prod_id,prod_price FROM products WHERE prod_price &lt;&gt; 3.49;SELECT * FROM products WHERE prod_price BETWEEN 5 AND 10 --价格在5-10之间-- ORDER BY 位于 WHERE 之后，否则将会产生错误SELECT * FROM products WHERE prod_price &gt; 10 ORDER BY prod_name ----------------------------------------------------------------------------# 多条件过滤AND OR IN NOT###注意：AND运算优先级高于OR 所以一般是使用()SELECT * FROM products WHERE prod_price &gt; 10 AND vend_id = 1005-- 可以包含任意数目的AND和ORSELECT * FROM products WHERE prod_price &gt; 10 AND vend_id = 1005 AND prod_id = 'JP1000' SELECT * FROM products WHERE vend_id = 1002 OR vend_id = 1003-- AND的优先级高于OR，一般使用()来明确SELECT * FROM products WHERE prod_price &gt; 10 AND (vend_id = 1002 OR vend_id = 1003)SELECT * FROM products WHERE vend_id IN(1002,1003)-- NOT 否定跟在它之后的条件SELECT * FROM products WHERE NOT prod_price &gt; 10 -- MySQL中NOT对IN 、 BETWEEN 和EXISTS子句取反SELECT * FROM products WHERE vend_id NOT IN(1002,1003) --判断email是否为空SELECT cust_id,cust_email from customers WHERE cust_email IS NULL ---------------------------------------------------------------------------------# 通配符(% _)过滤数据LIKE#### 注意只能用于文本字段，非文本字段不能使用通配符匹配#### 使用通配符查询一般时间较长，所以不要过度使用通配符-- %表示任何字符出现任意次数SELECT * FROM products WHERE prod_name LIKE 'Jet%' -- 产品名称以Jet开头SELECT * FROM products WHERE prod_name LIKE '%Jet' -- 产品名称以Jet结尾SELECT * FROM products WHERE prod_name LIKE '%Jet%' -- 产品名称包含Jet的SELECT * FROM products WHERE prod_name LIKE 's%e' -- 产品名称以s开头，以e结尾-- _匹配单个任意字符SELECT * FROM products WHERE prod_name LIKE '_ ton anvil' -- ton anvil前只能有一个字符## 正则表达式过滤数据REGEXP### REGEXP和LIKE的区别在于LIKE必须使用通配符，否则就是直接找这个值，没有就返回NULLSELECT prod_name FROM products WHERE prod_name REGEXP '.000'SELECT prod_name FROM products WHERE prod_name LIKE '1000' -- 返回null SELECT prod_name FROM products WHERE prod_name REGEXP '1000|2000' -- 1000或2000SELECT prod_name FROM products WHERE prod_name REGEXP '[123] Ton' -- 1或2或3SELECT prod_name FROM products WHERE prod_name REGEXP '[^123] Ton' -- 非1 2 3SELECT prod_name FROM products WHERE prod_name REGEXP '[1-3] Ton' -- 1-3的范围内匹配SELECT prod_name FROM products WHERE prod_name REGEXP '\\\\.' -- 特殊字符使用\\\\转义SELECT prod_name FROM products WHERE prod_name REGEXP '[0-9]{4}' -- 其他正则表达式同理 创建计算字段和拼接CONCAT 计算字段+ - * / CONCAT拼接后通常使用AS指定别名，AS可省略 1234567891011# 拼接字段CONCATSELECT CONCAT(vend_name,'(',vend_country,')') FROM vendors -- 此时列名为CONCAT(vend_name,'(',vend_country,')')SELECT CONCAT(vend_name,'(',vend_country,')') AS 'vend_title' FROM vendors -- 使用别名后为vend_titleSELECT CONCAT(vend_name,'(',vend_country,')') 'vend_title' FROM vendors -- AS可省略# 计算字段 + - * /SELECT prod_id,quantity,item_price,quantity*item_price AS expanded_price FROM orderitems -- 列使用*计算并使用别名# SELECT还支持数据查询SELECT 2*3 #2*3 6SELECT NOW() #显示系统当前时间 数据处理函数数据处理函数 作用于列，主要分为普通函数和聚集函数。 普通函数： 文本处理函数 日期和时间处理函数 数值处理函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 文本处理函数-- Lower() 将串转换为小写-- Upper() 将串转换为大写-- Length() 返回串的长度-- LTrim() 去掉串左边的空格-- RTrim() 去掉串右边的空格-- Left() 返回串左边的字符-- Right() 返回串右边的字符-- Locate() 找出串的一个子串-- Soundex() 返回串的SOUNDEX值-- SubStr() 返回子串的字符SELECT vend_name,UPPER(vend_name) AS 'vendup_name' FROM vendors -- vend_name列返回大写SELECT vend_name,LENGTH(vend_name) AS 'vendname_len' FROM vendors -- vend_name列数据长度SELECT vend_name,SUBSTR(vend_name FROM 1 FOR 5)AS 'vendname_len' FROM vendors -- 子串# 日期和时间处理函数-- AddDate() 增加一个日期（天、周等）-- AddTime() 增加一个时间（时、分等）-- CurDate() 返回当前日期-- CurTime() 返回当前时间-- Date() 返回日期时间的日期部分-- DateDiff() 计算两个日期之差-- Date_Add() 高度灵活的日期运算函数-- Date_Format() 返回一个格式化的日期或时间串-- Day() 返回一个日期的天数部分-- DayOfWeek() 对于一个日期，返回对应的星期几-- Hour() 返回一个时间的小时部分-- Minute() 返回一个时间的分钟部分-- Month() 返回一个日期的月份部分-- Now() 返回当前日期和时间-- Second() 返回一个时间的秒部分-- Time() 返回一个日期时间的时间部分-- Year() 返回一个日期的年份部分SELECT * FROM orders WHERE MONTH(order_date) = '09'SELECT NOW() -- 返回系统当前时间# 数值处理函数-- Abs() 返回一个数的绝对值-- Cos() 返回一个角度的余弦-- Exp() 返回一个数的指数值-- Mod() 返回除操作的余数-- Pi() 返回圆周率-- Rand() 返回一个随机数-- Sin() 返回一个角度的正弦-- Sqrt() 返回一个数的平方根-- Tan() 返回一个角度的正切SELECT ABS(-1) -- 1 聚集函数 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 注意： 因为汇总函数结果只有一行，所以不能和其他字段名一块使用，但可以几个汇总函数一块使用(因为都是一行) AVG()和COUNT()会忽略NULL行 WHERE子句中不允许出现汇总函数 1234567891011121314151617181920212223SELECT AVG(prod_price) AS avg_price FROM products; -- 平均值SELECT MAX(prod_price) AS max_price FROM products; -- 最大值SELECT MIN(prod_price) AS max_price FROM products; -- 最小值-- 可以用COUNTT(*)用来计算行数SELECT COUNT(*) AS prod_count FROM products;SELECT SUM(quantity) AS sum_quatity FROM orderitems; -- 求和SELECT SUM(quantity*item_price) AS sum_money FROM orderitems; -- 求计算式的和-- AVG() COUNT() SUM()可以使用DISTINCT进行去重,MAX()和MIN()去重没有意义SELECT AVG(DISTINCT prod_price) AS avg_price FROM products;-- 5个函数可以一块使用SELECT COUNT(*) AS num_items, MIN(prod_price) AS price_min, MAX(prod_price) AS price_maxFROM products 分组数据GROUP BY 使用了GROUP BY进行分组后,SELECT语句中只允许出现分组字段和多行函数 如果是多字段分组，先按第一字段进行分组，后按照第二字段进行分组，依次类推 使用GROUP BY之后，筛选使用HAVING关键字。一定记清楚HAVING是对分组后进行筛选，WHERE是分组前对字段进行筛选 12345678910-- 查询不同供应商提供的产品总数SELECT vend_id,COUNT(*) FROM products GROUP BY vend_id;-- 查询不同供应商提供的不同价格的产品总数-- 先按第一个字段分组，再按第二个字段分组SELECT vend_id,prod_price,COUNT(*) AS prodtotal FROM products GROUP BY vend_id,prod_price;-- 分组后使用HAVING-- 查询不同供应商提供的产品总数&gt;1的信息SELECT vend_id,prod_price,COUNT(*) AS prodtotal FROM products GROUP BY vend_id,prod_price HAVING prodtotal&gt;1; 子查询 当查询条件不明确时，考虑使用子查询。这里的不明确是指没有具体的数值，需要经过一次或多次查询后才能获得的数值。 因为要进行比较，所以作为子查询的SELECT语句只能查询单个列，查询多个列时会出错。 单列返回多条结果，使用ALL ANY IN处理 1234567891011121314151617181920-- 查询比平均价格高的商品信息SELECT * FROM products WHERE prod_price &gt; (SELECT AVG(DISTINCT prod_price) FROM products);# ALL ANY IN-- 查询产品价格高于1001提供的所有产品信息(&gt;1001提供产品的最大值的产品信息)SELECT * FROM productsWHERE prod_price &gt; ALL(SELECT prod_price FROM products WHERE vend_id = '1001');-- 查询产品价格高于1001提供的任意产品信息(&gt;1001提供产品的最小值的产品信息)SELECT * FROM productsWHERE prod_price &gt; ANY(SELECT prod_price FROM products WHERE vend_id = '1001');-- 查询产品价格与1001提供的产品价格相同且低于4的产品信息SELECT * FROM productsWHERE prod_price IN (SELECT prod_price FROM products WHERE vend_id = '1001') AND prod_price&lt;4;# 子查询还可以作为计算字段SELECT cust_name,cust_state,(SELECT COUNT(*) FROM orders WHERE orders.cust_id = customers.cust_id) AS orders FROM Customers ORDER BY cust_name; 多表联结查询当需要查询的信息位于多张表时，则需要使用多表联结查询 笛卡尔积 CROSS JOIN (CROSS可省略) 自然联结 NATURAL JOIN 内联结(等值联结) INNER JOIN 左外联结 LEFT JOIN，实际是省略了OUTER (LEFT OUTER JOIN) 右外联结 RIGHT JOIN，实际是省略了OUTER (RIGHT OUTER JOIN) 笛卡尔积笛卡尔积也称直积，两个集合X和Y的笛卡尓积表示为X × Y，第一个对象是X的成员而第二个对象是Y的所有可能有序对的其中一个成员。例如集合A={a, b}，集合B={0, 1, 2}，则两个集合的笛卡尔积为{(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}。 在数据库中即为将多个表中的数据进行一一对应，所得到的结果为多表的笛卡尔积，结果数量为所有表的数量乘积。 12-- vendors6条记录，products14条记录，总共查询出84条记录SELECT vend_name,prod_name FROM vendors CROSS JOIN products 自然联结底层先笛卡尔积，然后按照所有的 同名同值 的字段自动进行 等值筛选 12345-- products表和vendors表有同名同值的vend_id外键,所以会自动根据vend_id进行筛选SELECT vend_name,prod_name FROM vendorsNATURAL JOIN products; -- 如果有多个同名同值的字段，则会按照所有的同名同值字段进行等值筛选 内联结底层先笛卡尔积，然后筛选，筛选条件为等值筛选和自然联结不同的是内联结可以指定筛选条件和筛选字段 指定筛选字段是说：如果有两个或多个外键，但是只想按照部分字段进行筛选，使用 USING 关键字 12345-- 如果有两个或多个外键，但是只想按照部分字段进行筛选，使用USING关键字--这个例子不是很恰当，因为只有一个外键SELECT vend_name,prod_name FROM vendorsINNER JOIN products USING(vend_id); 上面的指定筛选字段还是需要两个表的供应商id表的名称均为vend_id,如果products表中为provend_id,vendors 表中为vend_id使用USING则无法指定，这时候使用 ON 可以更灵活的指定筛选条件 123456-- 指定不同名称筛选字段-- 使用ON自行指定时可以省略INNERSELECT v.vend_name,p.prod_nameFROM vendors AS vINNER JOIN products AS pON v.vend_id = p.provend_id; 如果上面的table1和table2是同一张表。例如一张员工表，包含员工编号、姓名、工作、工资、上级领导，其中员工中包含上级领导。这种也叫自联结 12345# 查询员工姓名，工作，工资及上级领导姓名SELECT e1.ename,e1.job,e1.sal, e2.enameFROM emp e1INNER JOIN emp e2ON e1.mgr=e2.empno; 外联结 许多联结将一个表中的行与另一个表中的行相关联，但有时候需要包含没有关联行的那些行,则需要使用外联结。外联结分为左外联结，右外联结以及全外联结。 这里我们新建两张表，更好的体现左外联结和右外联结的作用。 左外联结左外联结就是保留左表没有关联的行 为了更直观的说明问题，我们新建两个表如下所示： 1234SELECT s.stu_name,m.m_subject,m.m_gradeFROM student sLEFT JOIN mark mON s.stu_id = m.stu_id 左外联结运行结果如下所示，可以看到mark表中没有5号同学，但因为是左外联结，仍然保留了5号同学这行数据。 右外联结右外联结就是保留右表没有关联的行 1234SELECT s.stu_name,m.m_subject,m.m_gradeFROM student sRIGHT JOIN mark mON s.stu_id = m.stu_id 运行结果和左外联结类似，不过这里是保留右表没有关联的行即8号同学的数据。 组合查询 UNION 使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。 每个查询必须包含相同的列、表达式和聚集函数。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 12345678910111213141516171819202122232425-- 基本使用SELECT vend_id,prod_id,prod_priceFROM productsWHERE prod_price &lt;=5UNIONSELECT vend_id,prod_id,prod_priceFROM productsWHERE vend_id IN (1001,1002)-- 和下面效果一样SELECT vend_id,prod_id,prod_priceFROM productsWHERE prod_price &lt;=5 OR vend_id IN (1001,1002)-- 使用UNION完成全外联结-- 全外联结是指既保留左侧未关联行，又保留右侧未关联行SELECT s.*,subject,scoreFROM student s LEFT JOIN mark mON s.id=m.id UNION SELECT s.*,subject,scoreFROM student s RIGHT JOIN mark mON s.id=m.id; SELECT语句顺序123456789SELECT 内容 FROM 表名1 别名1INNER JOIN 表名2 别名2ON 连接条件INNER JOIN 表名3 别名3ON 连接条件WHERE 普通筛选条件GROUP BY 分组HAVING 多行函数筛选ORDER BY 排序 插入数据INSERT 插入指定列数据没有指定的列需要在创建表时指明初始值或者可以为null 12INSERT INTO customers(cust_id,cust_name,cust_address,cust_city,cust_state,cust_zip,cust_country)VALUES('1000000006','Toy Land','123 Any Street','New York','NY','11111','USA'); 全字段填充可以省略列名 12INSERT INTO customersVALUES('1000000006','Toy Land','123 Any Street','New York','NY','11111','USA',NULL,NULL); 插入检索出的数据 12345INSERT INTO customers(cust_id,cust_contact,cust_email,cust_name,cust_address,cust_city,cust_state,cust_zip,cust_country)SELECT id,contact,email,name,address,city,state,cust_zip,cust_countryFROM custNew; 从一个表复制到另一个表 12CREATE TABLE custcopySELECT * FROM customers; 修改数据UPDATE1234567891011# 修改顾客contact email和addressUPDATE customersSET cust_contact = 'Sam Roberts',cust_email = 'sam@toyland.com',cust_address = '123 street'WHERE cust_id = '1000000006';# NULL来删除某个数据，与''不同UPDATE customersSET cust_email = NULLWHERE cust_id = '1000000006'; 删除数据DELETE123# 删除ID=10000000006的客户DELETE FROM customersWHERE cust_id = '1000000006'; TRUNCATE TABLE 可以清空表，也就是删除所有行，但表结构仍然保留 1TRUNCATE TABLE mytable; 注意： 对于上面增删改的数据SQL语句执行完毕后，不会立马进行数据的写入。还需要手动对数据进行提交，如果数据有问题还可以回滚。但是在MYSQL中是默认提交的。 12345-- 查看是否是自动提交 ON表示开启自动提交 OFF表示关闭SHOW GLOBAL VARIABLES LIKE 'autocommit' -- ON-- 使用autocommit = 0关闭自动提交，autocommit = 1开启自动提交SET GLOBAL autocommit = 0;SHOW GLOBAL VARIABLES LIKE 'autocommit' -- OFF 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。 二维表的创建与维护二维表的创建123456789101112CREATE TABLE 表名( 字段名1 字段类型 约束条件, 字段名2 字段类型 约束条件, ......)CREATE TABLE student ( stu_id INT AUTO_INCREMENT, stu_name VARCHAR(50) NOT NULL, stu_age INT(3) DEFAULT 18, PRIMARY KEY(stu_id))ENGINE=InnoDB DEFAULT CHARSET=utf8; 字段类型 字符串类型 数值类型 日期和时间数据类型 二进制数据类型 放在这里以供参考 约束条件 主键约束 PRIMARY KEY 外键约束 FOREIGN KEY 非空约束 NOT NULL 唯一约束 UNIQUE 默认约束 DEFAULT 主键约束： 主键非空唯一，一般与AUTO_INCREMENT一块使用 每个表最多只允许一个主键，建立主键约束可以在列级别创建，也可以在表级别创建。 当创建主键的约束时，系统默认会在所在的列和列组合上建立对应的唯一索引。 12345678910111213141516171819202122232425262728293031# 基本模式CREATE TABLE temp ( t_id INT AUTO_INCREMENT, t_name VARCHAR (20), PRIMARY KEY(t_id));# 组合模式CREATE TABLE temp ( t_id INT(10), t_name VARCHAR (20), t_pwd VARCHAR (20), PRIMARY KEY (t_id,t_name));/*注意：组合模式不是设置了两个主键，而是几个字段组合起来作为一个主键比如学生选课系统：学生表student（sno，sname，……）课程表course（cno，cname，……）选课表sc（sno，cno，grade） 学生表的学号sno为主键，课程表的课程号cno为主键，而选课表是以（sno，cno）才能作为主键*/# 删除主键约束ALTER TABLE temp DROP PRIMARY KEY;# 添加主键约束ALTER TABLE temp ADD PRIMARY KEY (t_id, t_name);# 修改主键约束ALTER TABLE temp MODIFY t_id INT PRIMARY key; 外键约束当一张表的某个字段的值需要依赖另外一张表的某个字段的值，则使用外键约束其中主动依赖的表称为子表，被依赖的表称为父表。外键加在子表中 作用：当在子表中插入的数据在父表中不存在，则会自动报错 12345678910111213141516171819202122232425262728293031323334353637383940414243-- 基本模式-- 主表CREATE TABLE temp ( t_id INT AUTO_INCREMENT, t_name VARCHAR (20), PRIMARY KEY(t_id));-- 副表CREATE TABLE temp2 ( t2_id INT AUTO_INCREMENT, t_id INT NOT NULL, t2_name VARCHAR (20) NOT NULL, FOREIGN KEY (t_id) REFERENCES temp (t_id));-- 多列外键组合，必须用表级别约束语法-- 主表CREATE TABLE classes ( c_id INT, c_name VARCHAR (20), c_number INT, PRIMARY KEY (c_name, c_number));-- 副表CREATE TABLE student ( s_id INT auto_increment PRIMARY KEY, s_name VARCHAR (20), c_name VARCHAR (20), c_number INT, /*表级别联合外键*/ FOREIGN KEY (c_name,c_number) REFERENCES classes (c_name, c_number));-- 删除外键约束ALTER TABLE student DROP FOREIGN KEY s_id;-- 增加外键约束ALTER TABLE student ADD FOREIGN KEY ( c_name, c_number) REFERENCES classes (c_name, c_number); 非空约束 非空约束用于确保当前列的值不为空值，非空约束只能出现在表对象的列上。 Null类型特征：所有的类型的值都可以是null，包括int、float 等数据类型 123456789101112131415-- 创建table表，ID 为非空约束，name 为非空约束 且默认值为abcCREATE TABLE temp ( c_id INT(10) NOT NULL, c_name VARCHAR (50) NOT NULL DEFAULT 'abc', c_sex CHAR NULL) ;-- 增加非空约束ALTER TABLE temp c_MODIFY sex VARCHAR (2) NOT NULL;-- 取消非空约束ALTER TABLE temp MODIFY c_sex VARCHAR (2) NULL;-- 取消非空约束，增加默认值ALTER TABLE temp MODIFY c_sex VARCHAR (2) DEFAULT 'abc' NULL; 唯一约束 唯一约束是指定table的列或列组合不能重复，保证数据的唯一性。 唯一约束不允许出现重复的值，但是可以为多个null。 同一个表可以有多个唯一约束，多个列组合的约束。 12345678910111213141516-- 创建表时设置，表示用户名、密码不能重复CREATE TABLE temp ( t_id INT NOT NULL, t_name VARCHAR (20), t_pwd VARCHAR (10), UNIQUE (t_name, t_pwd));-- 添加唯一约束ALTER TABLE temp ADD UNIQUE (t_name, t_pwd);-- 修改唯一约束ALTER TABLE temp MODIFY t_name VARCHAR (25) UNIQUE;-- 删除约束ALTER TABLE temp DROP INDEX t_name; 默认约束 123456-- 名字默认为abcCREATE TABLE temp ( t_id INT NOT NULL, t_name VARCHAR (255) NOT NULL DEFAULT 'abc', t_sex CHAR NULL); 二维表维护 使用 ALTER 关键字 ADD 添加 , DROP 删除, MODIFY 修改(修改字段名使用 CHANGE ) 添加新的字段 12# 添加prod_haha字段ALTER TABLE products ADD prod_haha VARCHAR(10) NOT NULL; 修改原字段 12345# 修改字段名使用CHANGEALTER TABLE products CHANGE prod_haha prod_quatity VARCHAR(10);# 修改字段类型(COLUMN可省略)ALTER TABLE products MODIFY COLUMN prod_quatity VARCHAR(20); 删除原有字段 12# 删除原有字段(COLUMN可省略)ALTER TABLE products DROP COLUMN prod_quatity; 修改表名 1234#方式1ALTER TABLE orders RENAME TO orders2;#方式2RENAME TABLE orders2 TO orders; 删除二维表 1DROP TABLE custcopy;","link":"/2020/04/06/SQL%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E4%B9%8B%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/"},{"title":"数据库锁理论","text":"主要内容 事务的ACID特性 并发一致性问题 封锁 事务的隔离级别 MVCC+Next-Key Locks 索引和锁 之前我们学习事务时，只是简单的介绍了事务是一组原子性的SQL语句，要么全部执行，要么全部不执行。但仅仅了解这些是不够的。事务的ACID特性和事务的隔离级别等都是十分重要的概念，只有理解清楚这些概念，我们才能根号的使用事务。 事务的ACID特性1.原子性（Atomicity）一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚。这就是事务的原子性。 2.一致性（Consistency） 数据库在事务执行前后是从一个一致性状态转移到另一个一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。 3.隔离性（Isolation）通常来说，一个事务所做的修改在最终提交以前，对其它事务是不可见的。 4.持久性（Durability）一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对系统崩溃的情况。 并发一致性问题在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。 丢失修改T1和T2两个事务都对一个数据进行修改，T1先修改，T2后修改，T2的修改覆盖了T1的修改。 脏读脏读就是事务可以读取未提交的数据。 T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 不可重复读T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 幻影读当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时会发现其他事物插入了满足查询条件的新数据，这种现象称为幻影读。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 产生并发不一致性问题的主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。 虽然实际使用过程中我们很少自己手动封锁，但是了解了封锁的过程有助于我们更好的理解下面事务的隔离级别。所以我们这里先了解下封锁的相关知识。 封锁封锁粒度MySQL 中提供了两种封锁粒度：行锁和表锁。 顾名思义，表锁就是在用户进行写操作(增删改)时锁定整张表。而行锁就是只锁定需要操作的行。 我们知道锁定的数据量越小，发生锁争用的可能就越小，系统的并发程度就越高。所以行锁的并发性高于表锁。但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。所以行锁的系统开销高于表锁。因此在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。 MySQL中InnoDB引擎使用的是行锁，MyISAM引擎使用的是表锁。 封锁类型(读写锁)互斥锁（Exclusive）:简写为 X 锁，又称写锁。共享锁（Shared）:简写为 S 锁，又称读锁。 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 简单一句话就是： 读读共享、 读写 写读 写写互斥 封锁协议一级封锁协议事务T在修改数据A之前必须先对其加X锁，直到事务结束才释放。事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。 注意这里是修改数据时加X锁，读取数据不进行修改时不加锁。所以不能保证可重复读和不读脏数据。 一级封锁协议可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。 二级封锁协议在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。 但在二级封锁协议中，由于读完数据后即可释放S锁，所以它不能保证可重复读。 三级封锁协议在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。 可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。 一次封锁or两段锁？一次封锁协议：因为有大量的并发访问，为了预防死锁，一般应用中推荐使用一次封锁法，就是在方法的开始阶段，已经预先知道会用到哪些数据，然后全部锁住，在方法运行之后，再全部解锁。这种方式可以有效的避免循环死锁，但在数据库中却不适用，因为在事务开始阶段，数据库并不知道会用到哪些数据。 InnoDB引擎使用的是两段锁协议，即将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁） 加锁阶段：在该阶段可以申请获得任何数据行的任何类型的锁，但是不能释放锁。比如在对任何数据进行读操作之前要申请并获得S锁，在进行写操作之前要申请并获得X锁。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。 解锁阶段：事务可以释放任何类型的锁，但是不能加锁。锁只有在执行commit或者rollback时才会释放。 12345# 遵循两段锁协议lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B)# 不遵循两段锁协议lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) 图片来自博客 https://tech.meituan.com/2014/08/20/innodb-lock.html 两段锁协议无法避免死锁，但是可以保证事务的并发调度是串行化的（串行化很重要，尤其是在数据恢复和备份的时候）。 串行化调度是指如果一个调度的动作首先是一个事务的所有动作，然后是另一个事务的所有动作。以此类推，而没有动作的混合，就是串行化调度。 事务遵循两段锁协议是保证可串行化调度的充分条件，但不是必要条件。 12345# 遵循两段锁协议 串行化调度lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B)# 不遵循两段锁协议 仍然是串行化调度lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) 隐式和显式锁定InnoDB使用两段锁协议，在事务执行过程中，随时都可以锁定，锁只有在执行commit或者rollback时才会释放。这些说的都是隐式锁定，InnoDB会根据隔离级别在需要的时候自动加锁。另外InnoDB也支持通过特定的语句进行显式锁定: 12select ... from table where ? lock in share mode; -- s锁select ... from table where ? for update; -- x锁 乐观锁和悲观锁悲观锁总是假设最坏的情况，每次在拿数据的时候，都认为会被别人修改，所以每次拿数据都会上锁，这样别人想拿这个数据的时候就会阻塞 悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 我们上面学习的x锁和s锁都是悲观锁。 乐观锁总是假设最好的情况，每次在拿数据的时候，都认为不会被别人修改，所以拿数据的时候都不会上锁，但是在更新的时候会判断一下，在此期间有没有别人更新这个数据 乐观锁，大多是基于数据版本（ Version ）记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 我们之后要学习的MVCC就是使用了乐观锁的思想。 事务的隔离级别在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 未提交读（READ UNCOMMITTED） 事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED） 一个事务只能读取已经提交的事务所做的修改。 可重复读（REPEATABLE READ） 在同一个事务中多次读取同一数据的结果是一样的。MySQL事务的默认隔离级别 可串行化（SERIALIZABLE） 完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 MySQL数据库能够识别四种隔离级别，并且InnoDB引擎也支持上面四种隔离级别。MySQL默认隔离级别是可重复读，可以通过如下方法设置隔离级别： 12# 查看当前事物级别：SELECT @@tx_isolation; 1234567891011# 设置read uncommitted级别：set session transaction isolation level read uncommitted;# 设置read committed级别：set session transaction isolation level read committed;# 设置repeatable read级别：set session transaction isolation level repeatable read;#设置serializable级别：set session transaction isolation level serializable; 接下来我们通过demo验证一下我们比较常用的提交读和可重复读的隔离级别： 12345678910111213# 建表 CREATE TABLE `class_teacher` ( `id` int(11) NOT NULL AUTO_INCREMENT, `class_name` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL, `teacher_id` int(11) NOT NULL, PRIMARY KEY (`id`), KEY `idx_teacher_id` (`teacher_id`)) ENGINE=InnoDBCHARSET=utf8# 插数据insert into class_teacher (id,class_name,teacher_id) values(default,'初三一班',1);insert into class_teacher (id,class_name,teacher_id) values(default,'初二一班',2);insert into class_teacher (id,class_name,teacher_id) values(default,'初二二班',2); 首先我们来看提交读级别下的不可重复读： 1234# 设置为提交读级别set session transaction isolation level read committed;# 关闭自动提交set autocommit = 0; 事务B修改id=1的数据提交之后，事务A同样的查询，后一次和前一次的结果不一样，这就是不可重读（重新读取产生的结果不一样）。这就很可能带来一些问题。 接着我们来看看在可重复读级别中MySQL的表现： 12# 设置为可重复读级别set session transaction isolation level repeatable read; 我们注意到，当teacher_id=1时，事务A先做了一次读取，事务B中间修改了id=1的数据，并commit之后，事务A第二次读到的数据和第一次完全相同。所以说它是可重读的。 多版本并发控制(MVCC)之前我们已经了解了事务的隔离级别，那么数据库是如何实现这种隔离级别的呢？ 实际上基于提升并发性能的考虑，大部分数据库是通过实现多版本并发控制(MVCC)这种乐观锁的方法来进行并发的保证，而不是简单的使用我们上面提到的x锁和s锁。因为MVCC没有一个统一的实现标准，所以我们这里主要介绍MySQL的InnoDB引擎的MVCC的实现。 在InnoDB中，会在每行数据后添加两个额外的隐藏的列来实现MVCC，这两列一个记录这行数据的创建时间，另外一个记录这行数据过期时间（或者删除时间）。 在实际操作中，存储的并不是时间，而是版本号,即创建版本号和删除版本号。 事务版本号 TRX_ID ：事务开始时的系统版本号。每一个新事务都会使系统版本号加1。所以新事务等于上一个事务的版本号+1。 在可重复读Repeatable reads事务隔离级别下： SELECT时，读取创建版本号&lt;=当前事务版本号并且删除版本号为空或&gt;当前事务版本号的数据行。 INSERT时，保存当前事务版本号为行的创建版本号 DELETE时，保存当前事务版本号为行的删除版本号 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 更加底层的可以参考这篇文章 里面详细介绍了undo log和版本链,上面是undo log和版本链的简单理解。 我们上面的表格中提到过可重复读级别可以解决不可重复读的问题，但无法解决幻影读的问题，而只有在Serializable级别才能解决幻读。但是测试后发现，MySQL的可重复读级别中，是解决了幻影读的读问题的。 这里需要着重强调两点：1.我们说的是解决了幻影读的读问题，而不是说直接解决了幻影读问题。至于为什么，这里先卖下关子，下文会讲到。 2.不可重复读和幻影读的区别:不可重复读重点在于update和delete，而幻影读的重点在于insert如果用锁机制来解释的话，就是在可重复读级别时update和delete时会对这些数据加锁，从而实现可重复读。但这种方法无法锁住insert的数据，所以会出现幻影读的问题。 快照读和当前读上面我们提到了在可重复读级别下，MVCC解决了幻影读的读问题，而不是说解决了幻影读问题。是因为MySQL中的读，和事务隔离级别中的读，是不一样的。事务的隔离级别虽然都是对于读数据的定义，但在这里，就被拆成了读和写两个模块来讲解。 在可重复读级别中，通过MVCC机制，虽然让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。在MVCC中： 快照读：读取历史数据的方式就是select 1select * from table ….; 当前读：读取数据库当前版本数据的方式。 特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。 123456insert;update ;delete;# 手动加锁select * from table where ? lock in share mode; -- s锁select * from table where ? for update; -- x锁 事务的隔离级别中虽然只定义了读数据的要求，实际上这也可以说是对写数据(当前读)的要求。到这里应该清楚了，我们上面学习的MVCC解决的幻影读的读问题，实际是讲的快照读。 而为了解决写(当前读中)的幻影读问题，MySQL事务使用了Next-Key锁。 Next-Key LocksNext-Key锁是行锁和GAP（间隙锁）的合并，行锁上面已经介绍了，接下来说下GAP间隙锁。 间隙锁(Gap locks)： 锁定索引之间的间隙，但是不包含索引本身。 我们先看下在提交读和可重复读下的对比 提交读级别: 可重复读级别： 通过对比我们可以发现，在提交读级别中，事务A修改了所有teacher_id=30的数据，但是当事务Binsert进新数据后，事务A发现莫名其妙多了一行teacher_id=30的数据，而且没有被之前的update语句所修改，这就是“当前读”的幻读。 可重复读级别中，事务A在update后加锁，事务B无法插入新数据，这样事务A在update前后读的数据保持一致，避免了幻读。这个锁，就是间隙锁。 间隙锁在MySQL是这么实现的： 在class_teacher这张表中，teacher_id是个索引，那么它就会维护一套B+树的数据关系，为了简化，我们用链表结构来表达（实际上是个树形结构，但原理相同） Innodb将这段数据分成几个个区间 123(negative infinity, 5],(5,30],(30,positive infinity)； update class_teacher set class_name=‘初三二班’ where teacher_id=30;不仅用行锁，锁住了相应的数据行；同时也在两边的区间，（5,30]和（30，positive infinity），都加入了间隙锁。这样事务B就无法在这个两个区间insert进新数据。 受限于这种实现方式，Innodb很多时候会锁住不需要锁的区间。如下所示： update的teacher_id=20是在(5，30]区间，即使没有修改任何数据，Innodb也会在这个区间加gap锁，而其它区间不会影响，事务C正常插入。 行锁防止别的事务修改或删除，间隙锁防止别的事务新增，行锁和间隙锁结合形成的的Next-Key锁共同解决了可重复读级别在写数据时的幻影读问题。 索引和锁上面我们的demo都是建立在teacher_id是索引的情况下(我们建表时建了索引)： 1KEY `idx_teacher_id` (`teacher_id`) 但是如果我们因为自己写的select语句导致索引失效，或者说就没有建立索引比如比如update class_teacher set teacher_id=7 where class_name=‘初三八班（即使没有匹配到任何数据）’,那么MySQL会给整张表的所有数据行的加行锁，同时会给全表加入间隙锁。 对于行锁而言，整个加锁过程如下: 如果没有使用索引，MySQL并不知道哪些数据行是class_name = ‘初三八班’的数据行，如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由MySQL服务层进行过滤。 在MySQL Server过滤条件，发现条件不满足后，会调用unlock_row方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。 这种情况同样适用于MySQL的默认隔离级别可重复读。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL服务层过滤数据的的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象。 对于间隙锁而言，整个加锁过程如下:它不能像上文中行锁一样经过MySQL服务层过滤自动解除不满足条件的锁，因为没有索引，则这些字段也就没有排序，也就没有区间。除非该事务提交，否则其它事务无法插入任何数据。 这个很简单，就不再演示了。","link":"/2020/04/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%94%81%E7%90%86%E8%AE%BA/"},{"title":"深入理解MySQL","text":"主要内容 MySQL系统架构图 MyISAM引擎和InnoDB引擎 MySQL系统架构图和其他数据库相比，MySQL有点与众不同，它的架构可以在多种不同的场景中应用并发挥良好作用。主要体现在存储引擎的架构上。插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。 下图为MySQL的逻辑架构图： 我们简要分析下每层的作用：支持接口是第三方语言对数据库的操作接口，比如Java的JDBC等，这里不再赘述。 (1)连接层 最上层的连接池是提供一些连接服务，包含本地socket通信和大多数基于C/S工具实现的类似于TCP/IP的通信。我们常用的c3p0和druid连接池就是这一层的。连接池主要完成一些类似于连接处理、授权认证及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全连接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 (2)服务层 第二层架构主要完成大多数的核心服务功能，如SQL接口、缓存的查询、SQL的分析和优化、内置函数等。所有跨存储引擎的功能也在这一层实现，如存储过程、触发器、视图等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在频繁读操作的环境中能够很好的提升系统的性能。 (3)引擎层 存储引擎真正的负责MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信，不同的存储引擎具有的特性不同，我们可以根据实际需进行选取。MySQL中支持10多种的存储引擎，比较常用的就是MyISAM和InnoDB。下文会做具体介绍。 (4)存储层数据存储层，主要是将数据存储在物理机的文件系统，并完成与存储引擎的交互。 MySQL存储引擎MySQL中支持10多种的存储引擎，比较常用的就是MyISAM和InnoDB。MySQL在5.1版本之前默认存储引擎为MyISAM，在此版本之后默认为InnoDB。 可以使用如下命令查看MySQL中默认安装的存储引擎 1SHOW ENGINES; 使用如下命令查看正在使用的存储引擎 1SHOW VARIABLES LIKE '%storage_engine%'; 可以看到，我们正在使用的是InnoDB引擎 接下来我们就对比下我们常用的两个存储引擎InnoDB引擎和MyISAM引擎。 InnoDB引擎和MyISAM引擎对比这里把两种最常用的存储引擎基本的提点通过表格的形式对比如下: 除了上面常用的两种存储引擎外，MySQL中还有一些特殊用途的存储引擎，比如只支持INSERT和SELECT的Archive引擎等，这里就不再赘述，有需要可以去官网查看文档。 另外MySQL从2007年开始提供了插件式的存储引擎API，所以出现了很多第三方存储引擎，如InnoDB的改进版Percona等。 如何选择存储引擎从上面我们知道，MySQL中的存储引擎有非常多，那么如何选择适合业务需求的存储引擎呢? 主要从以下几方面考虑: 使用场景是否需要事务支持； 是否需要支持高并发，InnoDB的并发度远高于MyISAM； 是否需要支持外键； 是否需要支持在线热备； 高效缓冲数据，InnoDB对数据和索引都做了缓冲，而MyISAM只缓冲了索引； 索引，不同存储引擎的索引并不太一样； 总之可以简单的归纳为一句话:除非需要用到某些InnoDB不具备的特性，并且没有其他办法可以替代，否则都应该优先选择InnoDB引擎。","link":"/2020/04/10/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3MySQL/"},{"title":"分布式MySQL","text":"","link":"/2020/04/23/%E5%88%86%E5%B8%83%E5%BC%8FMySQL/"},{"title":"操作系统基础","text":"之前笔记在github上，这里直接贴上链接: 概述 启动与系统调用 进程与线程 内存管理 文件管理 I/O管理 参考资料 Tanenbaum A S, Bos H. Modern operating systems[M]. Prentice Hall Press, 2014. Bryant, R. E., &amp; O’Hallaron, D. R. (2004). 深入理解计算机系统. 计算机操作系统[M]. 西安电子科技大学出版社 操作系统(李治军老师) 配套实验","link":"/2020/04/28/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/"},{"title":"索引优化策略和SQL语句优化","text":"主要内容 建立合理高效的索引 索引优化策略(最佳左前缀法则) 建议看这篇文章之前，先看下我之前写的两篇文章： 索引和查询性能的优化 explain关键字详解 建立合理高效的索引什么情况需要建索引 主键自动建立唯一索引 查询中与其他表的关联字段，即外键建立索引，左外联右表建索引，右联结左表建索引 频繁作为查询条件的字段(WHERE字段)应该建立索引 查询中排序的字段，排序字段(ORDER BY字段)通过索引访问将大大提高排序速度 查询中统计或者分组字段(GROUP BY字段) 什么情况不需要建索引 表记录太少不需要建索引(MySQL官方支持500-800w条记录,但实际300w条记录MySQL性能会明显下降) 频繁增删改的表不需要建索引(每次更新不仅仅是更新记录，也会更新索引) 数据重复且分布平均的列不需要建索引(比如性别列只有男女,且男女比例分布为50%,那么建立索引一般不会提高数据库的查询速度。 这里涉及到一个索引的选择性问题。索引选择性是指索引列中不同值的数目与表中记录数的比。这个值越接近于1，索引的效率就越高。) demo参考文章 https://www.cnblogs.com/developer_chan/p/9219239.html 这里就不再演示。 单列索引还是复合索引？这个问题是什么意思呢？就以我们下面建的staff表为例，表中有属性： id、name、age、pos、salary。id这里就不说了，主键自动建立索引。比如我们发现经常通过name、age、pos查，那么是为每个列单独创建一个索引，即name列、age列、pos列分别建一个索引，一共三个索引，还是建立一个复合索引(name,age,pos)呢?(这里先不讨论复合索引的顺序) 答案是能复合索引就复合索引 原因在于多个单列索引在多条件查询时只会生效第一个索引(5.0版本后，虽然MySQL会同时使用几个单列索引做扫描并将结果合并，但是索引合并时会消耗大量的cpu和内存资源。另一方面也说明我们自己建的索引糟糕)。 而使用复合索引进行多条件查询时，只要复合最佳左前缀法则，那么就会用到多个索引列，从而增加查询的速度。所以多条件联合查询时最好使用联合索引！ 具体的demo这里也就不演示了，可以参考这篇文章(只看看实验就好了)。 索引优化策略这里先准备我们的测试用例： 1234567891011121314151617# 建表DROP TABLE IF EXISTS staff;CREATE TABLE IF NOT EXISTS staff ( id INT PRIMARY KEY auto_increment, name VARCHAR(50), age INT, pos VARCHAR(50) COMMENT '职位', salary DECIMAL(10,2));# 插入数据INSERT INTO staff(name, age, pos, salary) VALUES('Alice', 22, 'HR', 5000);INSERT INTO staff(name, age, pos, salary) VALUES('Bob', 22, 'RD', 10000);INSERT INTO staff(name, age, pos, salary) VALUES('David', 22, 'Sale', 120000);# 建立name、age、pos的复合索引CREATE INDEX idx_staff_nameAgePos ON staff(name,age,pos); WHERE子句的优化(到GROUP BY之前) 1.全值匹配全值匹配是指和索引中的所有列进行匹配。比如我们上面建立了name，age，pos的复合索引，查询时用到索引中的所有列，即name，age，pos三个列都用到。 2.最佳左前缀法则 如果索引了多列，要遵守最佳左前缀法则。即查询从索引的最左前列开始并且不跳过索引中的列。 带头大哥不能死 中间兄弟不能断 下面的例子中很好的体现了正确使用最佳左前缀法则的例子，并且从explain中看到都是使用到了我们建立的索引,并且随着条件的进一步准确，用到的索引列也就越多。 但是如果不是按照索引的最左列开始查找(这个例子中是name列)，则会使我们的索引失效,即开头大哥不能死。 同样，如果我们只用到了1和3列，跳过了索引中的第2列，那么索引中只用到了第1列(只有一个const)。即中间兄弟不能断 3.不在索引列上做任何操作(计算、函数、自动类型转换) 下面的例子中对id列做了计算操作，从而使得索引失效，转而执行全表扫描。 4.存储引擎不能使用索引中范围右边的列 下面的例子中也可以看出来，从=变为&gt;则会使索引访问从ref降低为range。但是从key列中我们也可以看到，范围中也使用到了索引。其实这里索引使用到了name列和age列两列（从key_len可以看出来），其中name列用于查询，age列用于排序。 5.尽量使用覆盖索引，减少select * 覆盖索引：只访问索引的查询(查询列覆盖索引列)，即查询只需要访问索引，而无需访问数据行。 当然一般不可能全部覆盖，都是只覆盖一部分 6.mysql使用！= 或者&lt;&gt;会导致索引失效 7.is null和is not null会导致索引失效 is null之前的版本中是无法使用索引的，这次使用的5.7版本中可使用到索引，应该是mysql底层做了优化。而is not null 还是不能够使用索引 8.like也是范围匹配的另一种形式。%号要加在右边才能用到索引。不会使右边的索引失效。 和范围右边的索引列失效不同的是，如果%放在右边不会使之后的索引列失效 我们知道%a% 和%a 以及 a%可能匹配到的数据是不一样的，如果我们必须要使用%在前面的即%a% 和 %a才能查出准确的数据，那如何解决%必须放在前面的问题? 使用覆盖索引解决： 9.varchar类型的字段不加单引号，mysql会进行自动类型转换(参考第3点)，导致索引失效。 10.or会导致索引失效 到这里，基本对于where的优化，基本就讲完了，但是看一下接下来这两个例子可能让你怀疑人生： 第一个SQL语句中，并没有按照索引列的顺序使用索引，但是为什么我们的复合索引(name,age,pos)都用到了呢？ 我们的最佳左前缀法则呢？其实这里是MySQL内部对我们的SQL语句进行了优化，找到最合适的索引。同样下面的范围查找也是同理。 但是，我们实际写的时候为了语义明确尽量按复合索引的索引列顺序进行书写。 where优化总结 《高性能MySQL》中对于索引的说明，已经包含在了上面的例子中，这里给出参考: 索引对如下类型有效：-全值匹配（参考1.）-匹配最左前缀（参考2.）-匹配列前缀（参考8.like）-匹配范围值（参考4.范围匹配和8.like）-精确匹配某一列并范围匹配另外一列（参考4. 8.）-只访问索引的查询（参考5.覆盖索引） B+树本身的机构问题导致的索引失效-如果不是按照索引的最左列开始，则无法使用索引(参考2)-不能跳过索引中的列（参考2）-如果查询中某个列有范围，则其右边所有列都无法使用索引（参考4. 8.） MySQL优化器和存储引擎使用索引方式导致的索引失效(参考3.6.7.9.10)这些可能会在之后的MySQL优化中取消.比如上面的is null之前是不能使用索引，5.7版本后就可以使用了。 从上面我们也可以看到对于索引的使用，关键还是要注意：全值匹配、最佳左前缀法则、范围匹配(&gt; &lt; 以及like)、覆盖索引这几个关键部分。 分组(GROUP BY)和排序(ORDER BY)的优化到目前为止，我们上面的例子都是针对where字句之后，group by和order by之前进行的优化。我们知道索引不仅仅能用来查询，还能用来分组(GROUP BY)和排序(ORDER BY)，接下来我们就看看这一部分的优化吧。 使用索引来做排序，我们只需要关注一个重点就是是否会产生filesort文件内排序。 ORDER BY中只有两种方式可以使用索引进行排序： 1. ORDER BY语句使用索引最左前缀法则 正确使用最佳左前缀法则，避免了文件排序 错误使用最佳左前缀用于排序，产生了文件内排序，降低了效率。 2. WHERE子句中前导列为常量时或者join子句中对这些列指定了常量从而构成最佳左前缀 前两个SQL语句中where子句中都是指定了常量和order by子句中的列组成了最左前缀，避免了filesort。对于第3条SQL语句，我们知道范围右边失效，所以同样由name age pos构成最左前缀，避免filesort。而第4条中由于where子句中不是常量，无法构成最佳左前缀所以产生filesort。 另外对于排序而言，全升序或全降序不会产生file sort，而只要有一个不同则会产生file sort 分组(GROUP BY)和排序(ORDER BY)优化总结1.ORDER BY子句尽可能避免using filesort方式排序2.尽可能在索引列上完成排序操作，遵照最佳左前缀法则。下表是对上面例子的总结。 3.如果不在索引列上，filesort有两种算法：双路排序和单路排序。 双路排序：在MySQL4.1之前使用双路排序。简单的讲就是从磁盘读取排序字段、在buffer进行排序、再按照buffer中拍好的序从磁盘中读取其他字段。总的来说进行了两次磁盘扫描，得到最终数据。 单路排序：从磁盘中查询所需的列，按照order by列在buffer中对它们进行排序，然后按照排序后的列表进行输出。它避免了第二次读取数据，并且把随机I/O变成了顺序I/O，但是会使用更多的空间，因为它把所有字段都保存在内存中了。 如果使用双路排序，取一批数据要对磁盘进行两次扫描，众所周知，I/O操作是很耗时的，因此出现了改进的算法：单路排序。 但是在单路排序中，当读取数据超过sort_buffer的容量时，就会导致多次读取数据，并创建临时表，最后多路合并，产生多次I/O，反而增加其I/O运算。 解决方法： 避免select * 增大sort_buffer_size参数的设置 增大max_length_for_sort_data参数的设置。 原因在于： 4.group by与order by很类似，其实质是先排序后分组， 遵照索引创建顺序的最佳左前缀法则 当无法使用索引列的时候，也要对sort_buffer_size和max_length_for_sort_data参数进行调整。 注意where高于having，能写在where中的限定条件就不要去having限定了","link":"/2020/04/16/%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/"},{"title":"查询截取分析","text":"查询截取分析流程1. 慢查询的开启并捕获2. explain+慢SQL分析3. show profile查询SQL在MySQL服务器里面的执行细节和生命周期情况4. SQL数据库服务器的参数调优 如果我们发现是SQL语句导致的我们系统变慢，首先需要做的是通过我们的慢查询日志定位我们的问题SQL语句，然后通过explain分析原因，并进行优化。如果无法找到原因，则需要借助更加详细的show profile进行分析和优化。最终都找不到原因，则考虑进行数据库服务器的参数调优。 之前我们学习索引的时候已经学习了explain，接下来我们就主要看看使用慢查询日志以及show profile。 慢查询日志分析之前我们学习了explain来分析SQL语句，但是我们写了很多条SQL语句，哪条语句导致的系统变慢呢？我们是不是需要把所有的SQL语句都分析一遍呢？答案肯定是不可能的。问题在于如何找出我们出问题的SQL语句，这时候就需要用到我们的慢查询日志。 慢查询日志是MySQL提供的一种日志记录，它记录MySQL中响应时间超过阈值的语句，具体指运行时间超过long_query_time值的sql语句，该sql语句会被记录到慢查询日志中。 1.查看是否开启慢查询默认情况下MySQL没有开启慢查询日志，需要我们手动设置。当然，如果不是调优需要的话，一般不建议启动该参数。 可以通过下面的命令查看是否开启慢查询日志： 1show variables like '%slow_query_log%'; 可以看到，MySQL没有开启慢查询日志，并且慢查询日志信息保存在主机名-slow.log文件下。 手动开启慢查询日志： 1set global slow_query_log = 1; 这种使用命令的方式只对当前数据库有效，如果MySQL重启后则会失效。如果需要永久生效，则必须修改mysql的配置文件my.cnf的配置文件。[mysqld]下添加参数： 12slow_query_log=1slow_query_log_file=/var/lib/mysql/centos7-slow.log 2. 查看和设置慢查询的阈值时间上面我们已经开启了慢查询日志，但是多慢的SQL语句会被记录下来呢？ 可以通过如下命令查看： 1show variables like 'long_query_time%'; 可以看到MySQL中默认是10s。这里需要注意是寻找大于 long_query_time 而不是大于等于 我们可以手动进行设置这个时间（在my.cnf中也可以修改）。 1set global long_query_time=3; 上面我们设置查询时间超过3秒为慢查询，但是我们看到sql语句执行成功，但是我们再次查询后还是初始时的10s。难道我们设置的无效吗？实际上这时候已经生效了。我们可以通过show global variables like 'long_query_time%';或者新开一个会话进行查看。 这里我们新开一个会话，执行一个4秒钟的sql语句: 接着我们去var/lib/mysql/centos7-slow.log下面查看： 可以看到慢查询日志中记录了超过阈值的mysql语句，这样我们就知道了哪条sql语句执行较慢，从而针对这条sql语句进行分析和优化。 这里贴一下配置文件中如何配置吧。在[mysqld]下进行如下配置就和我们上面通过命令行的方式相同了。 1234slow_query_log=1;slow_query_log_file=/var/lib/mysql/centos7-slow.log;long_query_time=3;log_output=FILE 3.慢查询日志分析工具mysqldumpslow上面只是做了个演示，实际上在生产环境中，如果要手工分析日志，查找分析SQL语句显然是个体力活，MySQL也给我们提供了日志分析工具mysqldumpslow。 使用时可以通过下面命令查询参数： 1mysqldumpslow --help 比较常用的参数有： s：是表示按照何种方式排序 c: 访问次数 l：锁定时间 r: 返回记录 t：查询时间 al: 平均锁定时间 ar: 平均返回记录数 at: 平均查询时间 t: 返回前面多少条的数据 g: 后面搭配一个正则表达式，大小写不敏感的 使用时可以参照下例： 1234567891011得到返回记录集最多的10个SQLmysqldumpslow -s r -t 10 /var/lib/mysql/centos7-slow.log得到访问次数最多的10个SQLmysqldumpslow -s c -t 10 /var/lib/mysql/centos7-slow.log得到按照时间排序的前10条里面含有左连接的查询语句mysqldumpslow -s t -t 10 -g &quot;left join&quot; /var/lib/mysql/centos7-slow.log另外建议在使用这些命令是结合|和more使用，否则可能会爆屏mysqldumpslow -s r -t 10 /var/lib/mysql/centos7-slow.log | more 批量插入数据，方便后边演示 一、首先我们创建两张表： 1.tb_dept_bigdata（部门表） 1234567# 部门表create table tb_dept_bigdata(id int unsigned primary key auto_increment,deptno mediumint unsigned not null default 0,dname varchar(20) not null default '',loc varchar(13) not null default '')engine=innodb default charset=utf8; 2.tb_emp_bigdata（员工表） 1234567891011create table tb_emp_bigdata(id int unsigned primary key auto_increment,empno mediumint unsigned not null default 0,/*编号*/empname varchar(20) not null default '',/*名字*/job varchar(9) not null default '',/*工作*/mgr mediumint unsigned not null default 0,/*上级编号*/hiredate date not null,/*入职时间*/sal decimal(7,2) not null,/*薪水*/comm decimal(7,2) not null,/*红利*/deptno mediumint unsigned not null default 0 /*部门编号*/)engine=innodb default charset=utf8; 3.打开log_bin_trust_function_creators参数 简单介绍一下，当二进制日志启用后，这个变量需要启用。它控制是否可以信任存储函数创建者，不会创建写入二进制日志引起不安全事件的存储函数。如果设置为0（默认值），用户不得创建或修改存储函数，除非它们具有除CREATE ROUTINE或ALTER ROUTINE特权之外的SUPER权限。 当开启二进制日志后，如果不开启这个参数，我们创建或修改函数会报错“ERROR 1418 (HY000): This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you *might* want to use the less safe log_bin_trust_function_creators variable)” 同样，这个参数对于触发器也适用。 12345# 查看是否关闭show variables like 'log_bin_trust_function_creators';# 开启参数set global log_bin_trust_function_creators=1; 二、其次我们创建两个函数:1.随机生成字符串的函数 12345678910111213delimiter $$drop function if exists rand_string;create function rand_string(n int) returns varchar(255)begindeclare chars_str varchar(52) default 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ';declare return_str varchar(255) default '';declare i int default 0;while i&lt;n doset return_str=concat(return_str,substring(chars_str,floor(1+rand()*52),1));set i=i+1;end while;return return_str;end $$ 2.创建随机生成编号的函数 123456789# 生成100-110的部门编号delimiter $$drop function if exists rand_num;create function rand_num() returns int(5)begindeclare i int default 0;set i=floor(100+rand()*10);return i;end $$ 三、然后我们创建两个存储过程用来往表中批量插入数据1.插入部门表的存储过程 123456789101112131415delimiter $$drop procedure if exists insert_dept;create procedure insert_dept(in start int(10),in max_num int(10))begindeclare i int default 0;set autocommit=0; -- 关闭自动提交repeatset i=i+1;-- 调用了上面定义的rand_string函数来随机产生部门名称和部门位置insert into tb_dept_bigdata (deptno,dname,loc) values((start+i),rand_string(10),rand_string(8)); until i=max_numend repeat;commit;end $$ 2.插入员工表的存储过程 123456789101112131415delimiter $$drop procedure if exists insert_emp;create procedure insert_emp(in start int(10),in max_num int(10))begindeclare i int default 0;set autocommit=0; -- 关闭自动提交repeatset i=i+1;-- 同样调用上面的rand_string和rand_num来产生员工名称和部门编号insert into tb_emp_bigdata (empno,empname,job,mgr,hiredate,sal,comm,deptno) values((start+i),rand_string(6),'developer',0001,curdate(),2000,400,rand_num());until i=max_numend repeat;commit;end $$ 四、调用存储过程批量插入数据 调用存储过程往部门表中插入10个部门 12delimiter ;call insert_dept(100,10); 调用存储过程往emp表中添加50万条记录 123delimiter ;-- 100002-600001call insert_dept(100001,500000); show profile我们上面提到，通过慢查询日志找到执行时间比较长的SQL语句之后，就会使用explain来分析SQL语句并进行优化。如果还是无法解决问题，就需要用到我们的show profile。 show profile是MySQL提供可以用来分析当前会话中语句执行的资源消耗情况。可以用于我们SQL的调优的测量。 MySQL中默认是关闭的，并且记录15条记录。可以通过下面的命令查看： 1234# 查看是否开启show variables like 'profiling';# 设置开启set profiling = on; 本来一开始是打算使用这两条sql语句进行测试，但是却出现了问题 12select *from tb_emp_bigdata group by id%10 limit 150000;select *from tb_emp_bigdata group by id%20 order by 5; 这里的意思是说MySQL处于only_full_group_by SQL模式(5.7.5之后的默认情况，我用的是5.7.26),在这种模式下需要select列中都要在group by中,或者本身是聚合列(SUM,AVG,MAX,MIN) 才行，所以去掉就好。 解决方法：首先查看sql_mode模式，可以看到是包含only_full_group_by模式的。 我们直接去掉它即可 注意上面只是对本次会话有效，如果需要一直有效，则需要在my.cnf的[mysqld]下进行配置 1sql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION 配置完成后重启MySQL即可。 错误解决后，我们回来继续看show profile。开启profiling后，我们随便写几条sql语句 123456789select * from staff;select name,age,pos from staff where name = 'Alice';select * from staff group by age;-- 当然还有我们要测试的SQL语句-- 这两个语句本身并没有意义，只是为了增加查询时间select *from tb_emp_bigdata group by id%10 limit 150000;select *from tb_emp_bigdata group by id%20 order by 5;select * from staff group by age;select *from tb_emp_bigdata group by id%20 order by 5; 接着我们看下下面的sql语句的执行结果 1show profiles; 可以看到show profiles的结果中记录了我们开启profiling后的每一条sql语句，默认是15条。(我这里是为了解决上面说的sql_mode问题，如果一开始就设置好了，那么应该是从序号1开始的) 我们看到这个表中有三列:Query_ID：是我们开启profiling后执行的每条SQL语句idDuration：是我们每条SQL语句的执行时间Query：不用说就是我们的SQL语句 通过上面的表中得到Query_ID后，才是show profile真正发挥作用的时候了： 123-- 这里只是写了比较常用的cpu,block io进行显示-- 其他参数参考下表show profile cpu,block io for query 上面表中的Query_ID 这里我们先看下简单的10号SQL的信息： 可以看到左边Status列就是我们这条SQL语句的完整的生命周期。Duration列是SQL每个步骤耗费时间。 CPU 和Block列则是CPU和IO耗费情况。 我们在来看下上面特意准备的执行时间比较长的SQL语句： 从上面我们可以看到具体执行时间比较长的有创建临时表creating tmp table和我们的sending data。这样我们就可以根据这两点来对我们的SQL语句进行优化。 如果在show profile诊断结果中出现了下面结果中的任何一条，则sql语句需要优化。①converting HEAP to MyISAM：查询结果太大，内存不够，数据往磁盘上搬了。②Creating tmp table：创建临时表。先拷贝数据到临时表，用完后再删除临时表。③Copying to tmp table on disk：把内存中临时表复制到磁盘上，危险！！！④locked。⑤sending data时间过长 比如常见的sending data时间过长的原因可能是因为没有索引或者没有正确使用索引。 总结如果发现是SQL语句执行的慢，导致系统变慢的话，我们需要做的有：1. 首先开启慢查询并捕获执行的慢的SQL语句2. 找出SQL语句后通过explain进行分析(一般在这里就能解决)3. 如果explain解决不了，则通过show profile进行更细致的分析，从而对SQL进行优化。(到这里基本95%的问题能解决)4. 最后还是解决不了，则需要与DBA沟通进行SQL数据库服务器的参数调优 到这里，我们的SQL调优的基本知识算是了解了。但要知道实际生产上的调优远比我们上面写的demo要复杂的多，所以还是多多积累吧。","link":"/2020/04/22/%E6%9F%A5%E8%AF%A2%E6%88%AA%E5%8F%96%E5%88%86%E6%9E%90/"},{"title":"查询性能优化","text":"主要内容 MySQL的查询过程 常见优化策略 查询过程 客户端发送一条查询给服务器 服务器先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划。 MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。 将结果返回给客户端。 常见优化策略","link":"/2020/04/22/%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"title":"数据库设计","text":"函数依赖异常三大范式第一范式(1NF)第二范式(2NF)第三范式(3NF)ER图数据库设计流程按照规范设计，将数据库的设计过程分为六个阶段： 系统需求分析阶段 概念结构设计阶段 逻辑结构设计阶段 物理结构设计阶段 数据库实施阶段 数据库运行与维护阶段 需求分析和概念结构设计独立于任何数据库管理系统。","link":"/2020/04/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/"},{"title":"二叉排序树、平衡二叉树、红黑树、B树、B+树","text":"主要内容 二叉排序树(BST树) 平衡二叉树(AVL树) 红黑树(R-B Tree) B/B+树 二叉排序树(BST树)二叉排序树（Binary Sort Tree）又称二叉查找树、二叉搜索树。它或者是一棵空树；或者是具有下列性质的二叉树： 1. 若左子树不空，则左子树上所有结点的值均小于它的根结点的值；2. 若右子树不空，则右子树上所有结点的值均大于它的根结点的值；3. 左、右子树也分别为二叉排序树 我们以数组{3,2,1,5,4,6}构建一个二叉排序树如下: 可以看到BST树因为有排序，所以检索的时间复杂度为O(logn)。但是BST树还有一个问题:如果我们的数组是{2,1,3,4,5…},或者甚至如果我们的数组为{1,2,3,4,5..}或者{5,4,3,2,1…},那么我们的数组就会退化成一个单链表。查询的时间复杂度从O(logn)退化为O(n) 所以对于BST树而言: 平均时间复杂度为O(logn),最坏时间复杂度为 O(n) 另外，二叉排序树的中序遍历是一个递增序列。(上面中序遍历为1,2,3,4,5,6) 我们知道,BST树主要是用来进行检索，但是由于它会退化成线性表，使得检索的时间复杂度为O(logn)，所以一般情况下很少使用。 平衡二叉树(AVL树)为了避免上述二叉排序树退化成链表，出现了平衡二叉排序树，又叫AVL树。那么是怎么定义的呢？ 首先平衡二叉树是二叉排序树。那么它或者为空树，或者满足上面BST中提到的三点要求。 其次和普通二叉排序树不同的是二叉平衡树要求左右的子树高度之差绝对值不超过1。 通常我们将二叉树上结点的左子树深度减去右子树深度的值称为平衡因子BF(Balance Factor),那么平衡二叉树的平衡因子只能是-1,0,1。上面的小的数字就是那个节点的BF。 左旋和右旋上面我们看到AVL树是自平衡的，那么AVL树是如何保证它的平衡性呢?实际上平衡二叉树在每次插入元素时都会做相应的旋转操作。旋转的四种情况如下： 插入点位于x的左孩子的左子树中。 左左LL 需要进行右旋。 插入点位于x的右孩子的右子树中。 右右RR 需要进行左旋。 插入点位于x的左孩子的右子树中。 左右LR 需要较低的先左旋，转换为LL问题，再右旋。 插入点位于x的右孩子的左子树中。 右左RL 需要较低的先右旋，转化为RR问题。再左旋。 当插入一个节点破坏了树的平衡状态时，我们需要做的有两点: 找出最小不平衡子树(距离插入节点最近，且BF不在[-1,1]范围内的节点为根的子树)。 通过旋转操作维持二叉树的平衡状态(LL、RR、LR、RL) 我们以数组a[9]={3,2,1,4,5,6,7,10,9}构建二叉排序树的过程来详细介绍上面的四种调整关系: 首先3,2先正常的构建，但是在插入1时，应该插入到2的左子树，这时3的BF因子为2，不满足平衡条件，最小平衡子树为3,2,1。由于BF=2 &gt; 0 所以是LL型调整，所以进行右旋。 接着插入4，平衡因子没有被破坏。接着插入5,这时3的BF因子为-2,2的BF因子也是-2，但是3离插入节点4最近，所以最小非平衡子树为3,4,5,由于由于BF=-2 &lt; 0 所以是RR型调整，所以进行左旋。 接着插入6,7 相信看懂了上面的过程，插入6,7的过程就非常简单了。 接着我们插入10，平衡状态未被破坏。但是插入9时，最小不平衡子树为7,10,9,但是如果直接进行左旋会出现9成为10的右子树，不符合排序二叉树的要求。这时候其实7的BF为-2,10的BF为1，符号不同（之前都是相同）所以需要先统一符号，首先9,10右旋，使得7和9的符号统一为负，再进行7,9,10的左旋。 对于LR型调整，在这个例子中不好描述，我们直接上一个新的例子： 插入节点5时，导致失衡。首先调整3,1,4,5使得BF因子和10同号转化为LL问题，之后再进行右旋。 对于AVL树而言: 因为AVL树中树是完全平衡的，所以平均检索复杂度和最坏检索复杂度都为O(logn) 虽然解决了最坏检索复杂度的问题，但是每次插入和删除时AVL树都会做调整，所以插入和删除效率相对较低。 红黑树(R-B Tree)和平衡二叉树一样，红黑树也是一种自平衡的二叉排序树。也就是说红黑树满足二叉排序树的三点要求，并且和AVL树一样，通过旋转操作进行自身的平衡。但是和AVL树不同的是， 红黑树不严格要求BF=-1,0,1,而是通过对任何一条从根到叶子的路径上各个节点着色的方式的限制,确保左右子树树高差不超过一倍因此它是一种弱平衡二叉树(由于是弱平衡,可以推出,相同的节点情况下,AVL树的高度低于红黑树), 红黑树在每个节点增加一个存储位表示节点的颜色,可以是红色或黑色。它必须满足下面性质： 节点非黑即红。 根节点是黑色。 所有叶子节点都是黑色，并且是NULL节点 每个红色节点的两个子节点都是黑色。（从每个叶子到根的所有路径上不能有两个连续的红色节点） 任意一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点。 红黑树为了维持自身的平衡性主要有两大操作: 1. 重新染色 (重新标记黑色或红色)2. 旋转 (就是我们上面提到的左旋和右旋) 注意这里是有先后顺序的，我们会先尝试重新染色，如果重新染色不能达到红黑树的5点要求，然后我们尝试旋转操作。 这里我们看下重新染色和旋转的规则: 将新插入的节点标记为红色 如果 X 是根结点(root)，则标记为黑色 各种情况的讨论 情况的讨论参考文章:https://zhuanlan.zhihu.com/p/79980618?utm_source=cn.wiz.notehttps://www.cnblogs.com/LiaHon/p/11203229.htmlhttps://www.jianshu.com/p/e136ec79235c 我们以插入 10，20，30，15 到一个空树中，简单介绍一下: AVL树和红黑树对比: 理论上来说相对于要求严格平衡的AVL树来说,红黑树因为不是严格平衡，所以查找的时间复杂度虽然为O(logn),但可能在某些时候不如AVL树。但是也正是由于其弱平衡，并且通过染色的方式对自己进行平衡调整，所以相对来说插入和删除效率比AVL树高。 所以对于只进行查询的操作,肯定是AVL树好，但是除了查询操作外，还要进行插入和删除操作，综合考虑还是选择红黑树。这也是为什么hashmap中使用红黑树而不是AVL树的原因。 多路查找树上面我们提到的树主要用来在内存内进行排序，考虑的是内存那种运算的时间复杂度。但是如果数据达到内存中放不下需要存到硬盘上，或者比如我们的MySQL数据库，数据本身就存储在硬盘上。那么我们更多的需要考虑的就是如何减少对磁盘的IO访问。 之前我们学习的树中一个节点只能存储一个元素，在元素非常多的时候，要么树的度非常大，要么树的高度非常大，甚至两个必须都必须足够大才行。(AVL树和红黑树度为2，树高非常高) 这就使得需要进行IO访问的次数增多，因此需要打破每一个节点只存储一个元素的限制，因此引入了多路查找树。 多路查找树，其每一个节点的孩子数可以多于两个，且每一个节点处可以存储多个元素。由于它是查找树，所有元素之间存在某种特定的排序关系 这里，每一个节点可以存储多少个元素，以及它的孩子数的多少是非常关键的。其中比较特殊的就是2-3树、2-3-4树、B树、B+树。 2-3树2-3树是这样一棵多路查找树: 每一个节点都是2节点(具有两个孩子)或3节点(三个孩子)。 一个2节点包含一个元素和两个孩子(或没有孩子) 一个3节点包含一小一大两个元素和三个孩子(或没有孩子)。 其中左子树包含小于较小元素的元素 右子树包含大于较大元素的元素 中间子树包含结余两元素之间的元素 2-3树中所有叶子节点位于同一层。 2-3-4树有了2-3树的理解，对于2-3-4树就很好理解了，它其实是2-3树的扩展，包括了4节点的使用。 一个4节点包含小中大三个元素和四个孩子(或没有孩子。如果某个4节点有孩子的话:-左子树包含小于最小元素的元素-第二子树包含大于最小元素，小于第二元素的元素-第三子树包含大于第二元素，小于最大元素的元素-右子树包含大于最大元素的元素 B树B树是一种平衡的多路查找树，2-3树和2-3-4树都是B树的特例。节点最大的孩子数目称为B树的阶，因此2-3树是3阶B树，2-3-4树是4阶B树。 一个m阶B树具有如下性质: 根节点的儿子数量范围[2,m] 每个中间节点包含 k-1 个关键字和 k 个孩子，孩子的数量 = 关键字的数量 +1，k 的取值范围为 [ceil(m/2), m]。 叶子节点位于同一层，且叶子节点包括 k-1 个关键字（叶子节点没有孩子），k 的取值范围为 [ceil(m/2), m]。 假设中间节点节点的关键字为：Key[1], Key[2], …, Key[k-1]，且关键字按照升序排序，即 Key[i]&lt;Key[i+1]。此时 k-1 个关键字相当于划分了 k 个范围，也就是对应着 k个指针，即为：P[1], P[2], …, P[k]，其中 P[1] 指向关键字小于 Key[1] 的子树，P[i] 指向关键字属于 (Key[i-1], Key[i]) 的子树，P[k] 指向关键字大于 Key[k-1] 的子树。 图片来自文章 https://mp.weixin.qq.com/s/k4-RaW4ROlo6chSXsO_4AA 举个例子上图为三阶图，查看磁盘3，关键字为20，30.三个孩子分别是(18,19),(22,25),(32,36).其中(18,19)小于20，(22,25)在(20,30)之间，(32，36)大于30. 如图所示，如果要查找数据项22，那么B树的查找过程如下: 首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确22大于18小于20，得到指向磁盘块3的P2指针。 通过磁盘块3的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，22在20和30之间，得到指向磁盘块9的P2指针。 通过磁块9的P2指针加载磁盘块9到内存，发生第三次IO，同时内存中做二分查找找到22，结束查询，总计三次IO。 上面还是一个简单的例子，假设我们每个磁盘块可以存储1000个数据，那么一个三层的B树能存储的数据量是1000*1000=100w条数据。进行查找时也只需要进行3次IO。 如果是红黑树或者AVL树的话，树高则会非常大，IO次数也无法想象。 B+树从上面可以看到B树优点非常多，但是B树还有一个问题，就是如果我们要对数据进行中序遍历，那么需要在硬盘的页面间进行多次访问，发生多次IO影响效率。比如上面的例子中需要先定位到磁盘块5，得到7 8 发生3次IO，接着读磁盘块2，得到9，又发生一次IO，接着读磁盘块6…. 最终IO次数会非常大，严重影响效率。 因此应文件系统的需要而提出了一种B+树结构。严格意义上来讲B+树已经不是一种树。 一棵m阶B+树和一个m阶B树的差别: 有n棵子树的节点中包含有n个关键字 所有叶子节点包含全部关键字信息，及指向含这些关键字记录的指针，叶子节点本身依关键字的大小自小二大顺序连接 所有非叶子节点中不包含真实数据仅含有其子树中的最大最小关键字。 图片来自文章 https://mp.weixin.qq.com/s/k4-RaW4ROlo6chSXsO_4AA 这样对于中序遍历来说我们需要做的是首先发生3次IO找到最小值10，因为关键字全部在叶子节点，所以就不用访问上层的非叶子节点。找到10之后，直接通过指针直接顺序访问就能得到中序遍历的结果。 上面提到的中序遍历最终还是用来进行排序和范围查找。比如我们要找18到32之间的数据，首先发生3次IO定位到18所在的磁盘块8，然后顺序读取到32即可。只发生3+2=5次IO，这也是为什么MySQL数据库中使用B+树结构来做索引的原因。","link":"/2020/04/29/%E4%BA%8C%E5%8F%89%E6%8E%92%E5%BA%8F%E6%A0%91%E3%80%81%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91%E3%80%81%E7%BA%A2%E9%BB%91%E6%A0%91%E3%80%81B%E6%A0%91%E3%80%81B-%E6%A0%91/"},{"title":"十大排序算法总结","text":"主要内容 排序的基本概念 10种排序算法代码演示 排序算法总结 Java中的排序算法 排序的基本概念排序的一般定义排序是计算机中经常进行的操作，目的在于将一组无序的数据元素调整为有序的数据元素。序列：1，20，45，5，2，12排序后：1，2，5，12，20，45 排序的稳定性如果序列中的两个元素R[i]、R[j]，关键字分别为K[i]、K[j]并且K[i]=K[j]，在排序之前R[i]排在R[j]前面，如果排序操作后，元素R[i]仍然排在R[j]前面，则排序方法是稳定的；否则排序是不稳定的。 通俗的讲，就是排序前有两个相等的数a和b，且a在b前面，排序后如果a依然在b前面，则这次排序为稳定排序 。 例如对4,2,1,3(A),3(B),5进行从小到大排序，则 稳定排序： 1,2,3(A),3(B),4,5 3(A),3(B)和排序前顺序相同。 不稳定排序: 1,2,3(B),3(A),4,5 3(A),3(B)和排序前顺序不同。 这里我们先提一下我们下面要讲的排序算法的稳定性如下: 稳定排序： 冒泡排序、插入排序、归并排序、 基数排序 不稳定排序：选择排序、快速排序、希尔排序、 堆排序 内排序和外排序内排序 ：待排序列完全存放在内存中所进行的排序过程，适合不太大的元素序列外排序：指的是大文件的排序，即待排序的记录存储在外存储器上，无法一次装入内存，需要在内存和外部存储器之间进行多次数据交换，以达到排序整个文件的目的。 排序算法的性能评价时间性能:主要性能差异体现在比较和交换的数量辅助存储空间:为完成排序操作需要的额外的存储空间。必要时可以空间换时间算法的实现复杂性： 过于复杂的排序算法影响可读性和可维护性 排序的模板方法如下；排序模板方法如下: 12345678910111213141516public class SortMethod { //比较 private boolean less(int i,int j){ return i&lt;j; } //交换 private void swap(int[] num,int i,int j){ int temp = num[i]; num[i] = num[j]; num[j] = temp; } //打印 private void show(int[] num){ System.out.println(Arrays.toString(num)); }} 选择排序算法过程:从数组中选择最小元素，将它与数组的第一个元素交换位置。再从数组剩下的元素中选择出最小的元素，将它与数组的第二个元素交换位置。不断进行这样的操作，直到将整个数组排序。 1234567891011public void selectionSort(int[] num){ int min = 0; int n = num.length; for (int i = 0; i &lt; n; i++) { min = i; for (int j = i + 1; j &lt; n; j++) { if(less(num[j],num[min])) min = j; } swap(num,i,min); }} 选择排序算法分析:时间复杂度: 因为无论怎样，选择排序都会进行1+2+……+（n-1）次比较操作，时间复杂度为O(n^2)空间复杂度： O(1) 原地排序稳定性: 不稳定排序。 比如5(A),8,5(B),2,9 最终排完序后为 2,5(B),5(A),8,9 冒泡排序算法过程:从左到右不断交换相邻逆序的元素，在一轮的循环之后，可以让未排序的最大元素上浮到右侧。 在一轮循环中，如果没有发生交换，那么说明数组已经是有序的,可以通过添加标志位来避免无用的比较操作。 123456789101112131415public void bubbleSort(int[] nums){ int n = nums.length; boolean flag; for (int i = 0; i &lt; n - 1; i++) { flag = false; for (int j = 0; j &lt; n-i-1; j++) { if(less(nums[j+1],nums[j])){ swap(nums,j,j+1); flag = true; } } if(flag) break; show(nums); }} 冒泡排序算法分析:时间复杂度:平均时间复杂度为O(n^2) 最好情况：数组正序，复杂度为O(n) 最坏情况：数组倒序，复杂度为O(n^2)空间复杂度：O(1) 原地排序稳定性:稳定排序 因为只有在后一位比自己大时才交换,相等时不进行交换。 插入排序算法思想:每次都将当前元素插入到左侧已经排序的数组中，使得插入之后左侧数组依然有序。 123456789public void insertionSort(int[] num){ int n = num.length; for (int i = 1; i &lt; n; i++) { for (int j = i; j &gt;0 &amp;&amp; less(num[j],num[j-1]); j--) { swap(num,j,j-1); } show(num); }} 对于数组 {3, 5, 2, 4, 1}，它具有以下逆序：(3, 2), (3, 1), (5, 2), (5, 4), (5, 1), (2, 1), (4, 1)，插入排序每次只能交换相邻元素，令逆序数量减少1，因此插入排序需要交换的次数为逆序数量。 插入排序的时间复杂度取决于数组的初始顺序，如果数组已经部分有序了，那么逆序较少，需要的交换次数也就较少，时间复杂度较低。 平均情况下插入排序需要 ~N2/4 比较以及 ~N2/4 次交换； 最坏的情况下需要 ~N2/2 比较以及 ~N2/2 次交换，最坏的情况是数组是倒序的； 最好的情况下需要 N-1 次比较和 0 次交换，最好的情况就是数组已经有序了。 插入排序算法分析:时间复杂度:取决于数组的初始顺序 通常认为平均时间复杂度为O(n^2)空间复杂度：O(1) 原地排序稳定性:稳定排序 因为只有在后一位比自己大时才插入,相等时直接插在后面所以是稳定排序。 希尔排序对于大规模的数组，插入排序很慢，因为它只能交换相邻的元素，每次只能将逆序数量减少1。希尔排序的出现就是为了解决插入排序的这种局限性，它通过交换不相邻的元素，每次可以将逆序数量减少大于 1。 算法思想:希尔排序也是一种插入排序，也称为缩小增量排序。主要思想是使用插入排序对增量h的序列进行排序。通过不断减小h，最后令 h=1，就可以使得整个数组是有序的。 可以参考上图，在此我们选择增量gap=length/2，缩小增量继续以gap = gap/2的方式，这种增量选择我们可以用一个序列来表示，{n/2,(n/2)/2...1}，称为增量序列。希尔排序的增量序列的选择与证明是个数学难题。通常比较常见的有两种: {n/2,(n/2)/2…1} 1/2(3^k-1) {1,4,…3h+1} (通常比上面的效率高) 实现时不用循环按组处理，我们可以从第h个元素开始，逐个跨组处理。 123456789101112131415161718public void shellSort(int[] num){ int n = num.length; //这里直接使用第二个增量序列 //找到初始增量 int h = 1; while(h&lt;n/3) h = 3*h+1; //1,4,13,40... //排序过程 while(h&gt;=1){ //就是一个插入排序 for (int i = h; i &lt; n; i++) { for (int j = i; j &gt;=h &amp;&amp; less(num[j],num[j-h]); j-=h) { swap(num,j,j-h); } } //改变增量 h = h/3; }} 时间复杂度：O(n1.3)~O(n2)之间空间复杂度：O(1) 原地排序稳定性:不稳定排序 相同元素可能分到不同的组中，每一组又单独进行插入排序，所以有可能其稳定性就会被打乱。 其实对于希尔排序的时间复杂度一直没有一个统一的说法，主要在于希尔排序取决于增量序列的选择。只需要知道 希尔排序超越了我们之前所讲的三种简单算法的O(n^2) 即可。 归并排序算法思想:将一个数组(递归地)分成两部分分别进行排序，最后将结果归并起来。使用了分治法的思想。 图片来自于文章 https://www.cnblogs.com/chengxiao/p/6194356.html 自顶向下归并排序(递归) 从上往下，将大数组分为小数组 12345678910111213141516171819202122232425262728293031private int[] workspace;public void mergeSort(int[] num){ workspace = new int[num.length]; sort(num,0,num.length-1);}private void sort(int[] num,int lo,int hi){ if(lo &gt;= hi) return; int mid = lo + (hi-lo)/2; sort(num,lo,mid); //左半边排序 sort(num,mid+1,hi); //右半边排序 merge(num,lo,mid,hi); //归并结果 show(num);}private void merge(int[] num, int lo, int mid, int hi) { int i = lo; int j = mid+1; for (int k = lo; k &lt;= hi; k++) { workspace[k] = num[k]; //先复制到临时数组中 } for (int k = lo; k &lt;=hi ; k++) { if(i &gt; mid) num[k] = workspace[j++]; //左边数组完成，右边没有完成 else if(j &gt; hi) num[k] = workspace[i++]; //右边数组完成，左边没有完成 else if(less(workspace[i],workspace[j])) num[k] = workspace[i++]; //左边小于右边，num数组中放左边的值 else num[k] = workspace[j++]; //右边小于左边，num数组中放右边的值 }} 贴一下运行结果 12345678初始数组:[6, 2, 7, 4, 8, 1, 5, 3][2, 6, 7, 4, 8, 1, 5, 3] //6,2排序为2,6[2, 6, 4, 7, 8, 1, 5, 3] //7,4排序为4,7[2, 4, 6, 7, 8, 1, 5, 3] //2,6,4,7排序为2, 4, 6, 7[2, 4, 6, 7, 1, 8, 5, 3] //8,1排序为1,8[2, 4, 6, 7, 1, 8, 3, 5] //5,3排序为3,5[2, 4, 6, 7, 1, 3, 5, 8] //1, 8, 3, 5排序为1, 3, 5, 8[1, 2, 3, 4, 5, 6, 7, 8] //最后整体排序 自底向上递归排序(迭代)先归并小数组，然后成对归并得到的小数组。 12345678910//merge没变，只需要更改sort即可private void sort2(int num[]){ int n = num.length; for (int sz = 1; sz&lt;n; sz+=sz) { //sz子数组的大小 for (int lo = 0; lo &lt; n - sz; lo += sz + sz) { //lo子数组索引 merge(num, lo, lo + sz - 1, Math.min(lo + sz + sz - 1, n- 1)); } show(num); }} 归并排序复杂度分析:时间复杂度：始终是O(nlogn) 与数据的顺序无关空间复杂度：O(n)，借助了临时数组workspace稳定性:稳定排序 上面我们首先判断左边是否小于右边，所以是稳定个排序 快速排序算法思想 归并排序将数组分为两个子数组分别排序，并将有序的子数组归并使得整个数组排序； 快速排序通过一个切分元素将数组分为两个子数组，左子数组小于等于切分元素，右子数组大于等于切分元素，将这两个子数组排序也就将整个数组排序了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public void quickSort(Integer[] num){ shuffle(num); //消除数组对数据的依赖性 qSort(num,0,num.length-1); } private void qSort(Integer[] num, int lo, int hi) { if(hi &lt;= low) return; int j = partion(num,lo,hi); qSort(num,lo,j-1); qSort(num,j+1,hi); show(num); } /** * 切分方法 */ private int partion(Integer[] num,int lo,int hi){ //将数组切分为num[lo..pivotkey-1],num[pivotkey],num[pivotkey+1..hi] int i = lo; //从左往右扫描指针 int j = hi + 1; //从右往左扫描指针 int pivotkey = num[lo]; while (true){ //从左往右扫描直到找到一个大于等于pivotkey的元素 while(less(num[++i],pivotkey)) if(i==hi) break; //从右往左扫描直到找到一个小于等于pivotkey的元素 while(less(pivotkey,num[--j])) if(j==lo) break; //当两个指针相遇时，跳出循环进行之后的交换操作 if(i &gt;= j) break; //交换位置 swap(num,i,j); } //进行num[lo]和左子数组最右侧元素num[j]交换，返回j即可 swap(num,lo,j); return j; } /** * 打乱数组 */ private void shuffle(Integer[] num){ List&lt;Integer&gt; list = Arrays.asList(num); Collections.shuffle(list); list.toArray(num); } 快速排序的复杂度分析时间复杂度：平均是O(nlogn) 最好情况下，递归树的深度为log2n + 1次，时间复杂度为O(nlog2n)。最坏情况下，序列为正序或逆序,递归树画出来就是一个斜树，因此要执行n-1次递归调用，总的时间复杂度为O(n^2) 空间复杂度：从上面的分析可以看到，最好情况递归树的深度为log2n，最坏情况为O(n)。所以平均情况下快速排序的空间复杂度为O(logn) 稳定性： 不稳定排序 因为关键字的比较和交换是跳跃进行的。 快速排序的改进1.切换到插入排序因为快速排序在小数组中也会递归调用自己，对于小数组，插入排序比快速排序的性能更好，因此在小数组中可以切换到插入排序。 12345//只需要在qSort中将if(hi &lt;= lo) return;替换为if(hi &lt;= lo + M){ insertionSort(num,lo,hi); return;} 2.三数取中法 一般找到数组的中位数最好，但是寻找数组中位数代价较高。一般取3个关键字先进行排序，将排在中间的数作为基准。一般是取左端，右端和中间三个数。当然数据量大的时候也可以九数取中。 1234567891011121314//原来在partion中直接取数组最左边第一个数做pivotkey int pivotkey = num[lo];//这里可以直接调用函数得到int pivotkey = getPivotkey(num);//找三个数的中间的数private int getPivotkey(Integer[] num,int lo,int hi){ int pivotkey; int mid = (lo+hi)/2; if(num[lo]&gt;num[hi]) swap(num,lo,hi); if(num[mid]&gt;num[hi]) swap(num,mid,hi); if(num[mid]&gt;num[lo]) swap(num,mid,lo); pivotkey = num[lo]; return pivotkey;} 3.三向切分法对于有大量重复元素的数组，可以将数组切分为三部分，分别对应小于、等于和大于切分元素。 对于有大量重复元素的随机数组，三向切分法可以将排序时间从线性对数级别降低到线性级别。 12345678910111213141516171819202122/** * 维护3个指针，lt, i , gt * num[lo..lt-1]都小于pivotkey num[gt+1..hi]都大于pivotkey * num[lt..i-1]都等于pivotkey num[i..gt]元素未确定 * * 最终num[lo..lt-1]&lt;pivotkey = num[lt..gt] &lt; num[gt+1..hi] * */public void quick3wayQsort(Integer[] num,int lo,int hi){ if(hi &lt;= lo) return; int lt = lo; int i = lo+1; int gt = hi; int pivotkey = num[lo]; while(i &lt;= gt){ if(less(num[i], pivotkey)) swap(num,lt++,i++); else if(less(pivotkey,num[i])) swap(num,i,gt--); else i++; } //现在num[lo..lt-1]&lt;pivotkey = num[lt..gt] &lt; num[gt+1..hi]成立 qSort(num,lo,lt-1); qSort(num,gt+1,hi);} 堆排序关于优先队列和堆的定义和基本性质参考我的文章 算法思想：把最大元素和当前堆中数组的最后一个元素交换位置，并且不删除它，那么就可以得到一个从尾到头的递减序列，从正向来看就是一个递增序列，这就是堆排序。 主要包含两步: 构建堆 交换堆顶元素与最后一个元素 无序数组建立堆最直接的方法是从左到右遍历数组进行上浮操作。一个更高效的方法是从右至左进行下沉操作，如果一个节点的两个子节点都已经是有序的，那么进行下沉操作可以使得这个节点为根节点的堆有序。叶子节点不需要进行下沉操作，可以忽略叶子节点的元素，因此只需要遍历一半的元素即可。 1234567891011121314151617181920212223242526272829303132333435363738public void heapSort(int[] num){ int N = num.length; //构造堆 //使用shiftDown方法将num[1]到num[N]排序 for(int k = N/2;k&gt;=1;k--){ shiftDown(num,k,N); } //将num[1]和num[N]交换并修复堆 //重复直到堆变空 while(N&gt;1){ swap(num,1,N--); shiftDown(num,1,N); }}private void shiftDown(int[] num, int k, int N) { while (2 * k &lt;= N) { int j = 2 * k; //和两个孩子节点中值大的交换 if (j&lt;N &amp;&amp; less(num,j, j + 1)) j++; if (!less(num,k, j)) break; swap(num,k, j); k = j; }}//数组num[0]放元素，如果要放元素，并且对数组进行排序，则需要将less和swap中的索引减1private boolean less(int[] num,int i, int j){ return num[i-1]&lt;num[j-1];}private void swap(int[] num,int i,int j){ int temp = num[i-1]; num[i-1] = num[j-1]; num[j-1] = temp;} 堆排序的复杂度分析时间复杂度：平均是O(nlogn) 构建初始堆经复杂度为O(n)，在交换并重建堆的过程中，需交换n-1次，而重建堆的过程中，根据完全二叉树的性质，[log2(n-1),log2(n-2)…1]逐步递减，近似为nlogn。空间复杂度：O(1) 就地排序稳定性： 不稳定排序 记录的比较与交换是跳跃式的 下面的三种方法都是线性时间非比较类排序： 计数排序算法思想：核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 1234567891011121314151617181920212223public void countSort(int[] num) { //1.先找出最大和最小元素 int min = num[0]; int max = num[0]; for (int i = 0; i &lt; num.length; i++) { if(num[i]&lt;min) min = num[i]; if(num[i]&gt;max) max = num[i]; } int[] bucket = new int[max-min+1]; Arrays.fill(bucket,0); //元素对应放入bucket中并计数 for (int i = 0; i &lt; num.length; i++) { bucket[num[i]-min]++; } int index = 0; for (int i = 0; i &lt; bucket.length; i++) { while(bucket[i] != 0){ num[index++] = i+min; bucket[i]--; } } System.out.println(\"排序后\"+Arrays.toString(num));} 计数排序复杂度分析时间复杂度：始终是O(n+k) 整个过程是对待排数组进行遍历。空间复杂度：O(num[max]-num[min]) 从上面可以看到，计数排序需要一个num[max]-num[min]长度的数组做辅助来排序稳定性:稳定排序 由于计数排序都没有出现比较元素的操作。这个算法很明显是稳定的所以计数排序是稳定排序 桶排序算法思想 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 12345678910111213141516171819202122232425262728public void bucketSort(int[] num,int bucketSize) { int min = num[0]; int max = num[0]; for (int i = 0; i &lt; num.length; i++) { if(num[i]&lt;min) min = num[i]; if(num[i]&gt;max) max = num[i]; } int bucketCount = (max-min)/bucketSize+1; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucketnum= new ArrayList&lt;&gt;(bucketCount); int[] newnum = new int[num.length]; for (int i = 0; i &lt; bucketCount; i++) { bucketnum.add(new ArrayList&lt;&gt;()); } // 把数据放入桶中 for (int i = 0; i &lt;num.length ; i++) { bucketnum.get((num[i]-min)/bucketSize).add(num[i]); } int index = 0; for (int i = 0; i &lt; bucketCount; i++) { //对于每个桶中的数据可以使用其他排序方式排序 //也可以使用桶排序进行递归调用，但是涉及到list和array之间的转换比较繁琐 //所以最好是待排序数组是list时比较方便 Collections.sort(bucketnum.get(i)); for (int j = 0; j &lt; bucketnum.get(i).size(); j++) { newnum[index++] = bucketnum.get(i).get(j); } }} 桶排序复杂度分析时间复杂度：平均是O(n+k)桶排序包括两个部分：1.循环计算每个关键字的桶映射函数，这个时间复杂度是O(n)。2.利用先进的比较排序算法对每个桶内的所有数据进行排序。很显然，第2部分是桶排序性能好坏的决定因素。 空间复杂度：O(n+k) 如果相对于同样的n，桶数量k越大，其效率越高，最好的时间复杂度达到O(n)。当然桶排序的空间复杂度为O(n+k)，如果输入数据非常庞大，而桶的数量也非常多，则空间代价无疑是昂贵的。稳定性:稳定排序 对于桶排序，在对每个桶内数据进行排序时，由于排序算法使用的不同，可能会出现不稳定排序，但是使用桶排序递归排序时是稳定排序，所以一般认为桶排序是稳定排序 基数排序算法思想：按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 1234567891011121314151617181920212223242526272829303132public static radixSort(int[] num) { //1.先找出最大位的位数 int maxNum = num[0]; for (int i = 1; i &lt; num.length; i++) { if(num[i]&gt;maxNum) maxNum = num[i]; } int maxDigit = 0; while(maxNum != 0){ maxNum /= 10; maxDigit++; } int mod = 10; int div = 1; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucketList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) { bucketList.add(new ArrayList&lt;Integer&gt;()); } for (int i = 0; i &lt; maxDigit; i++,mod *=10,div *=10) { for (int j = 0; j &lt; num.length; j++) { int num = (num[j] % mod)/div; bucketList.get(num).add(num[j]); } int index = 0; for (int j = 0; j &lt; bucketList.size(); j++) { for (int k = 0; k &lt; bucketList.get(j).size(); k++) { num[index++] = bucketList.get(j).get(k); } bucketList.get(j).clear(); } }} 基数排序复杂度分析时间复杂度：O(nk) 该算法所花的时间基本是在把元素分配到桶里和把元素从桶里串起来；把元素分配到桶里：循环 n 次；把元素从桶里串起来也是n次空间复杂度：O(n+k) 该算法的空间复杂度就是在分配元素时，使用的桶空间稳定性:稳定排序 排序算法总结 从上面的图中我们可以把非线性时间比较排序分为两类： 简单算法：直接插入、直接选择、冒泡 改进算法：希尔排序、堆排序、快速排序、 归并排序 从平均情况看，希尔排序是突破了O(n^2)。但是后三种改进算法要胜过希尔排序，并远胜过前三种简单算法。 从最好情况看，反而冒泡排序和直接插入排序效果最好，也就是说，如果待排序列总是基本有序，反而可以直接使用冒泡或者直接插入排序。 从最坏情况看，堆排序与归并排序又好于快速排序和其他简单排序，所以对于待排序列基本倒叙则可以考虑堆排序和归并排序。 从空间复杂度上来所，归并排序和快速排序都有相应的空间要求，反而堆排序确是O(1)，所以对于只有少量内存进行排序时可以考虑使用堆排序。并且对于海量数据的排序可以通过建立小顶堆或者大顶堆的方式进行排序。 另外对于线性时间非比较类的3种排序方法：计数排序更适用于在已知序列中的元素0-k之间，且要求排序的复杂度在线性效率上的情况。桶排序可应用于数据量分布比较均匀，或比较侧重区间数量时使用。基数排序最适用于基数很大但关键字较小的序列 Java中的排序算法Java中主要排序方法为 java.util.Arrays.sort()，对于原始数据类型使用三向切分的快速排序，对于引用类型使用归并排序。","link":"/2020/05/03/%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"title":"树的基本概念和前中后序遍历","text":"主要内容 树的基本概念 二叉树概念和常见性质 二叉树的前中后序遍历 树的基本概念树(Tree)是n(n&gt;=0)个结点的有限集。n=0时称为空树。在任意一颗非空树中： 有且仅有一个特等的称为根(Root)的结点; 当n&gt;1时，其余结点可分为m(m&gt;0)个互不相交的有限集T1、T2、……Tm,其中每一个集合本身又是一棵树，并且称为根的子树(SubTree) 结点分类 树的结点包含一个数据元素及若干指向其子树的分支。结点拥有的子树数称为结点的度(Degree)。度为0的结点称为叶结点(Leaf)或终端结点;度不为0的结点称为非终端结点或分支结点。除根结点外，分支结点也称为内部结点。树的度是树内各结点的度的最大值。 以下图为例，A为根结点，B、C、D、E为分支结点，也叫内部结点。F、G、H、I、J为叶结点。由于D的度最大为3，所以数的度为3。 结点间的关系 结点的子树的根称为该结点的子结点，相应的，该结点称为子节点的父结点。同一个父结点的子结点之间互称为兄弟结点。结点的祖先是从根结点到该结点所经分支上的所有结点。反之，以某结点为根的子树中的任一结点都称为该结点的子孙。 还是以上图为例，C结点的子结点为E、F，父结点为A，兄弟结点为B。H结点祖先为A、B、D。C的子孙为E、F、J 树的其他概念 结点的层次(Level)从根开始定义起，根为第一层，跟的子结点为第二层，依次类推。父结点在同一层的结点互为堂兄弟结点。树中结点的最大层次称为树的深度(Depth)或高度。 二叉树二叉树(Binary Tree)是n(n&gt;=0)个结点的有限集合，该集合或者为空集(称为空二叉树),或者是由一个根结点和两颗互不相交的、分别称为根结点的左子树和右子树的二叉树组成。 注意：1.二叉树的每个结点最多有两颗子树，所以二叉树中不存在度大于2的结点2.左子树和右子树是有顺序的，次序不能任意颠倒3.即使树中某结点只有一颗子树，也要区分它是左子树还是右子树 二叉树的常见性质： 特殊二叉树：斜树所有的结点都只有左子树的二叉树叫左斜树。所有结点都只有右子树的二叉树叫右斜树 满二叉树 在一颗二叉树中，如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树 满二叉树的特点： 1.叶子结点只能出现在最下一层，出现在其它层就不可能达到平衡。2.非叶子结点的度一定是23.在同样深度的二叉树中，满二叉树的结点个数最多，叶子数最多 完全二叉树对一颗具有n个结点的二叉树按层序编号，如果编号为i(1&lt;=i&lt;=n)的结点与同样深度的满二叉树中编号为i的结点在二叉树中位置完全相同，则这棵二叉树称为完全二叉树 完全二叉树的特点： 1.叶子结点只能出现在最后两层2.最下层的叶子一定集中在左步连续位置3.倒数第二层若有叶子，一定集中在右部连续位置4.如果结点度为1，那么该结点一定只有左孩子，不存在只有右子树的情况5.同样结点数的二叉树，完全二叉树的深度最小 二叉树的存储结构顺序存储: 链式存储: 由于顺序结构的使用性不强，所以考虑链式结构。二叉树的链式存储结构是通过二叉链表来实现的。结点结构图如下图所示。其中data为数据域，lchild和rchild为指针域，分别指向左孩子结点和右孩子结点。 二叉树的前中后序遍历 前中后序遍历的区别在于根节点的位置。前序遍历为根左右，中序遍历为左跟右，后序遍历为左右根。 二叉树的前中后序遍历实际上用处不是很大，只是让我们了解二叉树可以以不同的方式进行遍历。唯一有用的一点是如果这个二叉树是二叉搜索树(BST)的话，中序遍历出来为递增序列","link":"/2020/04/29/%E6%A0%91%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%89%8D%E4%B8%AD%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86/"},{"title":"数据结构的时间复杂度总结","text":"基本数据结构的复杂度总结表格 之前我们学习了基本的数据结构，但是我很少提到时间复杂度，是因为每个都写一下，实在是太麻烦了，而且有时候容易忘。所以这里给一个表格总结。 需要的时候过来查一查就好了。 注意: Access是访问，Search是查找。不要弄混了哦","link":"/2020/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%80%BB%E7%BB%93/"},{"title":"leetcode总结","text":"对leetcode题目进行了分类，完成的会加上链接。 数据结构 数组和链表 栈堆队列 哈希表 树结构 图结构 字符串 位运算 算法思想 二分查找 排序 搜索 双指针 递归和分治 贪心 动态规划 数学","link":"/2020/05/10/leetcode%E6%80%BB%E7%BB%93/"},{"title":"线性表之数组、链表、栈、队列、堆","text":"主要内容 线性表的基本概念 数组和链表 栈和队列 优先队列和堆 线性表的基本概念线性表：零个或多个数据元素的有限序列 这里需要注意的是有限序列，是说线性表是有顺序的。比如下面的线性表中，a1为a2的直接前驱元素，a2为a1的直接后继元素。并且除了开始元素(只有一个后继元素)和结束元素(只有一个前驱元素)外每个元素有且只有一个前驱元素和一个后继元素。 线性表是逻辑结构，根据其物理结构的不同分为顺序结构（数组）和链式存储结构（链表） 数组和链表数组线性表的顺序存储结构，指的是用一段地址连续的存储单元依次存储线性表的数据元素 Java中ArrayList和数组的主要区别在于ArrayList是可变长度，数组是不可变长度。 数组很简单，这里简单介绍下: 内存空间连续 通过下标可以直接获取元素(初始下标为0)，时间复杂度为O(1)。 插入和删除元素，则需要进行数组内元素的移动，因此插入和删除的时间复杂度为O(n)。 简单手工实现一下ArrayList 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class MyArrayList &lt;E&gt;{ private Object[] elementData; private int size; private final int DEFAULT_SIZE = 10; public MyArrayList(int initsize){ if(initsize&gt;=0){ elementData = new Object[initsize]; }else{ throw new IllegalArgumentException(\"初始长度不正确\"+ initsize); } } public MyArrayList(){ elementData = new Object[DEFAULT_SIZE]; } //添加元素 public boolean add(E element){ ensureCapacity(); //扩容机制 elementData[size] = element; size++; return true; } //指定位置添加元素 public boolean add(int index,E element){ rangeCheck(index); ensureCapacity(); //需要进行数组的拷贝工作，效率较低 System.arraycopy(elementData,index,elementData,index+1,size-index); elementData[index] = element; size++; return true; } //通过下标删除 public E remove(int index){ rangeCheck(index); E oldValue = (E) elementData[index]; System.arraycopy(elementData,index+1,elementData,index,size-index); size--; return oldValue; } //通过元素删除 public boolean remove(Object o){ for (int i = 0; i &lt; elementData.length; i++) { if (o.equals(elementData[i])){ System.arraycopy(elementData,i+1,elementData,i,size-i); size--; return true; } } return false; } //查找 public E get(int index){ rangeCheck(index); return (E) elementData[index]; } //数组长度 public int size(){ return size; } //是否为空 public boolean isEmpty(){ return size==0?true:false; } //******************************************** //扩容过程 先创建新数组，然后复制原数组，最后让原数组指向新数组 private void ensureCapacity(){ if(elementData.length-1&gt;=size){ Object[] newArray = new Object[elementData.length+(elementData.length&gt;&gt;1)]; //右移一位相当于除以2 System.arraycopy(elementData,0,newArray,0,elementData.length); //数组拷贝 elementData = newArray; } } //索引检查 private void rangeCheck(int index){ if(index&lt;0||index&gt;=size){ throw new IndexOutOfBoundsException(\"索引不合法\"+index); } } } 单链表 由节点构成，每个节点包含数据元素和指向下一个节点的指针。 内存空间可以连续，也可以不连续。 查找时，需要从头结点开始遍历链表，时间复杂度O(n) 插入和删除时，时间复杂度O(1) 123456789101112131415161718192021222324252627282930313233343536public class MySingleLinkedList&lt;E&gt; { //结点内部类 private class Node&lt;E&gt; { public E element; //元素 private Node&lt;E&gt; next; //指向下一个节点的指针 public Node(){ } public Node(E element){ this.element = element; } } private Node&lt;E&gt; head; //头结点 private int size; //元素总数 public MySingleLinkedList(){ head = new Node&lt;E&gt;(); //默认初始化为null }``` 上面提到的是链表中的节点的插入和删除操作，涉及到链表头和链表尾的特殊情况，操作是相同的。&lt;div align=\"center\"&gt;&lt;img src=\"http://coderchen33.life/2020-04-29-基本数据结构之数组、链表、栈、队列-2020-05-02-16-52-09\"&gt;&lt;/div&gt;&lt;br&gt;添加元素```javapublic void add(E element){ Node&lt;E&gt; newNode = new Node&lt;E&gt;(element); Node&lt;E&gt; temp = head; while(temp.next!=null){ temp = temp.next; } newNode.next = temp.next; //注意顺序 temp.next = newNode; size++;} 删除元素 12345678910111213public E remove(){ if (size==0){ throw new IndexOutOfBoundsException(\"链表为空\"); } Node&lt;E&gt; temp = head; while(temp.next.next!=null){ temp = temp.next; } E oldElement = temp.next.element; temp.next = null; size--; return oldElement;} 双向链表单链表虽然有很多优点，但是也存在一些问题，比如上面的a,b,c,d,e链表，如果我们遍历到d时，想往回走去找c，那么单链表只能重新从头开始遍历到c。为了解决单链表的单向性，才有了双向链表。 双向链表和单向链表基本相同，唯一区别在于在单向链表的基础上，每个节点增加了一个指向前驱节点的指针。 123456789101112131415161718192021public class MyDoubleLinkedList&lt;E&gt; { private class Node&lt;E&gt;{ public E element; //元素值 private Node&lt;E&gt; prior; //直接前驱指针 private Node&lt;E&gt; next; //直接后继指针 public Node(){ } public Node(E element) { this.element = element; } } private Node&lt;E&gt; head; //头结点 private int size; //链表长度 默认初始化为0 public MyDoubleLinkedList(){ head = new Node&lt;E&gt;(); head.prior = head; //初始全为空 head.next = head; } 双向链表进行插入和删除时需要改变两个指针变量: 添加元素 123456789public void add(E element){ Node&lt;E&gt; newNode = new Node&lt;&gt;(element); Node&lt;E&gt; temp = head.prior; newNode.prior = temp; newNode.next = temp.next; temp.next.prior = newNode; temp.next = newNode; size++;} 删除元素 1234567public E remove(){ Node&lt;E&gt; temp = head.prior; temp.prior.next = temp.next; temp.next.prior = temp.prior; size--; return temp.element;} 栈和队列栈栈是限定近在表位进行插入和删除操作的线性表。栈最重要的性质就是后进先出，即LIFO(Last In First Out)。 顺序栈 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//静态栈的顺序存储public class MyStack&lt;E&gt; { private Object[] elements; //使用数组存储栈 private int top; //栈顶标识 private int size; //当前栈的大小 private final int DEFAULT_CAPACITY = 30; //默认初始化长度为30 /** * 初始化一个空栈 */ public MyStack(){ elements = new Object[DEFAULT_CAPACITY]; top = -1; //top=-1时表示空栈 } /** * 入栈 * @param element 元素值 */ public void push(E element) { if (size&lt;DEFAULT_CAPACITY) { elements[top + 1] = element; top++; }else{ throw new OutOfMemoryError(\"栈内存不足\"); } } /** * 出栈 */ public void pop(){ if(top==-1){ throw new NullPointerException(\"栈为空栈\"); }else{ top--; } } /** * 获取栈顶元素 * @return 栈顶元素值 */ public E peek(){ if(top==-1){ throw new NullPointerException(\"栈为空栈\"); }else{ return (E)elements[top]; } } /** * 栈实际大小 * @return */ public int size(){ return top+1; }} 链栈 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//静态链栈public class MyLinkStack&lt;E&gt; { private class Node&lt;E&gt;{ public E element; private Node&lt;E&gt; next; public Node(E element){ this.element = element; } } private Node&lt;E&gt; top; private int size; public MyLinkStack(){ top = null; } /** * 入栈 * @param element 栈元素值 */ public void push(E element){ Node&lt;E&gt; newNode = new Node&lt;&gt;(element); newNode.next = top; top = newNode; size++; } /** * 出栈 * @return 出栈元素 */ public E pop() { if(top==null){ throw new NullPointerException(\"栈为空栈\"); }else{ Node&lt;E&gt; temp = top; top = top.next; temp.next = null; size--; return temp.element; } } /** * 取得栈顶元素 * @return 栈顶元素值 */ public E peek(){ if(top==null){ throw new NullPointerException(\"栈为空栈\"); }else{ return top.element; } } /** * 获得链栈长度 * @return 链栈长度 */ public int getSize(){ return size; }} 队列队列是只允许在一端进行插入操作，另一端进行删除操作的线性表。 队列最重要的性质就是先进先出，即FIFO(First In First Out)。 顺序循环队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class MyCircleQueue&lt;E&gt; { private final int MAX_SIZE = 10; //最大长度为10(为了演示方便) private Object[] elements; //定义一个数组用来存储元素 private int front; //头指针 private int rear; //尾指针 public MyCircleQueue(){ elements = new Object[MAX_SIZE]; front = 0; rear = 0; } /** * 入队列 */ public void EnQueue(E element){ if((rear+1)%MAX_SIZE==front){ //判断队列是否已满条件 throw new IndexOutOfBoundsException(\"队列已满\"); }else{ elements[rear] = element; rear = (rear+1)%MAX_SIZE; //队列满的话移到数组开始位置 } } /** * 出队列 */ public E DeQueue(){ if(rear==front){ throw new NullPointerException(\"队列为空\"); }else{ E OldElement = (E)elements[front]; front = (front+1)%MAX_SIZE; return OldElement; } } /** * 获得队列长度 */ public int getSize(){ return (rear-front+MAX_SIZE)%MAX_SIZE; //队列长度通用公式 } /** * 获得队列头部元素 */ public E get(){ return (E)elements[front]; } /** * 队列是否为空 */ public boolean isEmpty(){ return rear==front; }} 链队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class MyLinkQueue&lt;E&gt; { private class Node{ private E element; private Node next; public Node(){ } public Node(E element){ this.element = element; } } private Node front; private Node rear; private int size; public MyLinkQueue(){ front = new Node(); rear = front; } /** * 入队 */ public void EnQueue(E element) { Node newNode = new Node(element); rear.next = newNode; rear = newNode; size++; } /** * 出队 */ public E DeQueue(){ if(front==rear){ throw new NullPointerException(\"队列为空\"); }else{ Node OldNode = front.next; front.next = OldNode.next; if(rear==OldNode){ rear = front; } size--; return OldNode.element; } } /** * 获得链队列长度 */ public int getSize(){ return size; } /** * 返回队列头部元素 */ public E get(){ return front.next.element; }} 优先队列和堆优先队列(priority queue)优先队列:优先队列也是队列，正常入队列，但是出队列按照优先级出 (优先级通常是最大、最小值，当然其他的优先级比如出现次数等也可以) 优先队列的操作也很简单: 删除优先级最高的元素 插入带有优先级的元素 优先队列的实现: 无序数组 插入O(1),查找O(n) (了解即可) AVL树或者红黑树 插入删除O(logn) 查找O(1) 构建O(nlogn) (有时会用) 更加常用的是使用堆来实现 插入删除O(logn) 查找O(1) 构建O(n) 因为经常是堆来实现优先队列，所以人们经常弄错堆和优先队列的关系。这里贴上维基百科的解释: While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is a concept like “a list” or “a map”; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array. 也就是说优先队列是我们的list和map级别，而堆就是我们的arrayList、linkedList以及hashMap级别。 堆(heap)堆是一棵具有特定性质的二叉树: 每个结点的值都大于或等于其左右孩子结点的值(大顶堆）或者每个结点的值都小于或等于其左右孩子结点的值(小顶堆)。 当h=0时，所有叶子节点都处于第h或h-1层，所以堆是一棵完全二叉树 当每个节点最多只有两个孩子时称为二叉堆，下面的就是二叉小顶堆和二叉大顶堆 12345678910111213141516171819202122232425262728293031323334353637public class Heap&lt;T extends Comparable&lt;T&gt;&gt; { private T[] heap; private int N = 0; public Heap(int maxN) { this.heap = (T[]) new Comparable[maxN+1]; } //添加元素 public void insert(T t){ heap[++N] = t; shiftUp(N); } //删除元素 public T delMax(){ T max = heap[1]; //从根节点得到最大元素 swap(1,N--); //将其和最后一个节点交换 heap[N+1] = null; //防止越界 shiftDown(1); //进行下沉操作 return max; } public int size() { return N; } private boolean less(int i, int j) { return heap[i].compareTo(heap[j])&lt;0; } private void swap(int i, int j) { T t = heap[i]; heap[i] = heap[j]; heap[j] = t; } 关于堆的插入和删除操作时的维护过程简单概括下就是:1.堆插入时进行上浮操作 12345678910//上浮操作就是不断跟父节点比较，从而交换到合适位置// 这里要注意一下//1. k是新插入元素在数组中的下标//2. less（）函数是比较堆中两个数的大小private void shiftUp(int k) { while (k &gt; 1 &amp;&amp; less(k / 2, k)) { //k节点的父节点位置为k/2 swap(k / 2, k); k = k / 2; }} 2.堆删除时进行下沉操作 12345678910111213//下沉操作就是比较其跟孩子节点的大小private void shiftDown(int k) { while (2 * k &lt;= N) { int j = 2 * k; //和两个孩子节点中值大的交换 if (j &lt; N &amp;&amp; less(j, j + 1)) j++; if (!less(k, j)) break; swap(k, j); k = j; }} 详细过程图片可以参考下面的文章: https://en.wikipedia.org/wiki/Heap_(data_structure) https://www.cnblogs.com/wmyskxz/p/9301021.html 我们上面学习的二叉堆效率不是很高，实际上还有很多类型的堆，他们的效率非常高高。下表以最小堆为例，给出了各种堆结构的时间复杂度: 我们知道堆查找时都是O(1),主要性能的差异就在插入和删除后的维护过程。从上面我们可以看到二叉堆的效率相对较差，而斐波那契堆等的效率最好。","link":"/2020/04/29/%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%95%B0%E7%BB%84%E3%80%81%E9%93%BE%E8%A1%A8%E3%80%81%E6%A0%88%E3%80%81%E9%98%9F%E5%88%97/"},{"title":"数据结构与算法概述","text":"主要内容 数据结构概述 算法时间复杂度分析 数据结构概述数据结构主要从三个方面理解和学习: 按照分类角度的不同，数据结构包括逻辑结构和物理结构。 逻辑结构指的是数据对象中数据元素之间的关系，有以下四种： 集合 结构中的数据元素除了同属于一种类型外，别无其它关系。 线性结构 结构中的数据元素之间是一对一的关系。 树结构 结构中的数据元素之间存在一对多的多层关系。 图结构 结构中的数据元素之间是多对多的关系。 物理结构也叫存储结构，指的是数据的逻辑结构在计算机中的存储方式。有以下两种： 顺序存储:把数据元素存放在地址连续的存储单元里，其数据间的逻辑关系和物理关系是一致的。 链式存储结构：把数据元素存放在任意存储单元里，这组存储单元可以是连续的也可以是不连续的。但是需要在每一个数据元素中增加一个存放地址的指针，用此指针来表示数据元素之间的逻辑关系 算法概述算法定义 ：算法是解决特定问题求解步骤的描述，在计算机中为指令的有限序列，并且每条指令表示一个或多个操作 算法的特性： 输入：有0个或多个输入 输出：至少有1个或多个输出 有穷性：算法在有限的步骤后应该自动结束而不会无限循环。 确定性：算法中的每个步骤都有确定的含义，不会出现二义性 可行性：算法的每一步都是可行的 算法的设计要求： 正确性：算法对于合法数据能够得到满足要求的结果，能够处理非法输入，并得到合理的结果。 可读性：算法要便于阅读、理解和交流 健壮性：算法不应该得到莫名其妙的结果 性价比：利用最少的资源得到满足要求的结果 算法的时间复杂度算法的时间复杂度定义 在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随n的变化情况并确定T(n)的数量级。算法的时间复杂度，也就是算法的时间量度，记作:T(n)=O(f(n)) [大O表示法]。它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐近时间复杂度，简称为时间复杂度。其中f(n)是问题规模n的某个函数。 推导大O阶方法 1.用常数1取代运行时间中的所有加法常数2.在修改后的运行次数函数中，只保留最高阶项。3.如果最高阶项存在且不是1，则去除与这个项目相乘的常数。得到的结果就是大O阶 注意有时候，算法中基本操作重复执行的次数还随问题的输入数据集不同而不同，如在冒泡排序中，输入数据有序而无序，其结果是不一样的，通常我们看平均时间复杂度。 有时候，算法中基本操作重复执行的次数还随问题的输入数据集不同而不同，如在冒泡排序中，输入数据有序而无序，其结果是不一样的。此时，我们计算平均值。 以下面几个例子为例,简单介绍下推倒大O阶的方法： demo1 ：（1）执行1次，（2）执行1次, （3）执行1次 T(n) = 1+1+1= 3=O(1) 也称为常数阶 123int sum = 0; //(1)sum = (1+i)*i/2; //(2)System.out.println(sum); //(3) demo2: （1）执行1次，（2）执行n次（3）执行n次 T(n) = 1+2n =O(n) 也称为线性阶 1234int sum=0; //(1)for (int i = 0; i &lt;=n ; i++) { //(2) sum = sum + i; //(3)} demo3：（1）执行1次， （2）执行n次（3）执行n2次,（4）执行n2次 T(n)=1+n+2n2=O(n2),称为n方阶 123456int sum=0; //(1)for (int i = 0; i &lt;=n ; i++) { //(2) for(int j = 0; j&lt;=n ; j++){ //(3) sum = sum + i; //(4) } } demo4：(1)的频度是1,设(2)的频度是f(n)，则：2f(n)&lt;=n;f(n)&lt;=log2n，取最大值f(n)= log2n, T(n)=O(log2n) 也称为对数阶 1234int i = 1; //(1) while (i &lt;= n){ i = i*2; //(2) } 常见时间复杂度 O(1)&lt;O(logn)&lt;O(n)&lt;O(nlogn)&lt;O(n^2)&lt; O(n^3) &lt;O(2n)&lt;O(n!)&lt;O(n^n) O(1)、O(logn)、O(n)、O(n2)上文都已经介绍过比较经典的例子，O(nlog n)在之后排序算法会介绍到（归并排序和快排）。 O(n^3)、 O(2n)、O(n!)、O(n^n)等除非是很小的n，否则哪怕n=100，运行时间也会非常大。所以对于这种复杂度的算法一般没有讨论的意义。 算法的空间复杂度空间复杂度：算法所需存储空间的度量，记作： S(n)=O( f(n) ) ,其中 n 为问题的规模。 1234567891011int x;for(int i=0;i&lt;n;i++){ x = 10; //O（1） //使用x}for(int i=0;i&lt;n;i++){ int x = 10; //O(n) //使用a} 一个算法在计算机存储器上所占用的存储空间，包括三个方面: 存储算法本身所占用的存储空间 算法的输入输出数据所占用的存储空间 算法在运行过程中临时占用的存储空间 我们知道代码本身多几行少几行所占用的空间可以忽略不计，而算法的输入输出无论用什么算法都是一定的，所以我们需要关注的应该是3,即算法在运行过程中临时占用的存储空间。 另外如果额外空间相对于输入数据量来说是个常数，则称此算法是原地工作。","link":"/2020/04/29/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"},{"title":"leetcode树结构练习","text":"树结构练习题的tips: 整体而言，因为树结构是递归定义的，所以涉及到树的问题一般可以用递归进行解决。 对于树的前中后序遍历一定要掌握递归方法和非递归方法(深搜DFS使用栈解决)。 二叉树的层次遍历的 非递归方法(广搜BFS使用队列解决) 也需要掌握(因为是层次遍历,所以不存在递归地方法)。 对于二叉查找树BST而言，主要考点就两个: (1)BST的定义（①左子树节点均小于根节点的值 ②右子树的节点均大于根节点的值 ③左右子树也是一个平衡二叉树）(2)二叉查找树的中序遍历是一个递增序列。也正是因为定义的递归性，涉及到BST的问题，通常也是用递归解决。 建议练习前先看我的关于树的文章: 树的基本概念和前中后序遍历 二叉排序树、平衡二叉树、红黑树、B树、B+树 二叉树前中后序遍历 前序遍历144. Binary Tree Preorder Traversal (Medium) 后续遍历:145. Binary Tree Postorder Traversal (Hard) 中序遍历:94. Binary Tree Inorder Traversal (Medium) 1.递归实现二叉树的前中后序遍历① 前序 12345678910111213class Solution { public List&lt;Integer&gt; preorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); helpPre(root,res); return res; } private void helpPre(TreeNode root,List&lt;Integer&gt; res){ if(root ==null) return; res.add(root.val); //访问根节点 helpPre(root.left,res); //访问左子树 helpPre(root.right,res); //访问右子树 }} ② 后序 12345678910111213class Solution { public List&lt;Integer&gt; postorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); helpPost(root,res); return res; } private void helpPost(TreeNode root,List&lt;Integer&gt; res){ if(root == null) return; helpPost(root.left,res); //访问左子树 helpPost(root.right,res); //访问右子树 res.add(root.val); //访问根节点 }} ③ 中序 12345678910111213class Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); helpInorder(root,res); return res; } private void helpInorder(TreeNode root,List&lt;Integer&gt; res){ if(root ==null) return; helpInorder(root.left,res); //左子树 res.add(root.val); //根节点 helpInorder(root.right,res); //右子树 }} 2.非递归实现二叉树的前中后序遍历前中后序遍历使用深度优先搜索DFS实现 ① 前序 12345678910111213141516class Solution { public List&lt;Integer&gt; preorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.isEmpty()){ TreeNode node = stack.pop(); res.add(node.val); //先入栈右子树，保证左子树能够先被遍历 if(node.right !=null) stack.push(node.right); if(node.left != null) stack.push(node.left); } return res; }} ② 后序前序遍历为 root -&gt; left -&gt; right，后序遍历为 left -&gt; right -&gt; root。可以将前序遍历的代码改为root -&gt; right -&gt; left，那么这个顺序就和后序遍历正好相反，再反转链表即可得到后序遍历的结果。 12345678910111213141516class Solution { public List&lt;Integer&gt; postorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root==null) return res; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.isEmpty()){ TreeNode node = stack.pop(); res.add(node.val); if(node.left!=null) stack.push(node.left); if(node.right!=null) stack.push(node.right); } Collections.reverse(res); //反转链表 return res; }} ③ 中序 123456789101112131415161718class Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(root == null) return res; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) { while (cur != null) { stack.push(cur); cur = cur.left; } TreeNode node = stack.pop(); res.add(node.val); cur = node.right; } return res; }} 二叉树层次遍历层次遍历使用 广度优先搜索(BFS)实现，利用的就是 BFS 一层一层遍历的特性 1.二叉树的层次遍历102. Binary Tree Level Order Traversal (Medium) 1234567891011121314151617181920class Solution { public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) { List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); if(root == null) return list; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while(!queue.isEmpty()){ int levNum = queue.size(); List&lt;Integer&gt; subList = new ArrayList&lt;&gt;(); for(int i=0;i&lt;levNum;i++){ TreeNode node = queue.poll(); subList.add(node.val); if(node.left != null) queue.offer(node.left); if(node.right != null) queue.offer(node.right); } list.add(subList); } return list; }} 2.二叉树每层节点的平均数637. Average of Levels in Binary Tree(easy) 12345678910111213141516171819class Solution { public List&lt;Double&gt; averageOfLevels(TreeNode root) { List&lt;Double&gt; res = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while(!queue.isEmpty()){ int levNum = queue.size(); double sum = 0; for(int i=0;i&lt;levNum;i++){ TreeNode node = queue.poll(); sum+=node.val; if(node.left != null) queue.offer(node.left); if(node.right != null) queue.offer(node.right); } res.add(sum/levNum); } return res; }} 3.得到二叉树左下角的值513.Find Bottom Left Tree Value(Medium) 1234567891011121314class Solution { public int findBottomLeftValue(TreeNode root) { int res = 0; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); while(!queue.isEmpty()){ TreeNode node = queue.poll(); res = node.val; if(node.right!=null) queue.offer(node.right); if(node.left!=null) queue.offer(node.left); } return res; }} 平衡二叉树BST1.判断是否是二叉搜索树98. Validate Binary Search Tree (Medium) 12345678910class Solution { public boolean isValidBST(TreeNode root) { return isBSTHelper(root,Long.MIN_VALUE,Long.MAX_VALUE); } private boolean isBSTHelper(TreeNode node,long lower,long upper){ if(node == null) return true; if(node.val &lt;= lower || node.val &gt;= upper) return false; return isBSTHelper(node.left,lower,node.val) &amp;&amp; isBSTHelper(node.right,node.val,upper); }} 2.二叉搜索树最近公共祖先235. Lowest Common Ancestor of a Binary Search Tree (Easy) 递归法： 1234567class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root.val&gt;p.val &amp;&amp; root.val&gt;q.val) return lowestCommonAncestor(root.left,p,q); if(root.val&lt;p.val &amp;&amp; root.val&lt;q.val) return lowestCommonAncestor(root.right,p,q); return root; }} 迭代法： 1234567891011class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { TreeNode cur = root; while(cur != null){ if(cur.val&gt;p.val &amp;&amp; cur.val&gt;q.val) cur = cur.left; else if(cur.val&lt;p.val &amp;&amp; cur.val&lt;q.val) cur = cur.right; else return cur; } return null; }} 3.二叉树最近公共祖先236. Lowest Common Ancestor of a Binary Tree (Medium) 123456789class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root ==null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if(left != null &amp;&amp; right !=null) return root; return left==null?right:left; }} 4. 修剪二叉查找树669.Trim a Binary Search Tree (Easy) 12345678910111213class Solution { public TreeNode trimBST(TreeNode root, int L, int R) { if(root == null) return null; //节点值小于L，则该节点整个左子树的值小于L，丢弃左子树，返回右子树。 if(root.val &lt; L) return trimBST(root.right,L,R); //节点值大于R，则该节点整个右子树的值大于R，丢弃右子树，返回左子树。 if(root.val &gt; R) return trimBST(root.left,L,R); //节点值符合要求，则向下传递递归调用 root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root; }} 5.寻找二叉查找树的第k小的元素230.Kth Smallest Element in a BST (Medium) 123456789101112131415class Solution { private int res; private int count; public int kthSmallest(TreeNode root, int k) { inorder(root,k); return res; } private void inorder(TreeNode root,int k){ if(root==null) return; inorder(root.left,k); count++; if(count==k) res = root.val; inorder(root.right,k); }} 6.把二叉查找树每个节点的值都加上比它大的节点的值538. Convert BST to Greater Tree(Easy) 1234567891011class Solution { private int sum=0; public TreeNode convertBST(TreeNode root) { if(root==null) return null; convertBST(root.right); sum+=root.val; root.val = sum; convertBST(root.left); return root; }} 7.从有序数组中构造平衡二叉树108.Convert Sorted Array to Binary Search Tree (Easy) 123456789101112131415class Solution { public TreeNode sortedArrayToBST(int[] nums) { return helper(nums,0,nums.length-1); } private TreeNode helper(int[] nums,int lo,int hi){ if(lo &gt; hi) return null; int mid = lo + (hi - lo)/2; TreeNode left = helper(nums,lo,mid-1); TreeNode root = new TreeNode(nums[mid]); TreeNode right = helper(nums,mid+1,hi); root.left = left; root.right = right; return root; }} 8.从有序链表中构造二叉查找树109. Convert Sorted List to Binary Search Tree (Medium) 123456789101112131415161718192021222324class Solution { private ListNode head; public TreeNode sortedListToBST(ListNode head) { this.head = head; int length = 0; ListNode cur = head; while(cur != null){ cur = cur.next; length++; } return helper(0,length-1); } private TreeNode helper(int lo,int hi){ if(lo&gt;hi) return null; int mid = lo + (hi-lo)/2; TreeNode left = helper(lo,mid-1); TreeNode root = new TreeNode(head.val); head = head.next; //注意对当前元素操作完成后需要指向下一个元素 TreeNode right = helper(mid+1,hi); root.left = left; root.right = right; return root; }} 9.在二叉查找树中寻找两个节点，使它们的和为一个给定值653. Two Sum IV - Input is a BST (Easy) 123456789101112class Solution { public boolean findTarget(TreeNode root, int k) { Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); return helper(root,k,set); } private boolean helper(TreeNode root,int k,Set&lt;Integer&gt; set){ if(root == null) return false; if(set.contains(k-root.val)) return true; set.add(root.val); return helper(root.left,k,set) || helper(root.right,k,set); }} 10.在二叉查找树中查找两个节点之差的最小绝对值530.Minimum Absolute Difference in BST (Easy) 123456789101112class Solution { private int min = Integer.MAX_VALUE; private TreeNode prev = null; public int getMinimumDifference(TreeNode root) { if(root == null) return min; getMinimumDifference(root.left); if(prev != null) min = Math.min(min,root.val-prev.val); prev = root; getMinimumDifference(root.right); return min; }} 递归1.二叉树的最大深度104. Maximum Depth of Binary Tree (Easy) 123456789101112131415/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int maxDepth(TreeNode root) { if(root==null){return 0;} return Math.max(maxDepth(root.left),maxDepth(root.right))+1; }} 2.二叉树的最小深度111. Minimum Depth of Binary Tree (Easy) 1234567891011121314151617/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int minDepth(TreeNode root) { if(root == null){return 0;} int left = minDepth(root.left); int right = minDepth(root.right); return (left==0 || right==0)? left+right+1 : Math.min(left,right) + 1; }} 3.平衡二叉树110. Balanced Binary Tree (Easy) 12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean isBalanced(TreeNode root) { if(root == null) return true; return isBalanced(root.left) &amp;&amp; isBalanced(root.right) &amp;&amp; (Math.abs(maxDepth(root.left)-maxDepth(root.right))&lt;=1); } private int maxDepth(TreeNode root){ if(root == null) return 0; return 1+Math.max(maxDepth(root.left),maxDepth(root.right)); }} 4.二叉树翻转226. Invert Binary Tree (Easy) 12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode invertTree(TreeNode root) { if(root == null) return null; TreeNode left = invertTree(root.left); TreeNode right = invertTree(root.right); root.left = right; root.right = left; return root; }} 5.归并两个二叉树617. Merge Two Binary Trees (Easy) 12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null) return t2; if (t2 == null) return t1; t1.val = t1.val + t2.val; t1.left = mergeTrees(t1.left,t2.left); t1.right = mergeTrees(t1.right,t2.right); return t1; }} 6.路径和112. Path Sum (Easy)递归求解： 12345678910111213141516/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean hasPathSum(TreeNode root, int sum) { if(root==null) return false; if(root.left==null &amp;&amp; root.right==null &amp;&amp;sum-root.val==0) return true; return hasPathSum(root.left,sum-root.val) || hasPathSum(root.right,sum-root.val); }} 非递归求解： 1234567891011121314151617181920212223242526272829303132333435/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean hasPathSum(TreeNode root, int sum) { if(root==null) return false; Stack&lt;TreeNode&gt; nodeStack = new Stack&lt;&gt;(); Stack&lt;Integer&gt; sumStack = new Stack&lt;&gt;(); nodeStack.push(root); sumStack.push(root.val); while(!nodeStack.isEmpty()){ TreeNode node = nodeStack.pop(); int nodeVal = sumStack.pop(); if(node.left==null &amp;&amp; node.right==null &amp;&amp; nodeVal==sum){ return true; }else{ if(node.left!=null){ nodeStack.push(node.left); sumStack.push(node.left.val+nodeVal); } if(temp.right!=null){ nodeStack.push(node.right); sumStack.push(node.right.val+nodeVal); } } } return false; }} 7.路径和II113. Path Sum II (Medium) 1234567891011121314151617181920class Solution { public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); helper(root,sum,new ArrayList&lt;&gt;(),res); return res; } private void helper(TreeNode root,int sum,List&lt;Integer&gt; subList,List&lt;List&lt;Integer&gt;&gt; res){ if(root == null) return; subList.add(root.val); if(root.left==null &amp;&amp; root.right==null &amp;&amp; sum-root.val==0){ res.add(new ArrayList&lt;&gt;(subList)); } helper(root.left,sum-root.val,subList,res); helper(root.right,sum-root.val,subList,res); //如果路径和等于目标值会直接将subList加入到最终的结果中 //但是如果执行两个子递归后仍无法得到目标值，则说明当前root节点无法引导我们获得正确的值 //将其从subList中删除(回溯的思想) subList.remove(subList.size()-1); }} 8.路径和III437. Path Sum III (Easy) 12345678910111213class Solution { public int pathSum(TreeNode root, int sum) { if(root == null) return 0; return helper(root,sum) + pathSum(root.left,sum) + pathSum(root.right,sum); } private int helper(TreeNode root,int sum){ if(root == null) return 0; int res = 0; if(sum-root.val==0) res++; res+= helper(root.left,sum-root.val)+helper(root.right,sum-root.val); return res; }} 9.判断树是否是另一个数的子树572. Subtree of Another Tree (Easy) 123456789101112class Solution { public boolean isSubtree(TreeNode s, TreeNode t) { if(s==null) return false; return helper(s,t) || isSubtree(s.left,t) || isSubtree(s.right,t); } private boolean helper(TreeNode s, TreeNode t){ if(t==null &amp;&amp; s==null) return true; if(t==null || s== null) return false; if(t.val!=s.val) return false; return helper(s.left,t.left) &amp;&amp; helper(s.right,t.right); }} 10.树的对称101. Symmetric Tree (Easy) 123456789101112class Solution { public boolean isSymmetric(TreeNode root) { if(root==null) return true; return helper(root.left,root.right); } private boolean helper(TreeNode left,TreeNode right){ if(left==null &amp;&amp; right==null) return true; if(left==null || right==null) return false; if(left.val != right.val) return false; return helper(left.left,right.right) &amp;&amp; helper(left.right,right.left); }} 11.左叶子结点的和404. Sum of Left Leaves (Easy) 12345678910class Solution { public int sumOfLeftLeaves(TreeNode root) { if(root == null) return 0; int res = 0; if(root.left!=null &amp;&amp; root.left.left==null &amp;&amp; root.left.right==null){ res+=root.left.val; } return res+sumOfLeftLeaves(root.left)+sumOfLeftLeaves(root.right); }} 12.二叉树最大路径和124. Binary Tree Maximum Path Sum(Hard) 对于每个节点，我们都会从两个方面思考 是路径的root，此时的最大值来自于node.left+node.right 不是路径的root，此时的最大值来自于node.left或node.right 1234567891011121314class Solution { private int max = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) { helper(root); return max; } private int helper(TreeNode root){ if(root==null) return 0; int left = Math.max(0,helper(root.left)); int right = Math.max(0,helper(root.right)); max = Math.max(max,left+right+root.val); return Math.max(left,right)+root.val; }} 13.二叉树两结点最长路径543. Diameter of Binary Tree (Easy) 12345678910111213141516class Solution { private int max; public int diameterOfBinaryTree(TreeNode root) { depthHelper(root); //题目规定最大值是节点数-1 return max-1; } private int depthHelper(TreeNode root){ if(root == null) return 0; int left = depthHelper(root.left); int right = depthHelper(root.right); //这里记录的是节点数 max = Math.max(max,left+right+1); return Math.max(left,right)+1; }} 14. 相同节点值的最大路径长度687. Longest Univalue Path (Easy) 1234567891011121314151617class Solution { private int max; public int longestUnivaluePath(TreeNode root) { helper(root); return max; } private int helper(TreeNode root){ if (root == null) return 0; int left = helper(root.left); int right = helper(root.right); int leftPath = (root.left!=null &amp;&amp; root.left.val== root.val) ? left + 1 : 0; int rightPath = (root.right!=null &amp;&amp; root.right.val == root.val) ? right + 1 : 0; max = Math.max(max, leftPath + rightPath); return Math.max(leftPath, rightPath); }}","link":"/2020/05/16/leetcode%E6%A0%91%E7%BB%93%E6%9E%84%E7%BB%83%E4%B9%A0/"},{"title":"leetcode图结构练习","text":"","link":"/2020/06/06/leetcode%E5%9B%BE%E7%BB%93%E6%9E%84%E7%BB%83%E4%B9%A0/"},{"title":"leetcode栈堆队列练习","text":"栈堆队列的tips: 栈和队列的基本特性: 栈是后入先出、队列是先入先出。 对于单调栈问题的处理 堆主要用来处理Top K问题。(数据流和滑动窗口) 1.用栈实现队列232. Implement Queue using Stacks (Easy) 1.使用两个栈实现。 入队列O(1) 出队列和得到队列元素O(n) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class MyQueue { private Stack&lt;Integer&gt; stack1; private Stack&lt;Integer&gt; stack2; /** Initialize your data structure here. */ public MyQueue() { this.stack1 = new Stack&lt;&gt;(); this.stack2 = new Stack&lt;&gt;(); } /** Push element x to the back of queue. */ public void push(int x) { stack1.push(x); } /** Removes the element from in front of queue and returns that element. */ public int pop() { if(stack2.empty()){ while(!stack1.empty()){ stack2.push(stack1.pop()); } } return stack2.pop(); } /** Get the front element. */ public int peek() { if(stack2.empty()){ while(!stack1.empty()){ stack2.push(stack1.pop()); } } return stack2.peek(); } /** Returns whether the queue is empty. */ public boolean empty() { return stack1.isEmpty() &amp;&amp; stack2.isEmpty(); }}/** * Your MyQueue object will be instantiated and called as such: * MyQueue obj = new MyQueue(); * obj.push(x); * int param_2 = obj.pop(); * int param_3 = obj.peek(); * boolean param_4 = obj.empty(); */ 使用两个栈。 入队列O(n),出队列和取得队列元素O(1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445class MyQueue { private Stack&lt;Integer&gt; stack1; private Stack&lt;Integer&gt; stack2; /** Initialize your data structure here. */ public MyQueue() { this.stack1 = new Stack&lt;&gt;(); this.stack2 = new Stack&lt;&gt;(); } /** Push element x to the back of queue. */ public void push(int x) { while(!stack1.isEmpty()){ stack2.push(stack1.pop()); } stack1.push(x); while(!stack2.isEmpty()){ stack1.push(stack2.pop()); } } /** Removes the element from in front of queue and returns that element. */ public int pop() { return stack1.pop(); } /** Get the front element. */ public int peek() { return stack1.peek(); } /** Returns whether the queue is empty. */ public boolean empty() { return stack1.isEmpty(); }}/** * Your MyQueue object will be instantiated and called as such: * MyQueue obj = new MyQueue(); * obj.push(x); * int param_2 = obj.pop(); * int param_3 = obj.peek(); * boolean param_4 = obj.empty(); */ 2.用队列实现栈225. Implement Stack using Queues (Easy) 1.使用两个队列，入栈O(1) 出栈和获得栈顶元素O(n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class MyStack { private Queue&lt;Integer&gt; queue1; private Queue&lt;Integer&gt; queue2; /** Initialize your data structure here. */ public MyStack() { this.queue1 = new LinkedList&lt;&gt;(); this.queue2 = new LinkedList&lt;&gt;(); } /** Push element x onto stack. */ public void push(int x) { queue1.offer(x); } /** Removes the element on top of the stack and returns that element. */ public int pop() { int len = queue1.size(); for(int i=0;i&lt;len-1;i++){ queue2.offer(queue1.poll()); } Queue&lt;Integer&gt; temp = queue1; queue1 = queue2; queue2 = temp; return queue2.poll(); } /** Get the top element. */ public int top() { int len = queue1.size(); for(int i=0;i&lt;len-1;i++){ queue2.offer(queue1.poll()); } int top = queue1.peek(); queue2.offer(queue1.poll()); Queue&lt;Integer&gt; temp = queue1; queue1 = queue2; queue2 = temp; return top; } /** Returns whether the stack is empty. */ public boolean empty() { return queue1.isEmpty(); }}/** * Your MyStack object will be instantiated and called as such: * MyStack obj = new MyStack(); * obj.push(x); * int param_2 = obj.pop(); * int param_3 = obj.top(); * boolean param_4 = obj.empty(); */ 2.使用两个队列，入栈O(n) 出栈和获得栈顶元素O(1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445class MyStack { private Queue&lt;Integer&gt; queue1; private Queue&lt;Integer&gt; queue2; /** Initialize your data structure here. */ public MyStack() { this.queue1 = new LinkedList&lt;&gt;(); this.queue2 = new LinkedList&lt;&gt;(); } /** Push element x onto stack. */ public void push(int x) { while(!queue1.isEmpty()){ queue2.offer(queue1.poll()); } queue1.offer(x); while(!queue2.isEmpty()){ queue1.offer(queue2.poll()); } } /** Removes the element on top of the stack and returns that element. */ public int pop() { return queue1.poll(); } /** Get the top element. */ public int top() { return queue1.peek(); } /** Returns whether the stack is empty. */ public boolean empty() { return queue1.isEmpty(); }}/** * Your MyStack object will be instantiated and called as such: * MyStack obj = new MyStack(); * obj.push(x); * int param_2 = obj.pop(); * int param_3 = obj.top(); * boolean param_4 = obj.empty(); */ 3.使用一个队列，入栈O(n) 出栈和获得栈顶元素O(1) 1234567891011121314151617181920212223242526272829303132333435363738394041class MyStack { private Queue&lt;Integer&gt; queue1; /** Initialize your data structure here. */ public MyStack() { this.queue1 = new LinkedList&lt;&gt;(); } /** Push element x onto stack. */ public void push(int x) { queue1.offer(x); int len = queue1.size(); for(int i=0;i&lt;len-1;i++){ queue1.offer(queue1.poll()); } } /** Removes the element on top of the stack and returns that element. */ public int pop() { return queue1.poll(); } /** Get the top element. */ public int top() { return queue1.peek(); } /** Returns whether the stack is empty. */ public boolean empty() { return queue1.isEmpty(); }}/** * Your MyStack object will be instantiated and called as such: * MyStack obj = new MyStack(); * obj.push(x); * int param_2 = obj.pop(); * int param_3 = obj.top(); * boolean param_4 = obj.empty(); */ 3.最小值栈155. Min Stack (Easy) 123456789101112131415161718192021222324252627282930313233343536373839class MinStack { private Stack&lt;Integer&gt; dataStack; private Stack&lt;Integer&gt; minStack; int min = Integer.MAX_VALUE; /** initialize your data structure here. */ public MinStack() { this.dataStack = new Stack&lt;&gt;(); this.minStack = new Stack&lt;&gt;(); } public void push(int x) { dataStack.push(x); if(x&lt;min) min = x; minStack.push(min); } public void pop() { dataStack.pop(); minStack.pop(); min = minStack.isEmpty()?Integer.MAX_VALUE:minStack.peek(); } public int top() { return dataStack.peek(); } public int getMin() { return minStack.peek(); }}/** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.getMin(); */ 4.括号匹配20. Valid Parentheses(Easy) 12345678910111213class Solution { public boolean isValid(String s) { Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for(char c:s.toCharArray()){ if(c=='(' || c=='[' || c=='{'){stack.push(c);} else if(c==')' &amp;&amp; !stack.isEmpty()&amp;&amp;stack.peek()=='(') stack.pop(); else if(c==']' &amp;&amp; !stack.isEmpty()&amp;&amp;stack.peek()=='[') stack.pop(); else if(c=='}' &amp;&amp; !stack.isEmpty()&amp;&amp;stack.peek()=='{') stack.pop(); else return false; } return stack.isEmpty(); }} 下面的5.6.7都是单调栈的应用 5.数组中元素与下一个比它大的元素之间的距离739. Daily Temperatures (Medium) 我们维护一个单调递减的stack，stack内部存的是原数组的每个index。每当我们遇到一个比当前栈顶所对应的数（就是T[i]&gt;T[stack.peek()）大的数的时候，我们就遇到了一个“大数“。这个”大数“比它之前多少个数大我们不知道，但是至少比当前栈顶所对应的数大。我们弹出栈内所有对应数比这个数小的栈内元素，并更新它们在返回数组中对应位置的值。因为这个栈本身的单调性，当我们栈顶元素所对应的数比这个元素大的时候，我们可以保证，栈内所有元素都比这个元素大。对于每一个元素，当它出栈的时候，说明它遇到了自己的next greater element，我们也就要更新return数组中的对应位置的值。如果一个元素一直不曾出栈，那么说明不存在next greater element，我们也就不用更新return数组了。 1234567891011121314151617class Solution { public int[] dailyTemperatures(int[] T) { int[] res = new int[T.length]; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for(int i=0;i&lt;T.length;i++){ //2. 更改距离 //因为不知道有多少个数比新加入的T[i]小，所以使用while循环 while(!stack.isEmpty() &amp;&amp; T[i]&gt;T[stack.peek()]){ int index = stack.pop(); res[index] = i-index; } //1.把index先push进去 stack.push(i); } return res; }} 6.数组比当前数组中元素大的下一个元素496. Next Greater Element I (Easy) 123456789101112131415class Solution { public int[] nextGreaterElement(int[] nums1, int[] nums2) { int[] res = new int[nums1.length]; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for(int num:nums2){ while(!stack.isEmpty() &amp;&amp; stack.peek()&lt;num) map.put(stack.pop(),num); stack.push(num); } for(int i=0;i&lt;nums1.length;i++){ res[i] = map.getOrDefault(nums1[i],-1); } return res; }} 7.循环数组中比当前元素大的下一个元素503. Next Greater Element II (Medium) 这里需要注意对于循环数组的问题一个常见的处理手段就是通过余数，然后将数组的长度扩大两倍即可 1234567891011121314class Solution { public int[] nextGreaterElements(int[] nums) { int N = nums.length; int[] res = new int[N]; Arrays.fill(res,-1); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); for(int i=0;i&lt; 2*N;i++){ int num = nums[i%N]; while(!stack.isEmpty() &amp;&amp; num&gt;nums[stack.peek()]) res[stack.pop()] = num; if(i&lt;N) stack.push(i); } return res; }} 8.返回数据流中第K大元素703. Kth Largest Element in a Stream (Easy) 12345678910111213141516171819202122232425262728class KthLargest { private PriorityQueue&lt;Integer&gt; q; //Java中默认是小顶堆 private int k; public KthLargest(int k, int[] nums) { this.k = k; q = new PriorityQueue&lt;&gt;(k); for(int num:nums){ add(num); } } public int add(int val) { if(q.size()&lt;k) q.offer(val); else if(q.peek() &lt; val){ q.poll(); q.offer(val); } return q.peek(); }}/** * Your KthLargest object will be instantiated and called as such: * KthLargest obj = new KthLargest(k, nums); * int param_1 = obj.add(val); */ 9. 滑动窗口最大值239. Sliding Window Maximum (Hard) 1234567891011121314151617class Solution { public int[] maxSlidingWindow(int[] nums, int k) { if(nums == null || k==0 || nums.length&lt;k) return new int[0]; int[] res = new int[nums.length-k+1]; Deque&lt;Integer&gt; window = new LinkedList&lt;&gt;(); for(int i=0;i&lt;nums.length;i++){ //超出window左界 if(i&gt;=k &amp;&amp; window.peekFirst() &lt;= i-k) window.pollFirst(); while(!window.isEmpty() &amp;&amp; nums[i]&gt;nums[window.peekLast()]) window.pollLast(); window.offer(i); if (i &gt;= k-1) { res[i - k+1] = nums[window.peekFirst()]; } } return res; }}","link":"/2020/05/11/leetcode%E6%A0%88%E5%A0%86%E9%98%9F%E5%88%97%E7%BB%83%E4%B9%A0/"},{"title":"leetcode字符串练习","text":"字符串练习tips 通常字符串是结合其他算法进行考察，如果是单纯的字符串问题，只需要具体问题具体分析即可。 A的ascii码是65 小a的ascii码是97。使用过程中通常会定义数组int[256]进行哈希存储键盘可输入字符的次数。 Java中对于字符串的常用API等。 1.字符串循环移位包含编程之美 3.1 给定两个字符串 s1 和 s2，要求判定 s2 是否能够被 s1 做循环移位得到的字符串包含。 s1 = AABCD, s2 = CDAAReturn : true s1 进行循环移位的结果是 s1s1 的子字符串，因此只要判断 s2 是否是 s1s1 的子字符串即可。 例如如果保留循环移位前的单词，那么AABCD会变为AABCDA AABCDAA .. 最终变为AABCDAABCD,这时只需要在s1s1中判断是否存在s2即可。 12s1 += s1;return s1.indexOf(s2)!=-1; 2.字符串循环移位编程之美 2.17 将字符串向右循环移动 k 位。 s = “abcd123” k = 3Return “123abcd” 将 abcd123 中的 abcd 和 123 单独翻转，得到 dcba321，然后对整个字符串进行翻转，得到 123abcd。 1234567891011121314void RightShift(char[] str,int N,int k){ K%=N; //K可能大于N reverse(str,0,N-K-1); reverse(str,N-K,N-1); reverse(str,0,N-1);} void reverse(char[] str,int lo,int hi){ for(;lo&lt;hi;lo++,hi--){ char temp = str[lo]; str[lo] = str[hi]; str[hi] = temp; }} 3.字符串中单词的翻转程序员代码面试指南 s = “I am a student”Return “student a am I” 和上面的方法类似，先将每个单词翻转，然后将整个字符串翻转。 4.两个字符串包含的字符是否完全相同242. Valid Anagram (Easy) 12345678910class Solution { public boolean isAnagram(String s, String t) { int[] count = new int[26];//本题只包含小写字符 for(char c:s.toCharArray()) count[c-'a']++; for(char c:t.toCharArray()) count[c-'a']--; for(int x:count) if(x!=0) return false; return true; }} 5. 计算一组字符集合可以组成的回文字符串的最大长度409.Longest Palindrome (Easy) 123456789101112class Solution { public int longestPalindrome(String s) { int res = 0; int[] count = new int[256]; for(char c:s.toCharArray()) count[c]++; for(int x:count){ res += x/2*2; //比如3个可以拿出两个来凑回文字符串 } if(res&lt;s.length()) res++; return res; }} 6.字符串同构205. Isomorphic Strings (Easy) 只需要记录上次字符出现的位置即可。 12345678910111213class Solution { public boolean isIsomorphic(String s, String t) { if(s.length()!=t.length()) return false; int[] m1 = new int[256]; int[] m2 = new int[256]; for(int i=0;i&lt;s.length();i++){ if(m1[s.charAt(i)]!=m2[t.charAt(i)]) return false; m1[s.charAt(i)] = i+1; //这里只是为了避开初始化为0的情况，加几都可以 m2[t.charAt(i)] = i+1; } return true; }} 7. 回文子字符串个数647. Palindromic Substrings (Medium) 1234567891011121314151617class Solution { private int res; public int countSubstrings(String s) { for(int i=0;i&lt;s.length();i++){ extend(s,i,i); extend(s,i,i+1); } return res; } private void extend(String s,int start,int end){ while(start&gt;=0 &amp;&amp; end &lt; s.length() &amp;&amp; s.charAt(start)==s.charAt(end)){ res++; start--; end++; } }} 8.判断一个整数是否是回文数9. Palindrome Number (Easy) 要求不能使用额外空间，也就不能将整数转换为字符串进行判断。 将整数分成左右两部分，右边那部分需要转置，然后判断这两部分是否相等。 123456789101112class Solution { public boolean isPalindrome(int x) { if(x==0) return true; if(x&lt;0 || x%10==0) return false; int right = 0; while(right&lt;x){ right = right*10 + x%10; x =x/10; } return right==x || right/10==x; }}","link":"/2020/06/06/leetcode%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BB%83%E4%B9%A0/"},{"title":"leetcode链表练习","text":"链表题的tips: 常用方法： 递归、迭代、快慢指针 prev、cur指针结合题目的使用 如果头结点不确定，考虑是否需要dummy节点 1.反转链表206. Reverse Linked List (Easy) 递归法： 12345678910class Solution { public ListNode reverseList(ListNode head) { if(head == null || head.next == null){return head;} ListNode temp = head.next; ListNode newHead = reverseList(temp); temp.next = head; //反转 head.next = null; //避免出现环 return newHead; }} 迭代法： 1234567891011121314class Solution { public ListNode reverseList(ListNode head) { if(head == null || head.next == null) return head; ListNode cur = head; ListNode prev = null; while(cur!=null){ ListNode next = cur.next; cur.next = prev; //注意顺序 prev = cur; cur = next; } return prev; }} 2.反转链表中相邻结点24. Swap Nodes in Pairs (Medium) 1234567891011121314151617class Solution { public ListNode swapPairs(ListNode head) { ListNode dummy = new ListNode(-1); dummy.next = head; ListNode prev = dummy; ListNode cur = head; while(cur!=null &amp;&amp; cur.next!=null){ ListNode next = cur.next; cur.next = cur.next.next; //1--&gt;3 next.next = prev.next; //2--&gt;1 prev.next = next; //prev--&gt;2 prev = cur; cur = cur.next; } return dummy.next; }} 3.判断链表是否有环141. Linked List Cycle(Easy) 快慢指针法： 12345678910111213public class Solution { public boolean hasCycle(ListNode head) { if(head==null || head.next== null) return false; ListNode slow = head; ListNode fast = head; while(fast!=null &amp;&amp; fast.next!=null){ slow = slow.next; fast = fast.next.next; if(fast==slow) return true; } return false; }} 4.判断链表是否有环并输出环的起始位置142. Linked List Cycle II (Medium) To understand this solution, you just need to ask yourself these question.Assume the distance from head to the start of the loop is x1 the distance from the start of the loop to the point fast and slow meet is x2 the distance from the point fast and slow meet to the start of the loop is x3 What is the distance fast moved? What is the distance slow moved? And their relationship? x1 + x2 + x3 + x2x1 + x2x1 + x2 + x3 + x2 = 2 (x1 + x2)Thus x1 = x3 12345678910111213141516171819public class Solution { public ListNode detectCycle(ListNode head) { ListNode slow = head; ListNode fast = head; while(fast!=null &amp;&amp; fast.next!=null){ slow = slow.next; fast = fast.next.next; if(fast==slow){ ListNode slow2 = head; while(slow2!=slow){ slow = slow.next; slow2 = slow2.next; } return slow2; } } return null; }} 5.归并两个有序链表21. Merge Two Sorted Lists (Easy) 递归法： 12345678910111213class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if(l1 == null) return l2; if(l2 == null) return l1; if(l1.val &lt; l2.val){ l1.next = mergeTwoLists(l1.next, l2); return l1; }else{ l2.next = mergeTwoLists(l1, l2.next); return l2; } }} 迭代法： 12345678910111213141516171819class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(-1); ListNode cur = dummy; while(l1!=null &amp;&amp; l2!=null){ if(l1.val &lt; l2.val){ cur.next = l1; l1 = l1.next; }else{ cur.next = l2; l2 = l2.next; } cur = cur.next; } if(l1==null){cur.next = l2;} if(l2==null){cur.next = l1;} return dummy.next; }} 6.从有序链表中删除重复节点83. Remove Duplicates from Sorted List (Easy) 12345678910111213class Solution { public ListNode deleteDuplicates(ListNode head) { ListNode cur = head; while(cur!=null &amp;&amp; cur.next!=null){ if(cur.val == cur.next.val){ cur.next = cur.next.next; }else{ cur = cur.next; } } return head; }} 7.从有序链表中删除重复节点II82. Remove Duplicates from Sorted List II (Medium) 1234567891011121314151617181920class Solution { public ListNode deleteDuplicates(ListNode head) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode prev = dummy; ListNode cur = head; while(cur!=null){ while(cur.next!=null &amp;&amp; cur.val == cur.next.val){ cur = cur.next; } if(prev.next == cur){ prev = cur; }else{ prev.next = cur.next; } cur = cur.next; } return dummy.next; }} 8.删除链表的倒数第 n 个结点19. Remove Nth Node From End of List (Medium) 123456789101112131415161718class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode cur = head; int count = 0; while(cur!=null){ cur = cur.next; count++; } cur = dummy; for(int i=0;i&lt;count-n;i++){ cur = cur.next; } cur.next = cur.next.next; return dummy.next; }} 9.找出两个链表的交点160. Intersection of Two Linked Lists (Easy) a+c+b = a+b+c 1234567891011public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { ListNode curA = headA; ListNode curB = headB; while(curA != curB){ curA = curA==null?headB:curA.next; curB = curB==null?headA:curB.next; } return curA; }} 10.回文链表234. Palindrome Linked List (Easy) 12345678910111213141516171819202122232425262728293031class Solution { public boolean isPalindrome(ListNode head) { ListNode fast = head; ListNode slow = head; while(fast!=null &amp;&amp; fast.next!=null){ slow = slow.next; fast = fast.next.next; } ListNode slowFront = reverseList(slow); while(slowFront!=null){ if(head.val==slowFront.val){ head = head.next; slowFront = slowFront.next; }else{ return false; } } return true; } private ListNode reverseList(ListNode node){ ListNode prev = null; ListNode cur = node; while(cur!=null){ ListNode next = cur.next; cur.next = prev; prev = cur; cur = next; } return prev; }} 11.链表求和2. Add Two Numbers(Easy) 1234567891011121314151617181920212223class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(0); ListNode cur1 = l1; ListNode cur2 = l2; ListNode cur = dummy; int carry = 0; while (cur1 != null || cur2 != null) { int x = (cur1 != null) ? cur1.val : 0; int y = (cur2 != null) ? cur2.val : 0; int sum = carry + x + y; carry = sum / 10; cur.next = new ListNode(sum % 10); cur = cur.next; if (cur1 != null) cur1 = cur1.next; if (cur2 != null) cur2 = cur2.next; } if (carry &gt; 0) { cur.next = new ListNode(carry); } return dummy.next; }} 12.分隔链表725. Split Linked List in Parts(Medium) 123456789101112131415161718192021222324class Solution { public ListNode[] splitListToParts(ListNode root, int k) { int count = 0; ListNode cur = root; ListNode prev = null; ListNode[] parts = new ListNode[k]; while(cur != null){ count++; cur = cur.next; } int n = count/k; int r = count%k; cur = root; for(int i=0;i&lt;k &amp;&amp; cur!=null;i++,r--){ parts[i] = cur; for(int j=0;j&lt;n+(r&gt;0?1:0);j++){ prev = cur; cur = cur.next; } prev.next = null; } return parts; }} 13.链表元素按奇偶聚集328. Odd Even Linked List (Medium) 要求：空间复杂度O(1),时间复杂度O(n) 123456789101112131415161718192021222324/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Solution { public ListNode oddEvenList(ListNode head) { if(head==null){return null;} ListNode odd = head; ListNode evenHead = head.next; ListNode even = evenHead; while(even!=null &amp;&amp; even.next!=null){ odd.next = odd.next.next; even.next = even.next.next; odd = odd.next; even = even.next; } odd.next = evenHead; return head; }}","link":"/2020/05/11/leetcode%E9%93%BE%E8%A1%A8%E7%BB%83%E4%B9%A0/"},{"title":"leetcode双指针练习","text":"","link":"/2020/06/09/leetcode%E5%8F%8C%E6%8C%87%E9%92%88%E7%BB%83%E4%B9%A0/"},{"title":"leetcode位运算练习","text":"位运算练习tips: 通常如果比较简单的题目但是限制的要求比较多无法使用一些正常的方法时,或者题目中出现明显的位运算时，考虑使用位运算来解决。 比较常用的是异或^和n&amp;(n-1)以及mask运算来进行解决。 熟悉java中比价常用的api 0.位运算基本知识0s 表示一串 0，1s 表示一串 1。 123x ^ 0s = x x &amp; 0s = 0 x | 0s = xx ^ 1s = ~x x &amp; 1s = x x | 1s = 1sx ^ x = 0 x &amp; x = x x | x = x 利用 x ^ 1s = ~x 的特点，可以将位级表示翻转；利用 x ^ x = 0 的特点，可以将三个数中重复的两个数去除，只留下另一个数。 利用 x &amp; 0s = 0 和 x &amp; 1s = x 的特点，可以实现掩码操作。一个数 num 与 mask：00111100 进行位与操作，只保留 num 中与 mask 的 1 部分相对应的位。 利用 x | 0s = x 和 x | 1s = 1s 的特点，可以实现设值操作。一个数 num 与 mask：00111100 进行位或操作，将 num 中与 mask 的 1 部分相对应的位都设置为 1。 位与运算技巧 n&amp;(n-1) 去除 n 的位级表示中最低的那一位。例如对于二进制表示 10110100，减去 1 得到 10110011，这两个数相与得到 10110000。 n&amp;(-n) 得到 n 的位级表示中最低的那一位。-n 得到 n 的反码加 1，对于二进制表示 10110100，-n 得到 01001100，相与得到 00000100。 n-n&amp;(~n+1) 去除 n 的位级表示中最高的那一位。 移位运算 &gt;&gt; n 为算术右移，相当于除以 2n； &gt;&gt;&gt; n 为无符号右移，左边会补上 0。 &lt;&lt; n 为算术左移，相当于乘以 2n。 mask计算 要获取 111111111，将 0 取反即可，~0。 要得到只有第 i 位为 1 的 mask，将 1 向左移动 i-1 位即可，1&lt;&lt;(i-1) 。例如 1&lt;&lt;4 得到只有第 5 位为 1 的 mask ：00010000。 要得到 1 到 i 位为 1 的 mask，1&lt;&lt;(i+1)-1 即可，例如将 1&lt;&lt;(4+1)-1 = 00010000-1 = 00001111。 要得到 1 到 i 位为 0 的 mask，只需将 1 到 i 位为 1 的 mask 取反，即 ~(1&lt;&lt;(i+1)-1)。 Java 中的位操作 123static int Integer.bitCount(); // 统计 1 的数量static int Integer.highestOneBit(); // 获得最高位static String toBinaryString(int i); // 转换为二进制表示的字符串 1.统计两个数的二进制表示有多少位不同461.Hamming Distance (Easy) 12345678910111213class Solution { public int hammingDistance(int x, int y) { //对两个数异或,则不同的位结果为1 int n = x^y; int res = 0; //利用n&amp;(n-1)去掉最低位的1 while(n!=0){ n = n&amp;(n-1); res++; } return res; }} 也可以使用 Integer.bitcount() 来统计 1 个的个数 123public int hammingDistance(int x, int y) { return Integer.bitCount(x ^ y);} 2.统计从 0 ~ n 每个数的二进制表示中 1 的个数338.Counting Bits (Medium) 1234567891011121314151617class Solution { public int[] countBits(int num) { int[] res = new int[num+1]; for(int i=0;i&lt;=num;i++){ res[i] = count(i); } return res; } private int count(int i){ int sum = 0; while(i!=0){ i=i&amp;(i-1); sum++; } return sum; }} 3.数组中唯一一个不重复的元素136. Single Number (Easy) 首先第一个想到的是利用哈希表，但是题目要求我们不能使用额外空间并且线性时间复杂度，这时候可以考虑使用x^x=0进行求解。 123456789class Solution { public int singleNumber(int[] nums) { int res = 0; for(int num:nums){ res = res^num; } return res; }} 4.数组中不重复的两个元素260.Single Number III (Medium) 两个不相等的元素在位级表示上必定会有一位存在不同。 将数组的所有元素异或得到的结果为不存在重复的两个元素异或的结果。 diff &amp;= -diff 得到出 diff 最右侧不为 0 的位，也就是不存在重复的两个元素在位级表示上最右侧不同的那一位，利用这一位就可以将两个元素区分开来。 1234567891011121314class Solution { public int[] singleNumber(int[] nums) { int[] res = new int[2]; int diff = 0; for(int num:nums) diff ^= num; diff &amp;= - diff; //得到二进制中最右一位不为0的数 for(int num:nums){ //将剩下的数分成两组 if((num &amp; diff)== 0) res[0] ^= num; else res[1] ^= num; } return res; }} 5.找出数组中缺失的那个数268. Missing Number (Easy) 以0 4 3 1缺2为例(顺序无关) Index 0 1 2 3Value 0 1 3 4 missing =4∧(0∧0)∧(1∧1)∧(2∧3)∧(3∧4)=(4∧4)∧(0∧0)∧(1∧1)∧(3∧3)∧2=0∧0∧0∧0∧2=2 123456789class Solution { public int missingNumber(int[] nums) { int missing = nums.length; for(int i=0;i&lt;nums.length;i++){ missing = missing^i^nums[i]; } return missing; }} 6. 翻转一个数的比特位190. Reverse Bits (Easy) 123456789101112public class Solution { // you need treat n as an unsigned value public int reverseBits(int n) { int res = 0; for(int i=0;i&lt;32;i++){ res += n &amp; 1; n &gt;&gt;&gt;=1; if(i&lt;31) res &lt;&lt;=1; } return res; }} 如果该函数需要被调用很多次，可以将 int 拆成 4 个 byte，然后缓存 byte 对应的比特位翻转，最后再拼接起来。 1234567891011121314151617181920212223242526272829// cacheprivate final Map&lt;Byte, Integer&gt; cache = new HashMap&lt;Byte, Integer&gt;();public int reverseBits(int n) { byte[] bytes = new byte[4]; for (int i = 0; i &lt; 4; i++) // convert int into 4 bytes bytes[i] = (byte)((n &gt;&gt;&gt; 8*i) &amp; 0xFF); int result = 0; for (int i = 0; i &lt; 4; i++) { result += reverseByte(bytes[i]); // reverse per byte if (i &lt; 3) result &lt;&lt;= 8; } return result;}private int reverseByte(byte b) { Integer value = cache.get(b); // first look up from cache if (value != null) return value; value = 0; // reverse by bit for (int i = 0; i &lt; 8; i++) { value += ((b &gt;&gt;&gt; i) &amp; 1); if (i &lt; 7) value &lt;&lt;= 1; } cache.put(b, value); return value;} 7. 不用额外变量交换两个数的值程序员代码面试指南 123a = a ^ b;b = a ^ b;a = a ^ b; 8.判断一个数是不是2的n次方231. Power of Two (Easy) 2的整数次幂的二进制表示中只有1位是1，其他全是0 1234567class Solution { public boolean isPowerOfTwo(int n) { if(n&lt;=0) return false; //2的整数次幂的二进制中只有1位是1 return (n&amp;(n-1))==0; }} 9. 判断一个数是不是 4 的 n 次方342. Power of Four (Easy) 4的n次方的整数幂在二进制表示中有且只有一个奇数位为 1，例如 16（10000）。 123456class Solution { public boolean isPowerOfFour(int num) { if(num &lt;=0) return false; return (num&amp;(num-1))== 0 &amp;&amp; (num &amp; 0x55555555) != 0; //5表示0101 }} 10.判断一个数的位级表示是否不会出现连续的 0 和 1693. Binary Number with Alternating Bits (Easy) 对于 1010 这种位级表示的数，把它向右移动 1 位得到 101，这两个数每个位都不同，因此异或得到的结果为 1111。 1234public boolean hasAlternatingBits(int n) { int a = (n ^ (n &gt;&gt; 1)); return (a &amp; (a + 1)) == 0;} 11. 求一个数的补码476. Number Complement (Easy) 这里的补码按题目中的要求是将位中的1变0 0变1即可。并不是我们通常意义上说的补码 num = 00000101mask = 11111000~mask &amp; ~num = 00000010 1234567class Solution { public int findComplement(int num) { int mask = ~0; while ((num&amp;mask) != 0) mask &lt;&lt;= 1; return ~mask &amp; ~num; }} 12.实现整数的加法371. Sum of Two Integers (Easy) a ^ b 表示没有考虑进位的情况下两数的和，(a &amp; b) &lt;&lt; 1 就是进位。 递归会终止的原因是 (a &amp; b) &lt;&lt; 1 最右边会多一个 0，那么继续递归，进位最右边的 0 会慢慢增多，最后进位会变为 0，递归终止。 123public int getSum(int a, int b) { return b == 0 ? a : getSum((a ^ b), (a &amp; b) &lt;&lt; 1);}","link":"/2020/06/06/leetcode%E4%BD%8D%E8%BF%90%E7%AE%97%E7%BB%83%E4%B9%A0/"},{"title":"leetcode二分查找练习","text":"","link":"/2020/06/09/leetcode%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%BB%83%E4%B9%A0/"},{"title":"leetcode深搜、广搜和回溯练习","text":"深搜、广搜和回溯练习tips DFS用来求解可达性问题。使用栈(递归)+标记进行解决。 BFS用来解决最优解问题。使用队列+标记进行解决。 backtracking属于DFS,主要用来解决所有可能的解的问题。 深度优先搜索和广度优先搜索广泛运用于树和图中，但是它们的应用远远不止如此。 BFS0.原理广度优先搜索一层一层地进行遍历，每层遍历都是以上一层遍历的结果作为起点，遍历一个距离能访问到的所有节点。需要注意的是，遍历过的节点不能再次被遍历。 第一层： 0 -&gt; {6,2,1,5} 第二层： 6 -&gt; {4} 2 -&gt; {} 1 -&gt; {} 5 -&gt; {3} 第三层： 4 -&gt; {} 3 -&gt; {} 每一层遍历的节点都与根节点距离相同。设 di 表示第 i 个节点与根节点的距离，推导出一个结论：对于先遍历的节点 i 与后遍历的节点 j，有 di &lt;= dj。利用这个结论，可以求解最短路径等最优解问题：第一次遍历到目的节点，其所经过的路径为最短路径。 应该注意的是，从上面我们知道，使用 BFS 只能求解无权图的最短路径，无权图是指从一个节点到另一个节点的代价都记为 1。 在程序实现 BFS 时需要考虑以下问题： 队列：用来存储每一轮遍历得到的节点； 标记：对于遍历过的节点，应该将它标记，防止重复遍历。 1.计算在网格中从原点到特定点的最短路径长度1091. Shortest Path in Binary Matrix(Medium) 12345678910111213141516171819202122232425262728class Solution { public int shortestPathBinaryMatrix(int[][] grid) { int m = grid.length,n=grid[0].length; if(grid[0][0]==1 || grid[m-1][n-1]==1) return -1; int[][] direction = {{0,1},{0,-1},{1,0},{-1,0},{1,-1},{-1,1},{-1,-1},{1,1}}; boolean[][] visited = new boolean[m][n]; Queue&lt;int[]&gt; queue = new LinkedList&lt;&gt;(); queue.offer(new int[]{0,0}); int res = 0; while(!queue.isEmpty()){ int levNum = queue.size(); for(int i=0;i&lt;levNum;i++){ int[] node = queue.poll(); if(node[0]==m-1 &amp;&amp; node[1]==n-1) return res+1; for(int[] d:direction){ int nextX = node[0]+d[0]; int nextY = node[1]+d[1]; if(nextX&gt;=0 &amp;&amp; nextX&lt;m &amp;&amp; nextY&gt;=0 &amp;&amp; nextY&lt;n &amp;&amp; !visited[nextX][nextY] &amp;&amp; grid[nextX][nextY]==0){ queue.offer(new int[]{nextX,nextY}); visited[nextX][nextY] = true; } } } res++; } return -1; }} DFS0.原理广度优先搜索一层一层遍历，每一层得到的所有新节点，要用队列存储起来以备下一层遍历的时候再遍历。而深度优先搜索在得到一个新节点时立即对新节点进行遍历 从节点 0 出发开始遍历，得到到新节点 6 时，立马对新节点 6 进行遍历，得到新节点 4；如此反复以这种方式遍历新节点，直到没有新节点了，此时返回。返回到根节点 0 的情况是，继续对根节点 0 进行遍历，得到新节点 2，然后继续以上步骤。 从一个节点出发，使用 DFS 对一个图进行遍历时，能够遍历到的节点都是从初始节点可达的，DFS 常用来求解这种可达性问题。 在程序实现 DFS 时需要考虑以下问题： 栈：用栈来保存当前节点信息，当遍历新节点返回时能够继续遍历当前节点。可以使用递归栈。 标记：和 BFS 一样同样需要对已经遍历过的节点进行标记。 1.查找最大的连通面积695. Max Area of Island (Medium) 深搜： 1234567891011121314151617181920212223242526272829class Solution { private int m,n; private boolean[][] visited; private int[][] direction = {{1,0},{-1,0},{0,1},{0,-1}}; public int maxAreaOfIsland(int[][] grid) { if(grid==null || grid.length==0) return 0; m = grid.length; n = grid[0].length; visited = new boolean[m][n]; int res = 0; for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ res = Math.max(res,dfs(grid,i,j)); } } return res; } private int dfs(int[][] grid,int i,int j){ if(i&lt;0 || i&gt;=m || j&lt;0 || j&gt;=n || visited[i][j] || grid[i][j]==0) return 0; //1.标记为已访问 visited[i][j] = true; int area = 1; //递归的访问它没有标记过的邻居节点 for(int[] d:direction){ area += dfs(grid,i+d[0],j+d[1]); } return area; }} 当然这个题目也可以用广搜解决 1234567891011121314151617181920212223242526272829303132333435363738class Solution { private int m,n; private boolean[][] visited; private int[][] direction = new int[][]{{1,0},{-1,0},{0,1},{0,-1}}; public int maxAreaOfIsland(int[][] grid) { if(grid==null || grid.length==0 || grid[0].length==0) return 0; m = grid.length; n=grid[0].length; visited = new boolean[m][n]; int res = 0; for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(grid[i][j]==1 &amp;&amp; !visited[i][j]){ res = Math.max(res,bfs(grid,i,j)); } } } return res; } private int bfs(int[][] grid,int i,int j){ Queue&lt;int[]&gt; queue = new LinkedList&lt;&gt;(); queue.offer(new int[]{i,j}); visited[i][j] = true; int res = 0; while(!queue.isEmpty()){ int[] node = queue.poll(); res++; for(int[] d:direction){ int X = node[0]+d[0]; int Y = node[1]+d[1]; if(X&lt;0 || X&gt;=m || Y&lt;0 || Y&gt;=n || visited[X][Y] || grid[X][Y] == 0) continue; queue.offer(new int[]{X,Y}); visited[X][Y] = true; } } return res; }} 2.矩阵中的连通分量数目200. Number of Islands (Medium) 12345678910111213141516171819202122232425262728class Solution { private int m,n; private boolean[][] visited; private int[][] direction = {{1,0},{-1,0},{0,1},{0,-1}}; public int numIslands(char[][] grid) { if(grid==null || grid.length==0) return 0; m = grid.length; n = grid[0].length; visited = new boolean[m][n]; int count = 0; for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(!visited[i][j] &amp;&amp; grid[i][j]=='1'){ dfs(grid,i,j); count++; } } } return count; } private void dfs(char[][] grid,int i,int j){ if(i&lt;0 || i&gt;=m || j&lt;0 || j&gt;=n || visited[i][j] || grid[i][j] == '0') return; visited[i][j] = true; for(int[] d:direction){ dfs(grid,i+d[0],j+d[1]); } }} 同样也可以用广搜解决 123456789101112131415161718192021222324252627282930313233343536class Solution { private int m,n; private boolean[][] visited; private int[][] direction = new int[][]{{1,0},{-1,0},{0,1},{0,-1}}; public int numIslands(char[][] grid) { if(grid==null || grid.length==0 || grid[0].length==0) return 0; m = grid.length; n = grid[0].length; visited = new boolean[m][n]; int res = 0; for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(!visited[i][j] &amp;&amp; grid[i][j]=='1'){ res++; bfs(grid,i,j); } } } return res; } private void bfs(char[][] grid,int i,int j){ Queue&lt;int[]&gt; queue = new LinkedList&lt;&gt;(); queue.offer(new int[]{i,j}); visited[i][j] = true; while(!queue.isEmpty()){ int[] node = queue.poll(); for(int[] d:direction){ int x = node[0]+d[0]; int y = node[1]+d[1]; if(x&lt;0 || x&gt;=m || y&lt;0 || y&gt;=n || visited[x][y] || grid[x][y]=='0') continue; queue.offer(new int[]{x,y}); visited[x][y] = true; } } }} 3.好友关系的连通分量数目547. Friend Circles (Medium) 12345678910111213141516171819202122232425class Solution { private int n; private boolean[] visited; public int findCircleNum(int[][] M) { if(M==null || M.length==0) return 0; n = M.length; visited = new boolean[n]; int res = 0; for(int i=0;i&lt;n;i++){ if(!visited[i]){ dfs(M,i); res++; } } return res; } private void dfs(int[][] M,int i){ visited[i] = true; for(int j=0;j&lt;n;j++){ if(M[i][j] == 1 &amp;&amp; !visited[j]){ dfs(M,j); } } }} 4.填充封闭区域130. Surrounded Regions (Medium) 1234567891011121314151617181920212223242526272829303132333435363738class Solution { private int m, n; private int[][] direction = {{0, 1}, {0, -1}, {1, 0}, {-1, 0}}; public void solve(char[][] board) { if (board == null || board.length == 0) return; m = board.length; n = board[0].length; //边界为O的一定不会变为X //找到所有边界为O的联通区域标记为T for (int i = 0; i &lt; m; i++) { dfs(board, i, 0); dfs(board, i, n - 1); } for (int i = 0; i &lt; n; i++) { dfs(board, 0, i); dfs(board, m - 1, i); } //内层的标记为X for (int i = 0; i &lt; m; i++) { for (int j = 0; j &lt; n; j++) { if (board[i][j] == 'T') { board[i][j] = 'O'; }else if (board[i][j] == 'O') { board[i][j] = 'X'; } } } } private void dfs(char[][] board, int i, int j) { if (i &lt; 0 || i &gt;= m || j &lt; 0 || j &gt;= n || board[i][j] != 'O') return; board[i][j] = 'T'; for (int[] d : direction) { dfs(board, i + d[0], j + d[1]); } }} 5.能到达的太平洋和大西洋的区域417. Pacific Atlantic Water Flow (Medium) 12345678910111213141516171819202122232425262728293031323334353637class Solution { private int m,n; private boolean[][] p; private boolean[][] a; private int[][] direction = {{1,0},{-1,0},{0,1},{0,-1}}; public List&lt;List&lt;Integer&gt;&gt; pacificAtlantic(int[][] matrix) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if(matrix==null || matrix.length==0) return res; m = matrix.length; n = matrix[0].length; p = new boolean[m][n]; a = new boolean[m][n]; for(int i=0;i&lt;m;i++){ dfs(matrix,i,0,0,p); dfs(matrix,i,n-1,0,a); } for(int j=0;j&lt;n;j++){ dfs(matrix,0,j,0,p); dfs(matrix,m-1,j,0,a); } for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(p[i][j] &amp;&amp; a[i][j]){ res.add(Arrays.asList(i,j)); } } } return res; } private void dfs(int[][] matrix,int i,int j,int height,boolean[][] visited){ if(i&lt;0||i&gt;=m||j&lt;0||j&gt;=n||visited[i][j]||matrix[i][j]&lt;height) return; visited[i][j] = true; for(int[] d:direction){ dfs(matrix,i+d[0],j+d[1],matrix[i][j],visited); } }} backtracking0.原理Backtracking（回溯）属于 DFS。 普通 DFS 主要用在可达性问题 ，这种问题只需要执行到特点的位置然后返回即可。 而 Backtracking 主要用于求解排列组合问题，例如有 { ‘a’,’b’,’c’ } 三个字符，求解所有由这三个字符排列得到的字符串，这种问题在执行到特定的位置返回之后还会继续执行求解过程。 因为 Backtracking 不是立即返回，而要继续求解，因此在程序实现时，需要注意对元素的标记问题： 在访问一个新元素进入新的递归调用时，需要将新元素标记为已经访问，这样才能在继续递归调用时不用重复访问该元素； 但是在递归返回时，需要将元素标记为未访问，因为只需要保证在一个递归链中不同时访问一个元素，可以访问已经访问过但是不在当前递归链中的元素。 1.","link":"/2020/06/09/leetcode%E6%B7%B1%E6%90%9C%E5%B9%BF%E6%90%9C%E5%92%8C%E5%9B%9E%E6%BA%AF%E7%BB%83%E4%B9%A0/"},{"title":"leetcode贪心思想练习","text":"贪心思想练习tips 贪心思想在对问题进行求解时，总是做出在当前看来是最好的选择。所以适用贪心算法的场景是子问题的最优解能够递推得到最终问题的最优解。 1.分配饼干 给一个孩子的饼干应当尽量小并且又能满足该孩子，这样大饼干才能拿来给满足度比较大的孩子。 因为满足度最小的孩子最容易得到满足，所以先满足满足度最小的孩子。 455. Assign Cookies (Easy) 12345678910111213141516class Solution { public int findContentChildren(int[] g, int[] s) { Arrays.sort(g); Arrays.sort(s); int res = 0; int i=0,j=0; while(i&lt;s.length &amp;&amp; j&lt;g.length){ if(s[i]&gt;=g[j]){ i++; j++; res++; }else i++; } return res; }} 2.不重叠的区间个数435. Non-overlapping Intervals (Medium) 先计算最多能组成的不重叠区间个数，然后用区间总个数减去不重叠区间的个数。 在每次选择不重叠区间时，区间的结尾最为重要，选择的区间结尾越小，留给后面的区间的空间越大，那么后面能够选择的区间个数也就越大，那么需要删除的区间个数也就最小。 按区间的结尾进行排序，每次选择结尾最小，并且和前一个区间不重叠的区间。 123456789101112131415161718class Solution { public int eraseOverlapIntervals(int[][] intervals) { if(intervals.length==0) return 0; Arrays.sort(intervals,new Comparator&lt;int[]&gt;(){ public int compare(int[] o1,int[]o2){ return o1[1]-o2[1]; } }); int count = 1; int end = intervals[0][1]; for(int i=1;i&lt;intervals.length;i++){ if(intervals[i][0]&lt;end) continue; end = intervals[i][1]; count++; } return intervals.length-count; }} 3. 投飞镖刺破气球452. Minimum Number of Arrows to Burst Balloons (Medium)也是计算不重叠的区间个数，不过和上面不重叠区间的区别在于，[1, 2] 和 [2, 3] 在本题中算是重叠区间。 1234567891011121314151617class Solution { public int findMinArrowShots(int[][] points) { if(points.length==0) return 0; Arrays.sort(points,new Comparator&lt;int[]&gt;(){ public int compare(int[] o1,int[]o2){ return o1[1]-o2[1]; } }); int count = 1,end = points[0][1]; for(int i=1;i&lt;points.length;i++){ if(points[i][0]&lt;=end) continue; end = points[i][1]; count++; } return count; }} 4.根据身高和序号重组队列406. Queue Reconstruction by Height(Medium) 为了使插入操作不影响后续的操作，身高较高的学生应该先做插入操作，否则身高较小的学生原先正确插入的第 k 个位置可能会变成第 k+1 个位置。 身高 h 降序、个数 k 值升序，然后将某个学生插入队列的第 k 个位置中。 1234567891011121314class Solution { public int[][] reconstructQueue(int[][] people) { if (people == null || people.length == 0 || people[0].length == 0) { return new int[0][0]; } //身高升序 个数降序 Arrays.sort(people, (a,b) -&gt; (a[0]==b[0] ? a[1]-b[1] : b[0]-a[0])); List&lt;int[]&gt; list = new ArrayList&lt;&gt;(); for(int[] p:people){ list.add(p[1],p); } return list.toArray(new int[list.size()][]); }} 5.买卖股票最大的收益121. Best Time to Buy and Sell Stock (Easy) 123456789101112class Solution { public int maxProfit(int[] prices) { if(prices.length==0) return 0; int min = prices[0]; int res = 0; for(int p:prices){ if(p&lt;min) min = p; else res = Math.max(res,p-min); } return res; }} 6.买卖股票的最大收益 II122. Best Time to Buy and Sell Stock II (Easy) 对于 [a, b, c, d]，如果有 a &lt;= b &lt;= c &lt;= d ，那么最大收益为 d - a。而 d - a = (d - c) + (c - b) + (b - a) ，因此当访问到一个 prices[i] 且 prices[i] - prices[i-1] &gt; 0，那么就把 prices[i] - prices[i-1] 添加到收益中。 123456789class Solution { public int maxProfit(int[] prices) { int res = 0; for(int i=1;i&lt;prices.length;i++){ if(prices[i]&gt;prices[i-1]) res+=prices[i]-prices[i-1]; } return res; }} 7. 种植花朵605. Can Place Flowers (Easy) 123456789101112131415class Solution { public boolean canPlaceFlowers(int[] flowerbed, int n) { int count = 0; for(int i=0;i&lt;flowerbed.length;i++){ if(flowerbed[i]==1) continue; int pre = i==0 ? 0 : flowerbed[i-1]; //开始位置的前面记为0 int next = i==flowerbed.length-1 ? 0 : flowerbed[i+1]; //结束位置的后面也记为0 if(pre==0 &amp;&amp; next==0){ count++; flowerbed[i]==1; } } return count&gt;=n; }} 8.修改一个数成为非递减数组665. Non-decreasing Array (Easy) 在出现 nums[i] &lt; nums[i - 1] 时，需要考虑的是应该修改数组的哪个数，使得本次修改能使 i 之前的数组成为非递减数组，并且 不影响后续的操作 。优先考虑令 nums[i - 1] = nums[i]，因为如果修改 nums[i] = nums[i - 1] 的话，那么 nums[i] 这个数会变大，就有可能比 nums[i + 1] 大，从而影响了后续操作。还有一个比较特别的情况就是 nums[i] &lt; nums[i - 2]，修改 nums[i - 1] = nums[i] 不能使数组成为非递减数组，只能修改 nums[i] = nums[i - 1]。 12345678910111213class Solution { public boolean checkPossibility(int[] nums) { int count = 0; for(int i=1;i&lt;nums.length;i++){ if(nums[i]&lt;nums[i-1]){ count++; if(i-2&gt;=0 &amp;&amp; nums[i]&lt;nums[i-2]) nums[i] = nums[i-1]; else nums[i-1] = nums[i]; } } return count&lt;=1; }}","link":"/2020/06/09/leetcode%E8%B4%AA%E5%BF%83%E6%80%9D%E6%83%B3%E7%BB%83%E4%B9%A0/"},{"title":"leetcode哈希表练习","text":"哈希表相关tips:1.哈希表主要作用有两个：(1)去重 (2)检索时间复杂度O(1)2.需要存储key的次数或者索引时使用HashMap 1.两数之和1. Two Sum (Easy) 1234567891011121314class Solution { public int[] twoSum(int[] nums, int target) { int[] res = new int[2]; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int i=0;i&lt;nums.length;i++){ int temp = target-nums[i]; if(map.containsKey(temp)){ res[0] = map.get(temp); res[1] = i; }else map.put(nums[i],i); } return res; }} 2.判断数组是否含有重复元素217. Contains Duplicate (Easy) 12345678910class Solution { public boolean containsDuplicate(int[] nums) { Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); for(int num: nums){ if(set.contains(num)) return true; set.add(num); } return false; }} 3.判断数组是否含有重复元素II219. Contains Duplicate II 12345678910class Solution { public boolean containsNearbyDuplicate(int[] nums, int k) { Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int i=0;i&lt;nums.length;i++){ if(map.containsKey(nums[i]) &amp;&amp; Math.abs(i-map.get(nums[i]))&lt;=k) return true; map.put(nums[i],i); } return false; }} 4.有效的字母异位词242. Valid Anagram (Easy) 12345678910class Solution { public boolean isAnagram(String s, String t) { //hashset不能解决ab、a问题 int[] alphabet = new int[26]; for (int i = 0; i &lt; s.length(); i++) alphabet[s.charAt(i) - 'a']++; for (int i = 0; i &lt; t.length(); i++) alphabet[t.charAt(i) - 'a']--; for (int i : alphabet) if (i != 0) return false; return true; }} 5.最长和谐序列594. Longest Harmonious Subsequence (Easy) 123456789101112131415class Solution { public int findLHS(int[] nums) { int res = 0; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int num:nums){ map.put(num,map.getOrDefault(num,0)+1); } for(int num:map.keySet()){ if(map.containsKey(num+1)){ res = Math.max(res,map.get(num)+map.get(num+1)); } } return res; }} 6.最长连续序列128. Longest Consecutive Sequence (Hard) 123456789101112131415161718192021class Solution { public int longestConsecutive(int[] nums) { int res = 0; Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); for(int num:nums){ set.add(num); } for(int num:set){ //从之前不连续的第一个开始才可能是最长序列 if(!set.contains(num-1)){ int count = 1; while(set.contains(num+1)){ count++; num++; } res = Math.max(res,count); } } return res; }}","link":"/2020/05/16/leetcode%E5%93%88%E5%B8%8C%E8%A1%A8%E7%BB%83%E4%B9%A0/"},{"title":"leetcode动态规划练习","text":"动态规划tips: 动态规划三要素： 状态定义+初始条件+状态转移方程 常见的动态规划问题：斐波那契、矩阵路径、最长递增子序列和最长公共子序列、0-1背包问题 斐波那契数列1.斐波那契数列509. Fibonacci Number(Easy) 递归法 时间复杂度O(2^n) 空间复杂度O(n) 123456class Solution { public int fib(int N) { if(N&lt;=1) return N; return fib(N-1)+fib(N-2); }} 简单dp 时间复杂度O(n) 空间复杂度O(n) 123456789101112class Solution { public int fib(int N) { if(N&lt;=1) return N; int[] dp = new int[N+1]; dp[0] = 0; dp[1] = 1; for(int i=2;i&lt;=N;i++){ dp[i] = dp[i-1]+dp[i-2]; } return dp[N]; }} 考虑到 dp[i] 只与 dp[i - 1] 和 dp[i - 2] 有关，因此可以只用两个变量来存储 dp[i - 1] 和 dp[i - 2]，使得原来的 O(N) 空间复杂度优化为 O(1) 复杂度。 123456789101112class Solution { public int fib(int N) { if(N&lt;=1) return N; int pre2=0,pre1=1,cur=0; for(int i=2;i&lt;=N;i++){ cur = pre1+pre2; pre2 = pre1; pre1 = cur; } return pre1; }} 2.爬楼梯70. Climbing Stairs (Easy) 和斐波那契数列相同 状态定义 dp[i]:爬到第i级台阶的总方式递推公式 dp[i] = dp[i-1] + dp[i-2] 同样这里dp[i] 只与 dp[i-1]和dp[i-2]，有关为了降低空间复杂度使用两个变量代替 123456789101112class Solution { public int climbStairs(int n) { if(n&lt;=2) return n; int pre2 = 1,pre1 = 2,cur=0; for(int i=3;i&lt;=n;i++){ cur = pre1+pre2; pre2 = pre1; pre1 = cur; } return pre1; }} 3.强盗抢劫198. House Robber (Easy) 状态定义 dp[i]:抢劫前i间房屋的最大价值转移方程 dp[i]= Math.max(dp[i-2]+nums[i],dp[i-1])(抢第i间房子 和 不抢第i间房子 的最大值) 同样这里使用两个变量简化空间 1234567891011class Solution { public int rob(int[] nums) { int pre2=0,pre1=0,cur=0; for(int i=0;i&lt;nums.length;i++){ cur = Math.max(pre2+nums[i],pre1); pre2 = pre1; pre1 = cur; } return pre1; }} 4.强盗在环形街区抢劫213. House Robber II(Medium) 上面问题的升级，我们可以把这个问题分成两个子问题，分别寻找(0,n-2)和(1,n - 1)范围的最大值，并比较最终的最大值即可 1234567891011121314151617class Solution { public int rob(int[] nums) { if(nums==null || nums.length==0) return 0; int n = nums.length; if(n==1) return nums[0]; return Math.max(rob(nums,0,n-2),rob(nums,1,n-1)); } private int rob(int[] nums,int lo,int hi){ int pre2=0,pre1=0,cur=0; for(int i=lo;i&lt;=hi;i++){ cur = Math.max(pre2+nums[i],pre1); pre2 = pre1; pre1 = cur; } return pre1; }} 5.信件错排题目描述：有 N 个 信 和 信封，它们被打乱，求错误装信方式的数量。 状态定义 dp[i]表示前i个信和信封的错误方式数量。 假设第 i 个信装到第 j 个信封里面，而第 j 个信装到第 k 个信封里面。根据 i 和 k 是否相等，有两种情况： i==k，交换 i 和 j 的信后，它们的信和信封在正确的位置，但是其余 i-2 封信有 dp[i-2] 种错误装信的方式。由于 j 有 i-1 种取值，因此共有 (i-1)*dp[i-2] 种错误装信方式。 i != k，交换 i 和 j 的信后，第 i 个信和信封在正确的位置，其余 i-1 封信有 dp[i-1] 种错误装信方式。由于 j 有 i-1 种取值，因此共有 (i-1)*dp[i-1] 种错误装信方式。 综上所述，错误装信数量方式数量即状态转移方程为： dp[i] = (i-1)*dp[i-2] + (i-1)*dp[i-1] 6.母牛生产程序员代码面试指南-P181 题目描述：假设农场中成熟的母牛每年都会生 1 头小母牛，并且永远不会死。第一年有 1 只小母牛，从第二年开始，母牛开始生小母牛。每只小母牛 3 年之后成熟又可以生小母牛。给定整数 N，求 N 年后牛的数量。 第 i 年成熟的牛的数量为：dp[i] = dp[i-1] + dp[i-3] 矩阵路径1.矩阵的最小路径和64. Minimum Path Sum (Medium) 典型的dp问题 状态定义 dp[i][j] 到达坐标(i,j)时的路径和转移方程 dp[i][j] = Math.min(dp[i-1][j],dp[i][j-1])+grid[i][j] (即从左侧走过来 和 从上面走下来) 另外对于边界节点，如最左侧j=0时只能从上往下走 对于最上层i=0只能从左往右走进行处理即可 1234567891011121314151617class Solution { public int minPathSum(int[][] grid) { if(grid==null || grid.length==0 || grid[0].length==0) return 0; int m = grid.length; int n = grid[0].length; int[][] dp = new int[m][n]; dp[0][0] = grid[0][0]; for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(i==0 &amp;&amp; j!=0) dp[i][j] = dp[i][j-1] + grid[i][j]; if(i!=0 &amp;&amp; j==0) dp[i][j] = dp[i-1][j] + grid[i][j]; if(i!=0 &amp;&amp; j!=0) dp[i][j] = Math.min(dp[i-1][j],dp[i][j-1])+grid[i][j]; } } return dp[m-1][n-1]; }} 因为我们不需要每一层都需要进行记忆，因此可以把空间复杂度降低为O(n) 1234567891011121314151617class Solution { public int minPathSum(int[][] grid) { if(grid==null || grid.length==0 || grid[0].length==0) return 0; int m = grid.length; int n = grid[0].length; int[] dp = new int[n]; dp[0] = grid[0][0]; for(int i=0;i&lt;m;i++){ for(int j=0;j&lt;n;j++){ if(i==0 &amp;&amp; j!=0) dp[j] = dp[j-1] + grid[i][j]; if(i!=0 &amp;&amp; j==0) dp[j] = dp[j] + grid[i][j]; if(i!=0 &amp;&amp; j!=0) dp[j] = Math.min(dp[j],dp[j-1])+grid[i][j]; } } return dp[n-1]; }} 2.矩阵的总路径数62. Unique Paths (Medium) 状态定义 dp[i][j] 到达（i,j）节点的路径数转移方程 dp[i][j] = dp[i-1][j] + dp[i][j-1] 同样对于边界处理有dp[0][j]=dp[i][0]=1 同样可以可以把空间复杂度降低为O(n) 123456789101112class Solution { public int uniquePaths(int m, int n) { int[] dp = new int[n]; Arrays.fill(dp,1); for(int i=1;i&lt;m;i++){ for(int j=1;j&lt;n;j++){ dp[j] = dp[j] + dp[j-1]; } } return dp[n-1]; }} 最长递增子序列已知一个序列 {S1, S2,…,Sn}，取出若干数组成新的序列 {Si1, Si2,…, Sim}，其中 i1、i2 … im 保持递增，即新序列中各个数仍然保持原数列中的先后顺序，称新序列为原序列的一个子序列。 如果在子序列中，当下标 ix &gt; iy 时，Six &gt; Siy，称子序列为原序列的一个递增子序列 。 定义一个数组 dp 存储最长递增子序列的长度，dp[n] 表示以 Sn 结尾的序列的最长递增子序列长度。对于一个递增子序列 {Si1, Si2,…,Sim}，如果 im &lt; n 并且 Sim &lt; Sn，此时 {Si1, Si2,…, Sim, Sn} 为一个递增子序列，递增子序列的长度增加 1。满足上述条件的递增子序列中，长度最长的那个递增子序列就是要找的，在长度最长的递增子序列上加上 Sn 就构成了以 Sn 为结尾的最长递增子序列。因此dp[n] = max{ dp[i]+1 | Si &lt; Sn &amp;&amp; i &lt; n}。 因为在求 dp[n] 时可能无法找到一个满足条件的递增子序列，此时 {Sn} 就构成了递增子序列，需要对前面的求解方程做修改，令 dp[n] 最小为 1，即： 1dp[n] = max{1, dp[i]+1 | Si &lt; Sn &amp;&amp; i &lt; n} 对于一个长度为 N 的序列，最长递增子序列并不一定会以 SN 为结尾，因此 dp[N] 不是序列的最长递增子序列的长度，需要遍历 dp 数组找出最大值才是所要的结果，max{ dp[i] | 1 &lt;= i &lt;= N} 即为所求。 1.最长递增子序列300. Longest Increasing Subsequence (Medium) 状态定义 dp[i] 到元素i的最长递增子序列的数转移方程 dp[i] = Math.max(dp[i],dp[j]+1) （因为不一定就是i-1，所以是dp[j]） 12345678910111213141516class Solution { public int lengthOfLIS(int[] nums) { if(nums==null || nums.length==0) return 0; int n = nums.length; int[]dp = new int[n]; Arrays.fill(dp,1); int res = 1; for(int i=1;i&lt;n;i++){ for(int j=0;j&lt;i;j++){ if(nums[i]&gt;nums[j]) dp[i] = Math.max(dp[i],dp[j]+1); } res = Math.max(res,dp[i]); } return res; }} 2. 一组整数对能够构成的最长链646. Maximum Length of Pair Chain (Medium) 1234567891011121314151617class Solution { public int findLongestChain(int[][] pairs) { if(pairs==null || pairs.length==0 || pairs[0].length==0) return 0; Arrays.sort(pairs,(a,b)-&gt;(a[0]-b[0])); int n = pairs.length; int[] dp = new int[n]; Arrays.fill(dp,1); int res = 1; for(int i=1;i&lt;n;i++){ for(int j=0;j&lt;i;j++){ if(pairs[j][1]&lt;pairs[i][0]) dp[i] = Math.max(dp[i],dp[j]+1); } res = Math.max(res,dp[i]); } return res; }} 3.最长摆动子序列376. Wiggle Subsequence (Medium) 12345678910111213141516171819202122class Solution { public int wiggleMaxLength(int[] nums) { int n = nums.length; if(nums.length &lt;2) return n; int[] up = new int[n]; int[] down = new int[n]; up[0] = down[0] = 1; for(int i=1;i&lt;n;i++){ if(nums[i]&gt;nums[i-1]){ up[i] = down[i-1]+1; down[i] = down[i-1]; }else if(nums[i]&lt;nums[i-1]){ down[i] = up[i-1]+1; up[i] = up[i-1]; }else { down[i] = down[i-1]; up[i] = up[i-1]; } } return Math.max(down[n-1],up[n-1]); }} 由于只用到了前面一个元素，所以可以降低空间复杂度为O(1) 123456789101112class Solution { public int wiggleMaxLength(int[] nums) { int n = nums.length; if(nums.length &lt;2) return n; int up=1,down=1; for(int i=1;i&lt;nums.length;i++){ if(nums[i]&gt;nums[i-1]) up = down+1; else if(nums[i]&lt;nums[i-1]) down = up+1; } return Math.max(down,up); }} 最长公共子序列对于两个子序列 S1 和 S2，找出它们最长的公共子序列。 定义一个二维数组 dp 用来存储最长公共子序列的长度，其中 dp[i][j] 表示 S1 的前 i 个字符与 S2 的前 j 个字符最长公共子序列的长度。考虑 S1i 与 S2j 值是否相等，分为两种情况： 当 S1i==S2j 时，那么就能在 S1 的前 i-1 个字符与 S2 的前 j-1 个字符最长公共子序列的基础上再加上 S1i 这个值，最长公共子序列长度加 1，即 dp[i][j] = dp[i-1][j-1] + 1。 当 S1i != S2j 时，此时最长公共子序列为 S1 的前 i-1 个字符和 S2 的前 j 个字符最长公共子序列，或者 S1 的前 i 个字符和 S2 的前 j-1 个字符最长公共子序列，取它们的最大者，即 dp[i][j] = max{ dp[i-1][j], dp[i][j-1] }。综上，最长公共子序列的状态转移方程为： 对于长度为 N 的序列 S1 和长度为 M 的序列 S2，dp[N][M] 就是序列 S1 和序列 S2 的最长公共子序列长度。 与最长递增子序列相比，最长公共子序列有以下不同点： 针对的是两个序列，求它们的最长公共子序列。 在最长递增子序列中，dp[i] 表示以 Si 为结尾的最长递增子序列长度，子序列必须包含 Si ；在最长公共子序列中，dp[i][j] 表示 S1 中前 i 个字符与 S2 中前 j 个字符的最长公共子序列长度，不一定包含 S1i 和 S2j。 在求最终解时，最长公共子序列中 dp[N][M] 就是最终解，而最长递增子序列中 dp[N] 不是最终解，因为以 SN 为结尾的最长递增子序列不一定是整个序列最长递增子序列，需要遍历一遍 dp 数组找到最大者。 最长公共子序列1143. Longest Common Subsequence 12345678910111213141516class Solution { public int longestCommonSubsequence(String text1, String text2) { int n1 = text1.length(),n2 = text2.length(); int[][] dp = new int[n1+1][n2+1]; for(int i=1;i&lt;=n1;i++){ for(int j=1;j&lt;=n2;j++){ if(text1.charAt(i-1)==text2.charAt(j-1)){ dp[i][j] = dp[i-1][j-1]+1; }else { dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]); } } } return dp[n1][n2]; }} 0-1背包有一个容量为 N 的背包，要用这个背包装下物品的价值最大，这些物品有两个属性：体积 w 和价值 v。 定义一个二维数组 dp 存储最大价值，其中 dp[i][j] 表示前 i 件物品体积不超过 j 的情况下能达到的最大价值。设第 i 件物品体积为 w，价值为 v，根据第 i 件物品是否添加到背包中，可以分两种情况讨论： 第 i 件物品没添加到背包，总体积不超过 j 的前 i 件物品的最大价值就是总体积不超过 j 的前 i-1 件物品的最大价值，dp[i][j] = dp[i-1][j]。 第 i 件物品添加到背包中，dp[i][j] = dp[i-1][j-w] + v。 第 i 件物品可添加也可以不添加，取决于哪种情况下最大价值更大。因此，0-1 背包的状态转移方程为： 1dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-w]+v) 123456789101112131415161718// W 为背包总体积// N 为物品数量// weights 数组存储 N 个物品的重量// values 数组存储 N 个物品的价值public int knapsack(int W, int N, int[] weights, int[] values) { int[][] dp = new int[N + 1][W + 1]; for (int i = 1; i &lt;= N; i++) { int w = weights[i - 1], v = values[i - 1]; for (int j = 1; j &lt;= W; j++) { if (j &gt;= w) { dp[i][j] = Math.max(dp[i - 1][j], dp[i - 1][j - w] + v); } else { dp[i][j] = dp[i - 1][j]; } } } return dp[N][W];} 空间优化 在程序实现时可以对 0-1 背包做优化。观察状态转移方程可以知道，前 i 件物品的状态仅与前 i-1 件物品的状态有关，因此可以将 dp 定义为一维数组，其中 dp[j] 既可以表示 dp[i-1][j] 也可以表示 dp[i][j]。此时， 1dp[j] = Math.max(dp[j],dp[j-w]+v) 因为 dp[j-w] 表示 dp[i-1][j-w]，因此不能先求 dp[i][j-w]，防止将 dp[i-1][j-w] 覆盖。也就是说要先计算 dp[i][j] 再计算 dp[i][j-w]，在程序实现时需要按倒序来循环求解。 123456789101112public int knapsack(int W, int N, int[] weights, int[] values) { int[] dp = new int[W + 1]; for (int i = 1; i &lt;= N; i++) { int w = weights[i - 1], v = values[i - 1]; for (int j = W; j &gt;= 1; j--) { if (j &gt;= w) { dp[j] = Math.max(dp[j], dp[j - w] + v); } } } return dp[W];} 无法使用贪心算法的解释0-1 背包问题无法使用贪心算法来求解，也就是说不能按照先添加性价比最高的物品来达到最优，这是因为这种方式可能造成背包空间的浪费，从而无法达到最优。考虑下面的物品和一个容量为 5 的背包，如果先添加物品 0 再添加物品 1，那么只能存放的价值为 16，浪费了大小为 2 的空间。最优的方式是存放物品 1 和物品 2，价值为 22. id|w|v|v/w|:-:|:-:|:-:|:-:|0|1|6|6|1|2|10|5|2|3|12|4 变种完全背包：物品数量为无限个多重背包：物品数量有限制多维费用背包：物品不仅有重量，还有体积，同时考虑这两种限制其它：物品之间相互约束或者依赖 1.划分数组为和相等的两部分416. Partition Equal Subset Sum (Medium) 这道题可以转换为一个背包大小为sum/2的0-1背包问题 状态定义： dp[i][j] 前i个元素能否达到和为j转移方程 dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i-1]] 不拿当前元素i 和 拿当前元素i 123456789101112131415161718class Solution { public boolean canPartition(int[] nums) { int n = nums.length; int sum=0; for(int num:nums) sum+=num; if(sum%2==1) return false; int W = sum/2; boolean[][] dp = new boolean[n+1][W+1]; dp[0][0] = true; for(int i=1;i&lt;=n;i++){ for(int j=1;j&lt;=W;j++){ if(j&gt;=nums[i-1]) dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i-1]]; else dp[i][j] = dp[i-1][j]; } } return dp[n][W]; }} 同样可以简化空间 1234567891011121314151617class Solution { public boolean canPartition(int[] nums) { int n = nums.length; int sum=0; for(int num:nums) sum+=num; if(sum%2==1) return false; int W = sum/2; boolean[] dp = new boolean[W+1]; dp[0] = true; for(int i=1;i&lt;=n;i++){ for(int j=W;j&gt;=1;j--){ if(j&gt;=nums[i-1]) dp[j] = dp[j] || dp[j-nums[i-1]]; } } return dp[W]; }} 2.改变一组数的正负号使得它们的和为一给定数494. Target Sum (Medium) 该问题可以转换为 Subset Sum 问题，从而使用 0-1 背包的方法来求解。 可以将这组数看成两部分，P 和 N，其中 P 使用正号，N 使用负号，有以下推导： 123 sum(P) - sum(N) = targetsum(P) + sum(N) + sum(P) - sum(N) = target + sum(P) + sum(N) 2 * sum(P) = target + sum(nums) 因此只要找到一个子集，令它们都取正号，并且和等于 (target + sum(nums))/2，就证明存在解。 1234567891011121314151617class Solution { public int findTargetSumWays(int[] nums, int S) { int n = nums.length; int sum = 0; for(int num:nums) sum+=num; if(sum&lt;S || (sum + S) % 2 == 1) return 0; int W = (sum+S)/2; int[] dp = new int[W+1]; dp[0] = 1; for(int num:nums){ for(int j=W;j&gt;=num;j--){ dp[j] = dp[j] + dp[j-num]; } } return dp[W]; }} 3.01 字符构成最多的字符串474. Ones and Zeroes (Medium) 这是一个多维费用的 0-1 背包问题，有两个背包大小，0 的数量和 1 的数量。 dp[z][i][j] : 前z个元素在不超过i个0和j个1时的最大数量 同样进行了空间简化 12345678910111213141516171819class Solution { public int findMaxForm(String[] strs, int m, int n) { if(strs==null || strs.length==0) return 0; int[][] dp = new int[m+1][n+1]; for(String s:strs){ int zeros=0,ones=0; for(char c:s.toCharArray()){ if(c=='0') zeros++; else ones++; } for(int i=m;i&gt;=zeros;i--){ for(int j=n;j&gt;=ones;j--){ dp[i][j] = Math.max(dp[i][j],dp[i-zeros][j-ones]+1); } } } return dp[m][n]; }} 4.找零钱的最少硬币数322. Coin Change (Medium) 因为硬币可以重复使用，因此这是一个完全背包问题。完全背包只需要将 0-1 背包的逆序遍历 dp 数组改为正序遍历即可。 12345678910111213class Solution { public int coinChange(int[] coins, int amount) { int[] dp = new int[amount+1]; Arrays.fill(dp,amount+1); dp[0] = 0; for(int coin : coins){ for(int i=coin;i&lt;=amount;i++){ if(i&gt;=coin) dp[i] = Math.min(dp[i],dp[i-coin]+1); } } return dp[amount]&gt;amount ? -1 : dp[amount]; }} 5.找零钱的硬币数组合518. Coin Change 2 (Medium) 12345678910111213class Solution { public int change(int amount, int[] coins) { if(coins==null) return 0; int[] dp = new int[amount+1]; dp[0] = 1; for(int coin:coins){ for(int i=coin;i&lt;=amount;i++){ dp[i] = dp[i] + dp[i-coin]; } } return dp[amount]; }} 6.字符串按单词列表分割139. Word Break (Medium) dict 中的单词没有使用次数的限制，因此这是一个完全背包问题。该问题涉及到字典中单词的使用顺序，也就是说物品必须按一定顺序放入背包中。 求解顺序的完全背包问题时，对物品的迭代应该放在最里层，对背包的迭代放在外层，只有这样才能让物品按一定顺序放入背包中。 12345678910111213141516class Solution { public boolean wordBreak(String s, List&lt;String&gt; wordDict) { int n = s.length(); boolean[] dp = new boolean[n+1]; dp[0] = true; for(int i=1;i&lt;=n;i++){ for(String word:wordDict){ int len = word.length(); if(i&gt;=len &amp;&amp; word.equals(s.substring(i-len,i))){ dp[i] = dp[i] || dp[i-len]; } } } return dp[n]; }} 7.组合总和377. Combination Sum IV (Medium) 123456789101112class Solution { public int combinationSum4(int[] nums, int target) { int[] dp = new int[target+1]; dp[0] = 1; for(int i=1;i&lt;=target;i++){ for(int num:nums){ if(i&gt;=num) dp[i] = dp[i] + dp[i-num]; } } return dp[target]; }} 股票交易1. 需要冷却期的股票交易309. Best Time to Buy and Sell Stock with Cooldown(Medium) 如果我们把每次的状态进行细致分类的话一共有四种： Buy: 当天买股票S1: 持有股票观望Sell：卖出股票S2： 没有股票观望 那么状态转移方程如下： Buy[i] = S2[i-1]-prices[i]; 没有股票时买入股票s1[i] = Math.max(Buy[i-1],s1[i-1]); 买入后和不卖获得的最大利润sell[i] = Math.max(s1[i-1],Buy[i-1])+price 直接卖和观望后卖s2[i] = Math.max(sell[i-1],s2[i-1]) 卖后的利润和不买的利润 最终我们的最大利润在sell中或s2中。因为不可能买了不卖的利润最大。 12345678910111213141516171819class Solution { public int maxProfit(int[] prices) { if (prices == null || prices.length == 0) return 0; int N = prices.length; int[] buy = new int[N]; int[] s1 = new int[N]; int[] sell = new int[N]; int[] s2 = new int[N]; buy[0] = s1[0] = -prices[0]; s2[0] = sell[0] = 0; for(int i=1;i&lt;prices.length;i++){ buy[i] = s2[i-1]-prices[i]; s1[i] = Math.max(s1[i-1],buy[i-1]); sell[i] = Math.max(s1[i-1],buy[i-1])+prices[i]; s2[i] = Math.max(sell[i-1],s2[i-1]); } return Math.max(sell[N-1],s2[N-1]); }} 2.需要交易费用的股票交易714. Best Time to Buy and Sell Stock with Transaction Fee (Medium) 上面的问题看懂了，这个也就比较简单了。和上面的不同之处在于卖完之后可以直接买。 12345678910111213141516171819class Solution { public int maxProfit(int[] prices, int fee) { if (prices == null || prices.length == 0) return 0; int N = prices.length; int[] buy = new int[N]; int[] s1 = new int[N]; int[] sell = new int[N]; int[] s2 = new int[N]; buy[0] = s1[0] = -prices[0]; s2[0] = sell[0] = 0; for(int i=1;i&lt;prices.length;i++){ buy[i] = Math.max(s2[i-1],sell[i-1])-prices[i]; s1[i] = Math.max(s1[i-1],buy[i-1]); sell[i] = Math.max(s1[i-1],buy[i-1])+prices[i]-fee; s2[i] = Math.max(sell[i-1],s2[i-1]); } return Math.max(sell[N-1],s2[N-1]); }} 3.只能进行两次的股票交易123. Best Time to Buy and Sell Stock III (Hard) 123456789101112131415161718192021222324class Solution { public int maxProfit(int[] prices) { if(prices == null || prices.length == 0) return 0; int length = prices.length; int[] oneProfit = new int[length]; //min from 0 to i(included) int minSoFar = prices[0]; for(int i = 1; i &lt; length; i++){ oneProfit[i] = Math.max(oneProfit[i - 1], prices[i] - minSoFar); minSoFar = Math.min(minSoFar, prices[i]); } int[] twoProfit = new int[length]; int mindiff = prices[0]; for(int i = 1; i &lt; length; i++){ twoProfit[i] = Math.max(twoProfit[i - 1], prices[i] - mindiff); mindiff = Math.min(mindiff, prices[i] - oneProfit[i]); } return twoProfit[length - 1]; }} 简化空间 1234567891011121314151617181920212223242526class Solution { public int maxProfit(int[] prices) { if(prices == null || prices.length == 0) return 0; int length = prices.length; //min from 0 to i(included) int minSoFar = prices[0]; //min value of(prices[i] - oneProfit) from 0 to i(included) int mindiff = prices[0]; //max profit at day i if at most one transaction is allowed up to the day int oneProfit = 0; //max profit at day i if at most two transaction is allowed up to the day int twoProfit = 0; for(int i = 1; i &lt; length; i++){ oneProfit = Math.max(oneProfit, prices[i] - minSoFar); minSoFar = Math.min(minSoFar, prices[i]); twoProfit = Math.max(twoProfit, prices[i] - mindiff); mindiff = Math.min(mindiff, prices[i] - oneProfit); } return twoProfit; }} 4.只能进行 k 次的股票交易188. Best Time to Buy and Sell Stock IV (Hard) 1234567891011121314151617181920212223class Solution { public int maxProfit(int k, int[] prices) { int n = prices.length; if (k &gt;= n / 2) { // 这种情况下该问题退化为普通的股票交易问题 int maxProfit = 0; for (int i = 1; i &lt; n; i++) { if (prices[i] &gt; prices[i - 1]) { maxProfit += prices[i] - prices[i - 1]; } } return maxProfit; } int[][] dp = new int[k + 1][n]; for (int i = 1; i &lt;= k; i++) { int localMax = dp[i - 1][0] - prices[0]; for (int j = 1; j &lt; n; j++) { dp[i][j] = Math.max(dp[i][j - 1], prices[j] + localMax); localMax = Math.max(localMax, dp[i - 1][j] - prices[j]); } } return dp[k][n - 1]; }} 字符串编辑1.删除两个字符串的字符使它们相等583. Delete Operation for Two Strings (Medium) 转化为求最长公共子序列 12345678910111213class Solution { public int minDistance(String word1, String word2) { int m = word1.length(),n = word2.length(); int[][] dp = new int[m+1][n+1]; for(int i=1;i&lt;=m;i++){ for(int j=1;j&lt;=n;j++){ if(word1.charAt(i-1)==word2.charAt(j-1)) dp[i][j] = dp[i-1][j-1]+1; else dp[i][j] = Math.max(dp[i][j-1],dp[i-1][j]); } } return m+n-2*dp[m][n]; }} 2. 编辑距离72. Edit Distance (Hard) 状态定义：dp[i][j]表示从word1[0..i) 转换到 word2[0..j) 所需要的最小步骤 初始条件：转化为一个空字符串所需要的最小步骤为字符串长度。所以dp[i][0] = i; dp[0][j] = j; 状态转移方程：if word1[i-1]=word2[j-1] 不需要操作 dp[i][j] = dp[i-1][j-1];if word[i]!=word[j] 直接使用word2[j - 1]替换word1[i - 1] 递推公式 dp[i][j] = dp[i - 1][j - 1] + 1; 如果word1[0..i - 1) = word2[0..j) 直接删除word1[i - 1] 递推公式dp[i][j] = dp[i - 1][j] + 1; 如果word1[0..i) + word2[j - 1] = word2[0..j) 把word2[j - 1]插入到word1[0..i) 递推公式(dp[i][j] = dp[i][j - 1] + 1). 12345678910111213141516171819class Solution { public int minDistance(String word1, String word2) { if (word1 == null || word2 == null) return 0; int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i &lt;= m; i++) dp[i][0] = i; for (int i = 1; i &lt;= n; i++) dp[0][i] = i; for (int i = 1; i &lt;= m; i++) { for (int j = 1; j &lt;= n; j++) { if (word1.charAt(i - 1) == word2.charAt(j - 1)) { dp[i][j] = dp[i - 1][j - 1]; } else { dp[i][j] = Math.min(dp[i - 1][j - 1], Math.min(dp[i][j - 1], dp[i - 1][j])) + 1; } } } return dp[m][n]; }} 3.复制粘贴字符650. 2 Keys Keyboard (Medium) 123456789101112131415class Solution { public int minSteps(int n) { int[] dp = new int[n+1]; for (int i = 2; i &lt;= n; i++) { dp[i] = i; for (int j = i-1; j &gt; 1; j--) { if (i % j == 0) { dp[i] = dp[j] + (i/j); break; } } } return dp[n]; }}","link":"/2020/06/09/leetcode%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%BB%83%E4%B9%A0/"},{"title":"Redis初探","text":"","link":"/2020/06/18/Redis%E5%88%9D%E6%8E%A2/"},{"title":"Redis数据类型","text":"主要内容 String、List、Set、Hash、Zset应用场景及常用API Geospatial、Hyperloglog、Bitmap应用场景及常用API 五种基本数据类型String(字符串)String 类型是 Redis 中最常使用的类型，内部的实现是通过SDS（Simple Dynamic String ）来存储的。SDS 类似于 Java 中的 ArrayList，可以通过预分配冗余空间的方式来减少内存的频繁分配。 应用场景 缓存功能: 利用redis支持高并发的特点，可以大大加快系统的读写速度，降低后端数据库的压力 计数器： 可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库永久保存 共享用户session : 将用户session放入redis中进行集中管理，只要保证redis的高可用，每次用户session的获取都可以快速完成 常用API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121##########################################################################127.0.0.1:6379&gt; set k1 v1 # 设置值OK127.0.0.1:6379&gt; get k1 # 获得值\"v1\"127.0.0.1:6379&gt; keys * # 查看所有key1) \"k1\"127.0.0.1:6379&gt; exists k1 # 判断当前key是否存在(integer) 1127.0.0.1:6379&gt; append k1 hello # 追加字符串，如果当前key不存在，就相当于setkey(integer) 7127.0.0.1:6379&gt; get k1\"v1hello\"127.0.0.1:6379&gt; strlen k1 # 获取字符串的长度(integer) 7########################################################################## # incr 自增1# decr 自减1# incrby 和 decrby可以指定步长127.0.0.1:6379&gt; set views 0 # 初始浏览量为0OK127.0.0.1:6379&gt; incr views # 自增1 浏览量变为1(integer) 1127.0.0.1:6379&gt; incr views # 自增1 浏览量变为2(integer) 2127.0.0.1:6379&gt; get views\"2\"127.0.0.1:6379&gt; decr views # 自减1 浏览量-1(integer) 1127.0.0.1:6379&gt; decr views(integer) 0127.0.0.1:6379&gt; decr views(integer) -1127.0.0.1:6379&gt; incr views(integer) 0127.0.0.1:6379&gt; incrby views 10 # 可以设置步长，指定增量(integer) 10127.0.0.1:6379&gt; decrby views 5(integer) 5127.0.0.1:6379&gt; get views\"5\"###########################################################################mset 同时设置多个值#mget 同时获取多个值127.0.0.1:6379&gt; mset k1 v1 k2 v2 k3 v3 # 同时设置多个值OK127.0.0.1:6379&gt; keys *1) \"k1\"2) \"k2\"3) \"k3\"127.0.0.1:6379&gt; mget k1 k2 k3 # 同时获取多个值1) \"v1\"2) \"v2\"3) \"v3\"127.0.0.1:6379&gt; msetnx k1 v1 k4 v4 # msetnx 是一个原子性的操作，要么一起成功，要么一起失败！(integer) 0127.0.0.1:6379&gt; get k4(nil)########################################################################### getrange 获得指定范围内的值# setrange 修改指定范围内的值127.0.0.1:6379&gt; set k1 coderchen33OK127.0.0.1:6379&gt; get k1\"coderchen33\"127.0.0.1:6379&gt; getrange k1 0 4 # 截取字符串 [0,4]\"coder\"127.0.0.1:6379&gt; getrange k1 0 -1 # 获取全部的字符串 和 get k1是一样的\"coderchen33\"127.0.0.1:6379&gt; set k2 abcdefgOK127.0.0.1:6379&gt; get k2\"abcdefg\"127.0.0.1:6379&gt; setrange k2 1 xx # 替换指定位置开始的字符串(integer) 7127.0.0.1:6379&gt; get k2\"axxdefg\"########################################################################### setex (set with expire) # 设置过期时间# setnx (set if not exist) # 不存在再设置 （在分布式锁中会常常使用！）127.0.0.1:6379&gt; setex key3 30 \"hello\" # 设置key3 的值为 hello,30秒后过期OK127.0.0.1:6379&gt; ttl key3(integer) 26127.0.0.1:6379&gt; get key3\"hello\"127.0.0.1:6379&gt; setnx mykey \"redis\" # 如果mykey 不存在，创建mykey(integer) 1127.0.0.1:6379&gt; keys *1) \"key2\"2) \"mykey\"3) \"key1\"127.0.0.1:6379&gt; ttl key3(integer) -2127.0.0.1:6379&gt; setnx mykey \"MongoDB\" # 如果mykey存在，创建失败！(integer) 0127.0.0.1:6379&gt; get mykey\"redis\"########################################################################### 对象set user:1 {name:zhangsan,age:3} # 设置一个user:1 对象 值为 json字符来保存一个对象！# 这里的key是一个巧妙的设计： user:{id}:{filed} , 如此设计在Redis中是完全OK了！127.0.0.1:6379&gt; mset user:1:name zhangsan user:1:age 2OK127.0.0.1:6379&gt; mget user:1:name user:1:age1) \"zhangsan\"2) \"2\"########################################################################### getset 先get然后在set127.0.0.1:6379&gt; getset db redis # 如果不存在值，则返回 nil(nil)127.0.0.1:6379&gt; get db\"redis127.0.0.1:6379&gt; getset db mongodb # 如果存在值，获取原来的值，并设置新的值\"redis\"127.0.0.1:6379&gt; get db\"mongodb\" List(列表)Redis 的列表相当于 Java 语言中的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)。 应用场景 存储列表型数据: 如粉丝列表、文章评论列表等 分页查询： 借助于lrange读取某个闭区间内的元素，可以基于 List 实现分页查询 简单的消息队列 ： 左进右出来完成队列的设计。如数据的生产者可以通过Lpush命令从左边插入数据，多个数据消费者，可以使用BRpop命令阻塞的“抢”列表尾部的数据。 常用API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116##########################################################################127.0.0.1:6379&gt; lpush list one #列表头部添加元素(integer) 1127.0.0.1:6379&gt; lpush list two(integer) 2127.0.0.1:6379&gt; lpush list three(integer) 3127.0.0.1:6379&gt; lrange list 0 -1 # 查看列表中所有元素1) \"three\"2) \"two\"3) \"one\"127.0.0.1:6379&gt; lrange list 0 1 # 查看指定范围元素1) \"three\"2) \"two\"127.0.0.1:6379&gt; rpush list zero # 列表尾部插入元素(integer) 4127.0.0.1:6379&gt; lrange list 0 -11) \"three\"2) \"two\"3) \"one\"4) \"zero\"127.0.0.1:6379&gt; lpop list # 列表头部删除元素\"three\"127.0.0.1:6379&gt; lpop list\"two\"127.0.0.1:6379&gt; lrange list 0 -11) \"one\"2) \"zero\"127.0.0.1:6379&gt; rpop list # 列表尾部删除元素\"zero\"127.0.0.1:6379&gt; rpop list\"one\"###########################################################################lset 将列表中指定下标的值替换为另外一个值127.0.0.1:6379&gt; EXISTS list # 判断这个列表是否存在(integer) 0127.0.0.1:6379&gt; lset list 0 item # 如果不存在列表我们去更新就会报错(error) ERR no such key127.0.0.1:6379&gt; lpush list value1(integer) 1127.0.0.1:6379&gt; lrange list 0 01) \"value1\"127.0.0.1:6379&gt; lset list 0 item # 如果存在，更新当前下标的值OK127.0.0.1:6379&gt; lrange list 0 01) \"item\"127.0.0.1:6379&gt; lset list 1 other # 如果不存在，则会报错！(error) ERR index out of range###########################################################################linsert 将某个具体的value插入到列把你中某个元素的前面或者后面！127.0.0.1:6379&gt; rpush mylist \"hello\"(integer) 1127.0.0.1:6379&gt; rpush mylist \"world\"(integer) 2127.0.0.1:6379&gt; linsert mylist before \"world\" \"other\"(integer) 3127.0.0.1:6379&gt; lrange mylist 0 -11) \"hello\"2) \"other\"3) \"world\"127.0.0.1:6379&gt; linsert mylist after world new(integer) 4127.0.0.1:6379&gt; LRANGE mylist 0 -11) \"hello\"2) \"other\"3) \"world\"4) \"new\"########################################################################### lindex 获得指定下标处的值# llen 获得列表长度127.0.0.1:6379&gt; lpush list one (integer) 1127.0.0.1:6379&gt; lpush list two(integer) 2127.0.0.1:6379&gt; lindex list 1 #获得list的下标1处元素\"one\"127.0.0.1:6379&gt; lindex list 0\"two\"127.0.0.1:6379&gt; llen list # 获得列表长度(integer) 2########################################################################### lrem删除list集合中指定个数的元素127.0.0.1:6379&gt; lrange list 0 -11) \"three\"2) \"three\"3) \"two\"4) \"one\"127.0.0.1:6379&gt; lrem list 1 one # 删除list集合中指定个数的元素(integer) 1127.0.0.1:6379&gt; lrange list 0 -11) \"three\"2) \"three\"3) \"two\"127.0.0.1:6379&gt; lrem list 2 three(integer) 2127.0.0.1:6379&gt; lrange list 0 -11) \"two\"########################################################################### ltrim 修剪127.0.0.1:6379&gt; rpush mylist hello(integer) 1127.0.0.1:6379&gt; rpush mylist hello2(integer) 2127.0.0.1:6379&gt; rpush mylist hello3(integer) 3127.0.0.1:6379&gt; lrange mylist 0 -11) \"hello\"2) \"hello2\"3) \"hello3\"127.0.0.1:6379&gt; ltrim mylist 1 2 #修建mylist 只剩指定范围内的元素OK127.0.0.1:6379&gt; lrange mylist 0 -11) \"hello2\"2) \"hello3\" Hash(哈希)Redis 中的字典相当于 Java 中的 HashMap，内部实现也差不多类似，都是通过 “数组 + 链表” 的链地址法来解决部分哈希冲突，同时这样的结构也吸收了两种不同数据结构的优点。 应用场景 简单对象的存储 : Hash中存储对象的不同属性 常用API 123456789101112131415161718192021222324252627282930313233343536373839127.0.0.1:6379&gt; hset myhash field1 value1 #设置一个字段(integer) 2127.0.0.1:6379&gt; hmset user:1 name chen33 age 18 Province hebei # 设置多个字段 Hash更适合存储对象(integer) 3127.0.0.1:6379&gt; hget user:1 name # 获得一个字段的值\"chen33\"127.0.0.1:6379&gt; hmget user:1 age Province # 获得多个字段的值1) \"18\"2) \"hebei\"127.0.0.1:6379&gt; hgetall user:1 # 获得所有字段和对应的值1) \"name\"2) \"chen33\"3) \"age\"4) \"18\"5) \"Province\"6) \"hebei\"127.0.0.1:6379&gt; hlen user:1 # 获得hash长度(integer) 3127.0.0.1:6379&gt; hexists user:1 age #判断是否存在某个字段(integer) 1127.0.0.1:6379&gt; hkeys user:1 # 获得所有field1) \"name\"2) \"age\"3) \"Province\"127.0.0.1:6379&gt; hvals user:1 # 获得所有value1) \"chen33\"2) \"18\"3) \"hebei\"127.0.0.1:6379&gt; hdel user:1 Province #删除指定字段(integer) 1127.0.0.1:6379&gt; hkeys user:11) \"name\"2) \"age\"##########################################################################127.0.0.1:6379&gt; HINCRBY user:1 age 10 # 增加指定步长(integer) 28127.0.0.1:6379&gt; HINCRBY user:1 age -10(integer) 18 Set(集合)Redis 的集合相当于 Java 语言中的 HashSet，它内部的键值对是无序、唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值 NULL。 应用场景 set的最基本作用去重： 比如用户不能重复等。 交集、并集、补集操作： 比如交集。可以把两个人的好友列表进行交集操作，看看共同好友。 常用API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990##########################################################################127.0.0.1:6379&gt; sadd myset hello # 添加元素(integer) 1127.0.0.1:6379&gt; sadd myset coder(integer) 1127.0.0.1:6379&gt; sadd myset chen33(integer) 1127.0.0.1:6379&gt; SMEMBERS myset # 查看集合中所有元素1) \"coder\"2) \"chen33\"3) \"hello\"127.0.0.1:6379&gt; SISMEMBER myset hello # 判断元素是否在集合中(integer) 1127.0.0.1:6379&gt; SISMEMBER myset chen22(integer) 0127.0.0.1:6379&gt; scard myset # 获得集合的大小(integer) 3127.0.0.1:6379&gt; SREM myset hello #删除集合中的指定元素(integer) 1127.0.0.1:6379&gt; scard myset(integer) 2127.0.0.1:6379&gt; SMEMBERS myset1) \"coder\"2) \"chen33\"########################################################################### srandmember 随机抽取一个元素# spop 随机删除一个元素127.0.0.1:6379&gt; SMEMBERS myset1) \"v2\"2) \"v4\"3) \"v3\"4) \"v1\"127.0.0.1:6379&gt; SRANDMEMBER myset #随机抽取一个元素\"v2\"127.0.0.1:6379&gt; SRANDMEMBER myset\"v1\"127.0.0.1:6379&gt; SRANDMEMBER myset 2 #随机抽取指定个数元素1) \"v4\"2) \"v1\"127.0.0.1:6379&gt; SPOP myset # 随机删除一个元素\"v1\"127.0.0.1:6379&gt; SMEMBERS myset1) \"v2\"2) \"v4\"3) \"v3\"127.0.0.1:6379&gt; SPOP myset 2 #随机删除指定个数元素1) \"v3\"2) \"v2\"127.0.0.1:6379&gt; smembers myset1) \"v4\"########################################################################### smove 将一个集合中的元素移动到另一个集合127.0.0.1:6379&gt; SMEMBERS myset1) \"coder\"2) \"hello\"127.0.0.1:6379&gt; sadd myset2 set2(integer) 1127.0.0.1:6379&gt; smove myset myset2 hello # 从myset中移动hello元素到myset2中(integer) 1127.0.0.1:6379&gt; SMEMBERS myset1) \"coder\"127.0.0.1:6379&gt; smembers myset21) \"hello\"2) \"set2\"##########################################################################数字集合类：- 差集 SDIFF- 交集- 并集127.0.0.1:6379&gt; SMEMBERS myset11) \"a\"2) \"c\"3) \"b\"127.0.0.1:6379&gt; smembers myset21) \"e\"2) \"c\"3) \"d\"127.0.0.1:6379&gt; SDIFF myset1 myset2 # 差集 myset1中和myset2中的不同元素1) \"a\"2) \"b\"127.0.0.1:6379&gt; SINTER myset1 myset2 # 交集1) \"c\"127.0.0.1:6379&gt; SUNION myset1 myset2 # 并集1) \"a\"2) \"c\"3) \"b\"4) \"e\"5) \"d\" Zset(有序集合)这可能使 Redis 最具特色的一个数据结构了，它类似于 Java 中 SortedSet 和 HashMap 的结合体，一方面它是一个set，可以去重，保证了内部 value 的唯一性，另一方面它可以为每个 value 赋予一个 score 值，用来代表排序的权重。 应用场景 从上面我们知道，当我们需要一个有序且不重复的集合列表时，就可以选择zset数据结构作为选择方案。 排行榜：有序集合经典使用场景。比如游戏积分排名，视频播放量排名等，微博热搜等。 带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行 常用API 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263########################################################################### set key1 value1# zset key1 score value1127.0.0.1:6379&gt; zadd myset 1 one # 添加一个分数为1的元素one(integer) 1127.0.0.1:6379&gt; zadd myset 2 two(integer) 1127.0.0.1:6379&gt; zrange myset 0 -1 # 查看所有元素1) \"one\"2) \"two\"127.0.0.1:6379&gt; zcard myset # 获得有序集合大小(integer) 2127.0.0.1:6379&gt; zrem myset one # 删除集合中指定元素(integer) 1127.0.0.1:6379&gt; zrange myset 0 -11) \"two\"127.0.0.1:6379&gt; zadd myset 1 yi(integer) 1127.0.0.1:6379&gt; zadd myset 3 three(integer) 1127.0.0.1:6379&gt; zadd myset 1 one(integer) 1127.0.0.1:6379&gt; zrange myset 0 -11) \"one\"2) \"yi\"3) \"two\"4) \"three\"127.0.0.1:6379&gt; zcount myset 1 2 #获取指定区间的元素数量[1,2](integer) 3127.0.0.1:6379&gt; zcount myset 2 3(integer) 2##########################################################################排序如何实现# zrangebyscore 从小到大排序 可以指定范围# zrevrange 从大到小排序127.0.0.1:6379&gt; zadd age 18 chen11(integer) 1127.0.0.1:6379&gt; zadd age 20 chen22(integer) 1127.0.0.1:6379&gt; zadd age 19 chen33(integer) 1127.0.0.1:6379&gt; ZRANGEBYSCORE age -inf +inf # 显示全部的用户 从小到大1) \"chen11\"2) \"chen33\"3) \"chen22\"127.0.0.1:6379&gt; ZRANGEBYSCORE age -inf +inf withscores # 显示所有的用户和对应分数 从小到大1) \"chen11\"2) \"18\"3) \"chen33\"4) \"19\"5) \"chen22\"6) \"20\"127.0.0.1:6379&gt; ZRANGEBYSCORE age -inf 19 withscores # 显示小于19的元素1) \"chen11\"2) \"18\"3) \"chen33\"4) \"19\"127.0.0.1:6379&gt; ZREVRANGE age 0 -1 # 从大到小显示所有元素1) \"chen22\"2) \"chen33\"3) \"chen11\" 三种特殊数据类型Geospatial(地理位置)可以用来保存地理位置，并作位置距离计算或者根据半径计算位置等。 应用场景 和地图有关的操作: 附近的人、打车距离计算、朋友定位等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576##########################################################################127.0.0.1:6379&gt; GEOADD china:city 116.40 39.90 beijing(integer) 1127.0.0.1:6379&gt; GEOADD china:city 121.47 31.23 shanghai(integer) 1127.0.0.1:6379&gt; GEOADD china:city 106.50 29.53 chongqing 114.05 22.52 shengzhen(integer) 2127.0.0.1:6379&gt; GEOADD china:city 120.16 30.24 hangzhou 108.96 34.26 xian(integer) 2##########################################################################127.0.0.1:6379&gt; GEOPOS china:city beijing1) 1) \"116.39999896287918091\" 2) \"39.90000009167092543\"127.0.0.1:6379&gt; GEOPOS china:city beijing chongqing1) 1) \"116.39999896287918091\" 2) \"39.90000009167092543\"2) 1) \"106.49999767541885376\" 2) \"29.52999957900659211\"##########################################################################127.0.0.1:6379&gt; GEODIST china:city beijing shanghai km\"1067.3788\"127.0.0.1:6379&gt; GEODIST china:city beijing chongqing km\"1464.0708\"##########################################################################127.0.0.1:6379&gt; GEORADIUS china:city 110 30 1000 km1) \"chongqing\"2) \"xian\"3) \"shengzhen\"4) \"hangzhou\"127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km1) \"chongqing\"2) \"xian\"127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist1) 1) \"chongqing\" 2) \"341.9374\"2) 1) \"xian\" 2) \"483.8340\"127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withcoord1) 1) \"chongqing\" 2) 1) \"106.49999767541885376\" 2) \"29.52999957900659211\"2) 1) \"xian\" 2) 1) \"108.96000176668167114\" 2) \"34.25999964418929977\"##########################################################################127.0.0.1:6379&gt; GEORADIUSBYMEMBER china:city beijing 1000 km1) \"beijing\"2) \"xian\"127.0.0.1:6379&gt; GEORADIUSBYMEMBER china:city shanghai 400 km1) \"hangzhou\"2) \"shanghai\"##########################################################################127.0.0.1:6379&gt; GEOHASH china:city beijing1) \"wx4fbxxfke0\"127.0.0.1:6379&gt; GEOHASH china:city chongqing1) \"wm5xzrybty0\"##########################################################################127.0.0.1:6379&gt; zrange china:city 0 -11) \"chongqing\"2) \"xian\"3) \"shengzhen\"4) \"hangzhou\"5) \"shanghai\"6) \"beijing\"127.0.0.1:6379&gt; zrem china:city beijing(integer) 1127.0.0.1:6379&gt; zrange china:city 0 -11) \"chongqing\"2) \"xian\"3) \"shengzhen\"4) \"hangzhou\"5) \"shanghai\"127.0.0.1:6379&gt; Hyperloglog(基数统计) 什么是基数? 就是不重复的数比如 A = {1,2,3,4,3,5} 则A的基数为5 3重复只计一次 提供不精确的去重计数功能，比较适合用来做大规模数据的去重统计。如果允许容错，那么一定可以使用 Hyperloglog ！如果不允许容错，就使用 set 或者自己的数据类型即可！ 应用场景 网站UV(一个人访问一个网站多次，但是还是算作一个人！） :传统的方式，set 保存用户的id，然后就可以统计 set 中的元素数量作为标准判断 !这个方式如果保存大量的用户id，就会比较麻烦！我们的目的是为了计数，而不是保存用户id； 123456789101112131415161718##########################################################################127.0.0.1:6379&gt; pfadd codehole user1(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 1127.0.0.1:6379&gt; pfadd codehole user2 user3 user4 user5(integer) 1127.0.0.1:6379&gt; pfcount codehole(integer) 5127.0.0.1:6379&gt; pfadd codehole2 user4 user5 user6 user7 user8(integer) 1127.0.0.1:6379&gt; pfcount codehole2(integer) 5##########################################################################127.0.0.1:6379&gt; pfmerge codehole3 codehole codehole2OK127.0.0.1:6379&gt; pfcount codehole3(integer) 8 Bitmap(位图)位图是支持按 bit 位来存储信息。 应用场景 bool类型的数据存取 ： 打卡签到、活跃不活跃 实现布隆过滤器（之后讲） 使用bitmap 来记录 周一到周日的打卡！ 1234567891011121314151617181920127.0.0.1:6379&gt; setbit sign 0 1(integer) 0127.0.0.1:6379&gt; setbit sign 1 0(integer) 0127.0.0.1:6379&gt; setbit sign 2 0(integer) 0127.0.0.1:6379&gt; setbit sign 3 1(integer) 0127.0.0.1:6379&gt; setbit sign 4 1(integer) 0127.0.0.1:6379&gt; setbit sign 5 0(integer) 0127.0.0.1:6379&gt; setbit sign 6 0(integer) 0127.0.0.1:6379&gt; getbit sign 3(integer) 1127.0.0.1:6379&gt; getbit sign 6(integer) 0127.0.0.1:6379&gt; bitcount sign(integer) 3","link":"/2020/06/18/Redis%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"title":"redis中的事务","text":"redis中事务的概念和特点 Redis事务的基本概念Redis事务本质：一组命令的集合！ 一个事务中的所有命令都会被序列化，在事务执行过程的中，会按照顺序执行！ redis事务的特点: Redis单条命令式保存原子性的，但是事务不保证原子性！（发生运行时异常时其他命令可以成功执行） Redis事务没有隔离级别的概念！： 所有的指令在 exec 之前不执行，而是缓存在服务器的一个事务队列中，服务器一旦收到 exec 指令，才开执行整个事务队列，执行完毕后一次性返回所有指令的运行结果。 Redis事务的使用123开启事务（multi）命令入队（......）提交事务（exec） / 丢弃事务(discard) 正常提交 123456789101112131415161718127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED127.0.0.1:6379&gt; get k2QUEUED127.0.0.1:6379&gt; set k4 v4QUEUED127.0.0.1:6379&gt; exec1) OK2) OK3) OK4) \"v2\"5) OK 丢弃事务 123456789101112127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k5 v5QUEUED127.0.0.1:6379&gt; discardOK127.0.0.1:6379&gt; get k5(nil) 编译型异常（代码有问题！ 命令有错！） ，事务中所有的命令都不会被执行！ 12345678910111213141516127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; getset k2(error) ERR wrong number of arguments for 'getset' command127.0.0.1:6379&gt; set k3 v3QUEUED127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; get k1(nil)127.0.0.1:6379&gt; get k3(nil) 运行时异常（1/0）， 如果事务队列中存在语法性，那么执行命令的时候，其他命令是可以正常执行的，错误命令抛出异常！ 12345678910111213141516171819127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 &quot;v1&quot; # 字符串无法进行+-操作QUEUED127.0.0.1:6379&gt; incr k1QUEUED127.0.0.1:6379&gt; set k2 v2QUEUED127.0.0.1:6379&gt; set k3 v3QUEUED127.0.0.1:6379&gt; exec1) OK2) (error) ERR value is not an integer or out of range3) OK4) OK127.0.0.1:6379&gt; get k1 # 可以看到其他语句执行成功了&quot;v1&quot;127.0.0.1:6379&gt; get k3 # 可以看到其他语句执行成功了&quot;v3&quot; Watch考虑到一个转账的业务场景，Redis 存储了我们的账户余额数据，它是一个整数。现在我们要进行转账操作，首先整个转账流程因为是原子性的需要放到事务中处理。但是如果这个时候有两个并发的客户端要对账户余额进行修改操作，这时就会出现并发问题。 因为有多个客户端会并发进行操作。我们可以通过 Redis 的分布式锁来避免冲突，这是一个很好的解决方案。 但是分布式锁是一种悲观锁，那是不是可以使用乐观锁的方式来解决冲突呢？ Redis 提供了乐观锁watch来解决并发修改问题 首先我们看下watch的使用方法 123456789101112131415127.0.0.1:6379&gt; set money 100OK127.0.0.1:6379&gt; set out 0OK127.0.0.1:6379&gt; watch money # 监视money对象OK127.0.0.1:6379&gt; multi # 事务正常结束，数据期间没有发生变动，这个时候就正常执行成功！OK127.0.0.1:6379&gt; decrby money 20QUEUED127.0.0.1:6379&gt; incrby out 20QUEUED127.0.0.1:6379&gt; exec1) (integer) 802) (integer) 20 演示watch解决并发修改问题 事务1 1234567891011127.0.0.1:6379&gt; watch moneyOK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; decrby money 10QUEUED127.0.0.1:6379&gt; incrby out 10QUEUED#################### 这时候事务2对money进行了改动127.0.0.1:6379&gt; exec(nil) # 可以看到，事务执行失败 事务2对money进行了修改 1234127.0.0.1:6379&gt; get money\"80\"127.0.0.1:6379&gt; set money 200OK 事务1 12345678910111213127.0.0.1:6379&gt; unwatch # 发现事务执行失败，我们需要先解锁OK127.0.0.1:6379&gt; watch money # 解锁后重新对money进行监视OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; decrby money 15QUEUED127.0.0.1:6379&gt; incrby out 15QUEUED127.0.0.1:6379&gt; exec1) (integer) 1852) (integer) 35","link":"/2020/06/20/redis%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1/"},{"title":"leetcode数组和矩阵练习","text":"数组和矩阵练习tips 涉及到遍历数组时，如果使用for循环不方便时考虑使用while循环(数组下标可以在一定条件下进行+ -操作) 一个小tips是可以自定义一个数组下标idx，从而自由进行idx++操作。 对于矩阵而言，通常需要我们各种进行花式打印矩阵。这时候，就需要结合具体问题分析矩阵行、列的约束关系。 通常约束关系还涉及到矩阵的维度n 1.把数组中的 0 移到末尾283. Move Zeroes (Easy) 1234567891011class Solution { public void moveZeroes(int[] nums) { int idx = 0; for(int num:nums){ if(num!=0) nums[idx++] = num; } while(idx&lt;nums.length){ nums[idx++] = 0; } }} 2.改变矩阵维度566. Reshape the Matrix (Easy) 12345678910111213141516171819class Solution { public int[][] matrixReshape(int[][] nums, int r, int c) { int row=nums.length,col=nums[0].length; if(row*col != r*c) return nums; int[][] res = new int[r][c]; int rows=0,cols=0; for(int i=0;i&lt;row;i++){ for(int j=0;j&lt;col;j++){ res[rows][cols] = nums[i][j]; cols++; if(cols==c){ rows++; cols = 0; } } } return res; }} 3. 找出数组中最长的连续 1485. Max Consecutive Ones (Easy) 123456789101112class Solution { public int findMaxConsecutiveOnes(int[] nums) { int max = 0; int count = 0; for(int i=0;i&lt;nums.length;i++){ if(nums[i]==1) count++; else count = 0; max = Math.max(max,count); } return max; }} 4.有序矩阵查找I74. Search a 2D Matrix(Medium) 从左下或者右上进行寻找 12345678910111213class Solution { public boolean searchMatrix(int[][] matrix, int target) { if(matrix==null || matrix.length==0) return false; int row=matrix.length,col=matrix[0].length; int i=row-1,j=0; while(i&gt;=0 &amp;&amp; j&lt;col){ if(target==matrix[i][j]) return true; else if(target&lt;matrix[i][j]) i--; else j++; } return false; }} 5.有序矩阵查找II240. Search a 2D Matrix II (Medium) 和I完全没区别 12345678910111213class Solution { public boolean searchMatrix(int[][] matrix, int target) { if(matrix==null || matrix.length==0) return false; int row=matrix.length,col=matrix[0].length; int i=row-1,j=0; while(i&gt;=0 &amp;&amp; j&lt;col){ if(target==matrix[i][j]) return true; else if(target&lt;matrix[i][j]) i--; else j++; } return false; }} 6.有序矩阵的 Kth Element378. Kth Smallest Element in a Sorted Matrix ((Medium)) 1234567891011121314151617class Solution { public int kthSmallest(int[][] matrix, int k) { //Java默认是小顶堆，但这里为了求第k小的值我们需要大顶堆 //所以使用Collections.reverseOrder()进行倒叙 PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;(Collections.reverseOrder()); for(int[] row:matrix){ for(int num:row){ if(queue.size()&lt;k) queue.offer(num); else if(queue.peek()&gt;num){ queue.poll(); queue.offer(num); } } } return queue.peek(); }} 7. 一个数组元素在 [1, n] 之间，其中一个数被替换为另一个数，找出重复的数和丢失的数645. Set Mismatch (Easy) 最直接想到的方法是对数组进行排序或者使用过map存储元素和次数。但是排序时间复杂度为O(nlogn),使用map空间复杂度为O(n)，这道题目可以使用时间复杂度O(n),空间复杂度O(1)的方法进行解决。 主要思想是通过交换数组元素，使得数组上的元素在正确的位置上。 1234567891011121314151617181920class Solution { public int[] findErrorNums(int[] nums) { for (int i = 0; i &lt; nums.length; i++) { while (nums[i] != i + 1 &amp;&amp; nums[nums[i] - 1] != nums[i]) { swap(nums, i, nums[i] - 1); } } for (int i = 0; i &lt; nums.length; i++) { if (nums[i] != i + 1) { return new int[]{nums[i], i + 1}; } } return null; } private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; }} 找出数组中重复的数，数组值在 [1, n] 之间 287. Find the Duplicate Number (Medium) 双指针解法： 123456789101112131415class Solution { public int findDuplicate(int[] nums) { int lo=0,hi=nums.length-1; while(lo&lt;=hi){ int mid = lo +(hi-lo)/2; int count = 0; for(int i=0;i&lt;nums.length;i++){ if(nums[i]&lt;=mid) count++; } if(count&gt;mid) hi = mid-1; else lo = mid+1; } return lo; }} 快慢指针法，类似于我们之前的找链表环的交点 123456789101112131415class Solution { public int findDuplicate(int[] nums) { int slow = nums[0], fast = nums[nums[0]]; while (slow != fast) { slow = nums[slow]; fast = nums[nums[fast]]; } fast = 0; while (slow != fast) { slow = nums[slow]; fast = nums[fast]; } return slow; }} 8. 对角元素相等的矩阵766. Toeplitz Matrix (Easy) 1234567891011class Solution { public boolean isToeplitzMatrix(int[][] matrix) { int m = matrix.length,n=matrix[0].length; for(int i=0;i&lt;m-1;i++){ for(int j=0;j&lt;n-1;j++){ if(matrix[i][j]!=matrix[i+1][j+1]) return false; } } return true; }} 9. 嵌套数组565. Array Nesting (Medium) 123456789101112131415161718class Solution { public int arrayNesting(int[] nums) { int res = 0; boolean[] visited = new boolean[nums.length]; for(int i=0;i&lt;nums.length;i++){ if(!visited[i]){ int start = nums[i],count = 0; while(count==0 || start!=nums[i]){ visited[start] = true; start = nums[start]; count++; } res = Math.max(res,count); } } return res; }} 10.分隔数组769. Max Chunks To Make Sorted (Medium) original: 0, 2, 1, 4, 3, 5, 7, 6max: 0, 2, 2, 4, 4, 5, 7, 7sorted: 0, 1, 2, 3, 4, 5, 6, 7index: 0, 1, 2, 3, 4, 5, 6, 7 The chunks are: 0 | 2, 1 | 4, 3 | 5 | 7, 6 12345678910class Solution { public int maxChunksToSorted(int[] arr) { int res = 0,max = 0; for(int i=0;i&lt;arr.length;i++){ max = Math.max(max,arr[i]); if(max == i) res++; } return res; }}","link":"/2020/06/06/leetcode%E6%95%B0%E7%BB%84%E5%92%8C%E7%9F%A9%E9%98%B5%E7%BB%83%E4%B9%A0/"},{"title":"Java集合类概述","text":"","link":"/2020/06/21/Java%E9%9B%86%E5%90%88%E7%B1%BB%E6%A6%82%E8%BF%B0/"},{"title":"HashMap源码分析","text":"基于JDK1.8的HashMap的源码解析 HashMap无论是我们平时使用还是在面试中，都是十分常见的，所以很有必要仔细了解下它的源码。 HashMap底层由数组+链表+红黑树组成。 Node结点JDK1.8之后，KV键值对存储在Node结点中。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; //key的hash值 final K key; //key V value; //value Node&lt;K,V&gt; next; //链表的后继节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } //还有一些方法，由于篇幅原因没有放上来} hash方法hash方法就是用来将key映射成数组下标index，即key--&gt;index。 1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 看这个函数前首先我们需要知道 key.hashCode中的hashCode函数是定义在Object类中，是一个native方法public native int hashCode();。我们无法查看源码，但是从返回值上看该方法将任意对象映射成一个int类型值。完成Object--&gt;int的映射。 h&gt;&gt;&gt;16中&gt;&gt;&gt;为无符号右移，也叫逻辑右移，即若该数为正，则高位补0，若该数为负，右移后高位同样补0。 了解了上面两点，我们接下来再看。我们知道，int类型是32位的, h ^ h &gt;&gt;&gt; 16其实就是将hashCode的高16位和低16位进行异或, 这充分利用了高半位和低半位的信息, 对低位进行了扰动, 目的是为了使该hashCode映射成数组下标时可以更均匀, 详细的解释可以参考这里。 上面只是将key转换为了int类型，但是int范围-2147483648到2147483648，前后加起来大概40亿的映射空间。首先一个40亿长度的数组内存是放不下的，其次我们知道HashMap初始化时数组长度只有16。因此我们不可能直接将hash值用来当做数组下标。而是采用取模运算： key.hashCode() % table.length 但是取模运算是十分耗时的。另一方面, 我们知道, 当一个数是 2^n 时, 任意整数对2^n取模等效于: h % 2^n = h &amp; (2^n -1) 这样我们就将取模操作转换成了位操作, 而位操作的速度远远快于取模操作。这也是为什么HashMap的数组长度都是2的整数次幂。 另外, 从这个函数中, 我们还可以知道: HashMap中key值可以为null, 且null值一定存储在数组的第一个位置. 构造函数我们首先来看下HashMap的构造函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable { //默认数组初始大小16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 //数组最大长度为2^30，超过则不会进行扩容 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认负载因子0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; //链表长度阈值8,否则转换为红黑树 static final int TREEIFY_THRESHOLD = 8; //resize时，红黑树节点个数小于6会退化成链表 static final int UNTREEIFY_THRESHOLD = 6; //桶容量小于64，优先进行扩容，而不是转换为红黑树 static final int MIN_TREEIFY_CAPACITY = 64; //散列表进行resize的阈值 int threshold; //元素的真实存储位置 transient Node&lt;K,V&gt;[] table; //没有指定时使用默认值 //默认数组初始长度为16，默认负载因子为0.75 public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } //指定初始大小，但是用默认负载因子 //注意这里调用了下面的构造函数 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } //指定初始大小和负载因子 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } //利用已经存在的map创建HashMap public HashMap(Map&lt;? extends K, ? extends V&gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } 可以看到即使我们在构造函数中指定了initialCapacity, 这个值也只被用来计算 threshold 1this.threshold = tableSizeFor(initialCapacity); 我们先来看看tableSizeFor函数干了什么事： 123456789static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 我们只需要知道tableSizeFor这个方法用于找到大于等于initialCapacity的最小的2的幂，详细参考这篇博客 最后我们来看最后一个构造函数, 它调用了putMapEntries方法: 12345678910111213141516171819final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) { int s = m.size(); if (s &gt; 0) { if (table == null) { // pre-size float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); if (t &gt; threshold) threshold = tableSizeFor(t); } else if (s &gt; threshold) resize(); for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); } } } 我们知道, 当使用构造函数HashMap(Map&lt;? extends K, ? extends V&gt; m) 时, 我们并没有为 table 赋值, 所以, table值一定为null, 我们先根据传入Map的大小计算 threshold 值, 然后判断需不需要扩容, 最后调用 putVal方法将传入的Map插入table中。 小总结：到这里其实我们只知道HashMap的四个构造函数中都没有初始化table。并且也只是初始化了threshold和loadFactor这两个变量。 实际上table的初始化是定义在下面要讲的HashMap扩容时的resize方法。 resize方法我们知道resize方法主要有两个作用： table的初始化 数组扩容 接下来就来看看这个函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //原来的table中已经有值 if (oldCap &gt; 0) { //已经超过最大限制，不再进行扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //否则数组长度扩容成原来的2倍 //有个条件是数组长度大于等于默认长度即16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // threshold也变为原来2倍 newThr = oldThr &lt;&lt; 1; } // 在上面讲解构造函数时我们知道 // 如果没有指定initialCapacity, 则不会给threshold赋值, 该值被初始化为0 // 如果指定了initialCapacity, 该值被初始化成大于initialCapacity的最小的2的次幂 // 这里是指, 如果构造时指定了initialCapacity, 则用threshold作为table的实际大小 else if (oldThr &gt; 0) newCap = oldThr; // 如果构造时没有指定initialCapacity, 则用默认值。 //初始容量16，负载因子0.75。超过16*0.75=12时进行扩容 else { newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } //计算指定了initialCapacity下的新的threshold if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } //这里总结一下 //如果没有指定initialCapacity，则数组长度为16，threshold为16*0.75=12 //如果指定了initialCapacity，则数组长度为大于initialCapacity的最小的2的次幂，threshold为新的数组长度*负载因子 //即threshold为数组长度*负载因子。 threshold = newThr; // 从下面开始, 初始化table或者扩容, 实际上都是通过新建一个table来完成的 @SuppressWarnings({\"rawtypes\",\"unchecked\"}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 下面这段就是把原来table里面的值全部搬到新的table里面 if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; // 这里注意, table中存放的只是Node的引用, 这里将oldTab[j]=null只是清除旧表的引用, 但是真正的node节点还在, 只是现在由e指向它 if ((e = oldTab[j]) != null) { oldTab[j] = null; // 如果该存储桶里面只有一个节点, 就直接将它放到新表的目标位置 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果该存储桶里面存的是红黑树, 则拆分树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //存储桶里存储的是链表 //链表拆分这部分单独讲 else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab;} resize时的链表拆分 这部分单独拿出来讲一下： 第一段 12Node&lt;K,V&gt; loHead = null, loTail = null;Node&lt;K,V&gt; hiHead = null, hiTail = null; 这里定义了两个链表, 我们把它称为 lo链表 和 hi链表, loHead 和 loTail 分别指向 lo链表的头节点和尾节点, hiHead 和 hiTail以此类推. 第二段 12345678910111213141516171819do { next = e.next; if ((e.hash &amp; oldCap) == 0) { //插入lo链表 if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { //插入hi链表 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; }} while ((e = next) != null); 首先这个do-while循环就是用来按顺序遍历该存储桶位置上的链表的节点。 然后这个大的if-else中的小的if-else很显然就是用来进行链表的插入。 最后这个大的if-else用来对链表进行拆分，拆分标准为： 1(e.hash &amp; oldCap) == 0 到这里我们就知道了这一段的目的： 我们首先准备了两个链表 lo 和 hi, 然后我们顺序遍历该存储桶上的链表的每个节点, 如果 (e.hash &amp; oldCap) == 0, 我们就将节点放入lo链表, 否则, 放入hi链表. 第三段 12345678if (loTail != null) { loTail.next = null; newTab[j] = loHead;}if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead;} 这一段看上去就很简单了: 如果lo链表非空, 我们就把整个lo链表放到新table的j位置上如果hi链表非空, 我们就把整个hi链表放到新table的j+oldCap位置上 所以链表的resize过程可以表示为下图： 关于(e.hash &amp; oldCap) == 0、j以及j+oldCap 上面我们已经弄懂了链表拆分的代码, 但是这个拆分条件看上去很奇怪, 这里我们来稍微解释一下: 首先我们要明确三点: oldCap一定是2的整数次幂, 这里假设是2^m newCap是oldCap的两倍, 则会是2^(m+1) hash对数组大小取模(2^m - 1) &amp; hash 其实就是取hash的低m位 例如:我们假设 oldCap = 16, 即 2^4,16 - 1 = 15, 二进制表示为 0000 0000 0000 0000 0000 0000 0000 1111可见除了低4位, 其他位置都是0（简洁起见，高位的0后面就不写了）, 则 (16-1) &amp; hash 自然就是取hash值的低4位,我们假设它为 abcd. 同样,当我们将oldCap扩大两倍后, 新的index的位置就变成了 (32-1) &amp; hash, 其实就是取 hash值的低5位. 那么对于同一个Node, 低5位的值无外乎下面两种情况: 0abcd1abcd 其中, 0abcd与原来的index值一致, 而1abcd = 0abcd + 10000 = 0abcd + oldCap 故虽然数组大小扩大了一倍，但是同一个key在新旧table中对应的index却存在一定联系： 要么一致，要么相差一个 oldCap。 而新旧index是否一致就体现在hash值的第5位(我们把最低为称作第0位), 怎么拿到这一位的值呢, 只要: 1hash &amp; 0000 0000 0000 0000 0000 0000 0001 0000 上式就等效于 1hash &amp; oldCap 故得出结论: 如果 (e.hash &amp; oldCap) == 0 则该节点在新表的下标位置与旧表一致都为 j如果 (e.hash &amp; oldCap) == 1 则该节点在新表的下标位置 j + oldCap 根据这个条件, 我们将原位置的链表拆分成两个链表, 然后一次性将整个链表放到新的Table对应的位置上。 resize时的红黑树拆分 //暂时先放一下。 小总结 HashMap初始化时： 如果没有指定初始容量和负载因子，则数组默认初始长度为16，负载因子为0.75 如果指定了初始容量，实际新建时，数组长度为超过初始容量的最小2的整数次幂 无论是否指定初始容量，threshold均为新的数组长度*负载因子 每次扩容都会扩容为原数组大小的2倍。 扩容时,会将原table中的节点rehash到新的table中(通过hash&amp;原数组长度==0拆分)。节点在新旧table中的位置: 要么下标相同, 要么相差一个oldCap(原table的大小)。 put方法put方法可以说是HashMap的核心，我们看下它的源码： 123public V put(K key, V value) { return putVal(hash(key), key, value, false, true);} 可以看到put函数中实际调用的是putVal方法，在我们分析putVal源码之前，我们先看下传的这几个参数的含义： hash key的hash值 key 待存储的键 value 待存储的值 onlyIfAbsent 判断是否保留原来key处的值true，保留原有值 false覆盖原有值 evict 判断是用在put函数中，还是之前提到过的通过已有map初始化map的构造函数中。true表示用在put函数，false表示用在构造函数中 根据上面的put函数调用putVal(hash(key), key, value, false, true)我们知道现在是使用在put函数，并且如果原key有值，hashmap会覆盖原有值。 接下来我们逐行分析putVal方法的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 首先判断table是否是空的 // 我们知道, HashMap的三个构造函数中, 都不会初始Table, 因此第一次put值时, table一定是空的, 需要初始化 // 上面我们已经提到，table的初始化用到了resize函数 // 由此可见table的初始化是延迟到put操作中的 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 这里利用 `(n-1) &amp; hash` 方法计算 key 所对应的下标 // 如果key所对应的桶里面没有值, 我们就新建一个Node放入桶里面 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 否则，说明目标位置桶里已经有东西了 else { Node&lt;K,V&gt; e; K k; // 判断当前待存储的key值和已经存在的key值是否相等 // key值相等必须满足两个条件 // 1. hash值相同 // 2. 两者 `==` 或者 `equals` 等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //key已经存在的情况下，e保存原有键值对 //到这里说明要保存的桶已经被占用，且被占用的位置存放的key与待存储的key不一致 //判断是不是红黑树存储 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //到这里说明是链表存储，我们需要遍历链表 else { for (int binCount = 0; ; ++binCount) { // 如果已经找到了链表的尾节点了,还没有找到目标key, 则说明目标key不存在 //那我们就新建一个节点, 把它接在尾节点的后面 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 如果链表的长度达到了8个, 就将链表转换成红黑数以提升查找性能 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // 如果在链表中找到了目标key则直接退出 // 退出时e保存的是待存储key的键值对 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } // 如果待存储的key值已经存在 if (e != null) { // existing mapping for key V oldValue = e.value; // 这里是说旧值存在或者旧值为null的情况下, 用新值覆盖旧值 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); //这个函数只在LinkedHashMap中用到, 这里是空函数 // 返回旧值 return oldValue; } } // 到这里说明table中不存在待存储的key, 并且我们已经将新的key插入进数组了 ++modCount; // 这个暂时用不到 // 因为又插入了新值, 所以我们得把数组大小加1, 并判断是否需要重新扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict);//这个函数只在LinkedHashMap中用到, 这里是空函数 return null;} 小总结： 在put操作之前，会检查table是否为空，说明table真正的初始化并不是发生在构造函数中，而是发生在第一次put的时候。 通过(n - 1) &amp; hash方法计算 key 所对应的下标，如果目标桶内没有值，则直接加入新建Node节点。 如果有值，则通过p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))判断待存储key值与目标桶中的key值是否相同。不同则直接插入到链表尾部(链表长度超过8个时，转换为红黑树)，相同则覆盖原有的oldValue值，并返回oldValue值。 每次插入操作结束后，都会检查当前table节点数是否大于threshold, 若超过，则扩容为原来的2倍。 当链表长度超过8个时，并且数组长度超过64时(不超过64会直接扩容，而不是树化)，链表会转换为红黑树。 get方法get方法中是调用了getNode方法 1234public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;} getNode方法其实很简单，我们只需要知道我们需要传的两个参数： hash 通过hash值确定数组下标 key 在目标桶中寻找目标key12345678910111213141516171819202122232425final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //判断目标桶位置有值 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { //第一个节点就是，则直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //第一个节点不是，则判断是不是红黑树，从红黑树中查找返回 if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //到这里，说明是链表，则遍历链表寻找 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } //都没找到返回null return null; } 总结 HashMap底层是由数组+链表+红黑树组成。数组中存储的是KV键值对构成的Node节点。 当我们new一个HashMap时： 如果没有指定初始容量和负载因子，则数组默认初始长度为16，负载因子为0.75。指定了初始容量，实际新建时，数组长度为超过初始容量的最小2的整数次幂。负载因子0.75一般不建议更改 每次put操作时，都会检查是否产生hash冲突。解决hash冲突时，使用链地址法。当链表长度超过8时，并且数组长度超过64时(不超过64会直接扩容，而不是树化)，链表会转换为红黑树。 put完成后会检查数组节点是否已经超过数组长度*负载因子，即threshold，超过则直接进行扩容，扩容为原来的2倍。扩容后，需要通过hash&amp;原数组长度==0进行rehash。新节点与旧节点要么数组下标相同，要么是数组下标+原来的数组长度 常见问题1.为什么HashMap数组长度为2的整数次幂？ 原因是因为当一个数是2 ^ n时，任意整数对2 ^ n取模都等于该整数和2 ^ n-1进行与运算，即h % 2^n = h &amp; (2^n -1)。其实就是将取模运算变为了位运算这样在将hash值转换成数组下标时，提升了效率。 2.为什么HashMap默认初始容量是16，以及负载因子是0.75？ 负载因⼦的默认值是0.75，⽆论是初始⼤了还是初始⼩了对我们HashMap的性能都不好 负载因⼦初始值⼤了，可以减少散列表再散列(扩容的次数)，但同时会导致散列冲突的可能性变⼤(散列冲突也是耗性能的⼀个操作，要得操作链表(红⿊树)！ 负载因⼦初始值⼩了，可以减⼩散列冲突的可能性，但同时扩容的次数可能就会变多！ 初始容量的默认值是16，它也⼀样，⽆论初始⼤了还是⼩了，对我们的HashMap都是有影响的： 初始容量过⼤，那么遍历时我们的速度就会受影响~ 初始容量过⼩，散列表再散列(扩容的次数)可能就变得多，扩容也是⼀件⾮常耗费性能的⼀件事 Hashmap中的链表大小超过8个时会自动转化为红黑树，当删除小于6时重新变为链表，为啥呢？ 根据泊松分布，在负载因子默认为0.75的时候，单个hash槽内元素个数为8的概率小于百万分之一，所以将7作为一个分水岭，等于7的时候不转换，大于等于8的时候才进行转换，小于等于6的时候就化为链表。 3.为什么HashMap存放自定义对象时需要重写hashCode和equals方法 如果我们不重写hashCode和equals方法，则默认是继承Object类的hashCode和equals方法。 而Object类中的HashCode方法返回的是对象的内存地址。而equals方法实际是用的==判断。而==判断对于值对象，比较的是两个对象的值，而对于引用对象，比较的是两个对象的内存地址。 如果不重写hashCode和equals方法，即便我们新建的两个对象属性完全相同，但因为是不同的对象，所以内存地址是不同的。 HashMap存放key时，是根据key的hashCode值来找到数组下标的，这样的话,我们自定义的两个相同的对象由于hashCode值不同，所以都会存入HashMap中，违背了使用HashMap的初衷，所以要重写hashCode方法 即使重写了hashCode方法，两个对象放入了数组同一个位置，但我们在通过key取值的时候，是根据key的具体内容来取值，所以也需要重写equals方法，不然，只能找到数组下标，无法找到具体的key对应的value。 4.为什么1.7之前链表使用头插法，JDK1.8之后采用尾插法 参考这篇文章 https://blog.csdn.net/Ho528528/article/details/103903998 5.为什么使用红黑树而不是二叉查找树或者AVL树或者B树或者B+树 不用二叉查找树的原因： 虽然二叉查找树查找复杂度也是O(log n)，并且实现容易，但是增删改操作会破环二叉查找树的平衡性，最坏的情况有可能变成一个线性链表，搜索复杂度退化为O(n)。 不用二叉平衡查找树(AVL树)的原因: AVL树规定每一个结点的左右结点之差不超过1，追求绝对的平衡。在插入之后进行调整的次数不能确定。因此插入和删除较慢，而查找较快。 而红黑树的平衡条件不是那么严格，而且插入之后的调整在3次以内，保证其效率为O(logn)，所以红黑树添加、删除相对较快，查找相对较慢。但整体性能上还是红黑树更好一些。 不用B/B+树的原因B和B+树主要用于数据存储在磁盘上的场景，比如数据库索引就是用B+树实现的。这两种数据结构的特点就是树比较矮胖，每个结点存放一个磁盘大小的数据，这样一次可以把一个磁盘的数据读入内存，减少磁盘转动的耗时，提高效率。而红黑树多用于内存中排序，也就是内部排序，因此HashMap使用红黑树作为它的一种数据结构。 6.HashMap和HashTable的区别 最重要的区别在于HashMap是线程不安全的，而HashTable是线程安全的。 (这个可以展开很多，这里就不描述了) Hashtable 是不允许键或值为 null 的，HashMap 的键值则都可以为 null。(Hashtable在我们put空值的时候会直接抛空指针异常，但是HashMap对于null它的hash值为0) 初始化容量不同：HashMap 的初始容量为：16，Hashtable 初始容量为：11，两者的负载因子默认都是：0.75。 扩容机制不同：当现有容量大于总容量 * 负载因子时，HashMap 扩容规则为当前容量翻倍，Hashtable 扩容规则为当前容量翻倍 + 1。 迭代器不同：HashMap 中的 Iterator 迭代器是 fail-fast 的，而 Hashtable 的 Enumerator 不是 fail-fast 的 为什么HashMap允许为null而HashTable不允许呢? 主要原因是HashTable使用的是安全失败机制(fail-safe)而HashMap使用的是快速失败机制(fail-fast) 快速失败（fail—fast)：是java集合中的一种机制， 在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。 原理:迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。 Tip：这里异常的抛出条件是检测到 modCount！=expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。(有点类似于CAS) 因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的bug。 场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）算是一种安全机制吧。 安全失败（fail—safe) :在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。 原理：由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。 缺点：基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。","link":"/2020/06/21/HashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"Java连接redis的Jedis以及Springboot整合redis","text":"主要内容 Jedis的基本使用 Springboot整合redis Springboot整合redis源码分析 自定义redisTemplate类 Jedis基本使用 什么是Jedis? Jedis是 Redis 官方推荐的 java连接开发工具！ 使用Java 操作Redis 中间件！如果我们要使用java操作redis，那么一定要对Jedis 十分的熟悉！ 1.导入Jedis的依赖 去maven仓库 https://mvnrepository.com/ 中找到Jedis和fastjson的依赖 1234567891011121314&lt;dependencies&gt; &lt;!--导入Jedis的包--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--导入fastjson包方便我们下面进行测试--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.71&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.编码测试 1234567891011121314151617181920212223package com.company.test;import redis.clients.jedis.Jedis;import java.util.Set;public class TestJedis { public static void main(String[] args) { //1.新建Jedis对象 点开源码会有很多重载的方法 // 这里就使用常用的 主机地址+端口号 Jedis jedis = new Jedis(\"redis服务器的IP地址\",6379); //2.使用jedis对象进行操作 这里的方法和我们之前学习的redis基本指令完全相同 jedis.set(\"k1\",\"v1\"); jedis.set(\"k2\",\"v2\"); System.out.println(jedis.get(\"k1\")); Set&lt;String&gt; keys = jedis.keys(\"*\"); System.out.println(keys); //3.关闭连接 jedis.close(); }} 同样，我们这里也来测试一下redis的事务 123456789101112131415161718192021222324252627282930313233package com.company.test;import com.alibaba.fastjson.JSONObject;import redis.clients.jedis.Jedis;import redis.clients.jedis.Transaction;public class TestTX { public static void main(String[] args) { Jedis jedis = new Jedis(\"redis服务器的IP地址\",6379); JSONObject jsonObject = new JSONObject(); jsonObject.put(\"name\",\"chen33\"); jsonObject.put(\"age\",18); //开启事务 Transaction multi = jedis.multi(); String result = jsonObject.toJSONString(); try { multi.set(\"user1\",result); multi.set(\"user2\",result); //执行事务 multi.exec(); } catch (Exception e) { //出错则放弃事务 multi.discard(); e.printStackTrace(); } finally { System.out.println(jedis.get(\"user1\")); System.out.println(jedis.get(\"user2\")); jedis.close(); } }} Springboot整合redis1.导入redis依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 2.配置连接 application.properties下添加配置 123# 配置redis 其他配置用的时候再配spring.redis.host= redis服务器的IP地址spring.redis.port=6379 3.测试 12345678910111213141516171819202122232425262728@SpringBootTestclass Redis02SpringbootApplicationTests { //所有操作通过redisTemplate进行 @Autowired private RedisTemplate redisTemplate; @Test public void test(){ // redisTemplate 操作不同的数据类型，api和我们的指令是一样的 // opsForValue 操作字符串 类似String // opsForList 操作List 类似List // opsForSet // opsForHash // opsForZSet // opsForGeo // opsForHyperLogLog // 除了基本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务，和基本的CRUD // 获取redis的连接对象 // RedisConnection connection = redisTemplate.getConnectionFactory().getConnection(); // connection.flushDb(); // connection.flushAll(); redisTemplate.opsForValue().set(\"k1\",\"v1\"); System.out.println(redisTemplate.opsForValue().get(\"k1\")); } Springboot整合redis源码分析 RedisAutoConfiguration.class源码 1234567891011121314151617181920212223242526272829303132333435@Configuration( proxyBeanMethods = false)@ConditionalOnClass({RedisOperations.class})//绑定了RedisProperties的配置文件@EnableConfigurationProperties({RedisProperties.class})@Import({LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class})public class RedisAutoConfiguration { public RedisAutoConfiguration() { } @Bean //@ConditionalOnMissingBean是说当这个bean不存在的时候，这个类就生效 //说明我们可以自定义这个redisTemplate类 @ConditionalOnMissingBean( name = {\"redisTemplate\"} ) public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { // 默认的 RedisTemplate 没有过多的设置，但是redis对象都需要序列化！ // 两个泛型都是 Object, Object 的类型，我们需要强制转换为&lt;String, Object&gt; RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; } // 由于String是redis中最常使用的类型，所以说单独提出来了一个bean！ @Bean @ConditionalOnMissingBean public StringRedisTemplate stringRedisTemplate(RedisConnectionFactory redisConnectionFactory) throws UnknownHostException { StringRedisTemplate template = new StringRedisTemplate(); template.setConnectionFactory(redisConnectionFactory); return template; }} redisTemplate这个类非常庞大，我们主要看下它的序列化配置 自定义RedisTemplate1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.company.config;import com.fasterxml.jackson.annotation.JsonAutoDetect;import com.fasterxml.jackson.annotation.PropertyAccessor;import com.fasterxml.jackson.databind.ObjectMapper;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;@Configurationpublic class RedisConfig { // 自己定义 RedisTemplate @Bean @SuppressWarnings(\"all\")public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) { // 我们为了自己开发方便，一般直接使用 &lt;String, Object&gt; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(factory); // Json序列化配置 Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); // String 的序列化 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); // key采用String的序列化方式 template.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 template.setHashKeySerializer(stringRedisSerializer); // value序列化方式采用jackson template.setValueSerializer(jackson2JsonRedisSerializer); // hash的value序列化方式采用jackson template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; }}","link":"/2020/06/20/Java%E8%BF%9E%E6%8E%A5redis%E7%9A%84Jedis%E4%BB%A5%E5%8F%8ASpringboot%E6%95%B4%E5%90%88redis/"},{"title":"HashSet、LinkedHashSet和TreeSet源码分析","text":"基于JDK1.8的HashSet、LinkedHashSet和TreeSet源码分析 HashSet源码分析 构造方法1234567891011121314151617181920212223242526272829public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable{ //底层由HashMap组成 private transient HashMap&lt;E,Object&gt; map; //value处存放的对象 private static final Object PRESENT = new Object(); public HashSet() { map = new HashMap&lt;&gt;(); } public HashSet(Collection&lt;? extends E&gt; c) { map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); } public HashSet(int initialCapacity, float loadFactor) { map = new HashMap&lt;&gt;(initialCapacity, loadFactor); } public HashSet(int initialCapacity) { map = new HashMap&lt;&gt;(initialCapacity); } HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } 从构造方法中我们可以看到，HashSet底层实际是由HashMap组成的。 add方法123public boolean add(E e) { return map.put(e, PRESENT)==null;} 可以看到add()方法是把set中的元素放到了key处，value处是一个空的Object对象 remove方法123public boolean remove(Object o) { return map.remove(o)==PRESENT;} 同样是HashMap的remove方法。 HashSet总结到这里我们就知道了: HashSet底层由HashMap组成 set中的数据存放在key处，value处存放的是一个空的Object对象 LinkedHashSet源码分析 从类结构图我们知道和LinkedHashMap继承自HashMap一样，LinkedHashSet也继承自HashSet 构造方法1234567891011121314151617181920public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable { public LinkedHashSet(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor, true); } public LinkedHashSet(int initialCapacity) { super(initialCapacity, .75f, true); } public LinkedHashSet() { super(16, .75f, true); } public LinkedHashSet(Collection&lt;? extends E&gt; c) { super(Math.max(2*c.size(), 11), .75f, true); addAll(c); } 从上面的构造方法中我们可以知道，实际上都是调用的HashSet中的构造方法: 123HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } 总结所以从上面我们就知道了： LikedHashSet继承自HashSet,并且底层是LinkedHashMap构成。 LinkedHashSet没有add和remove方法，说明使用的是继承自HashSet的方法。 TreeSet源码分析 知道了前两个，这个TreeSet我不用说，估计各位也能猜到了，它底层是TreeMap并且value处存放的也是空的Object对象。我们照例贴下它的源代码看下是不是吧。 1234567891011121314151617181920212223242526272829public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable{ private transient NavigableMap&lt;E,Object&gt; m; //value处存放的object对象 private static final Object PRESENT = new Object(); TreeSet(NavigableMap&lt;E,Object&gt; m) { this.m = m; } public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } public TreeSet(Comparator&lt;? super E&gt; comparator) { this(new TreeMap&lt;&gt;(comparator)); } public TreeSet(Collection&lt;? extends E&gt; c) { this(); addAll(c); } public TreeSet(SortedSet&lt;E&gt; s) { this(s.comparator()); addAll(s); } 从上面的构造方法中可以看到我们的猜想是没有错的，TreeSet底层由TreeMap构成。 add()方法123public boolean add(E e) { return m.put(e, PRESENT)==null;} remove()方法123public boolean remove(Object o) { return m.remove(o)==PRESENT;} 总结 TreeSet底层是由TreeMap组成 和HashSet一样，value处存放的是空的object对象。 综上，我们就知道了： HashSet、LinkedHashSet和TreeSet底层都是由对应的HashMap、LinkedHashMap和TreeMap组成 Set中的元素存放在key处，value处存放的都是空的Object对象。","link":"/2020/06/21/HashSet%E3%80%81LinkedHashSet%E5%92%8CTreeSet%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"LinkedList源码分析","text":"基于JDK1.8的LinkedList源码分析 之前我们学习ArrayList源码时知道,在指定位置进行插入和删除时需要进行数组的拷贝，使得时间复杂度降为O(n)。今天我们来看看LinkedList的源码。 Node节点我们知道LinkedList底层是一个双向链表，所以我们先看一下他的Node节点。 12345678910private static class Node&lt;E&gt; { E item; //元素值 Node&lt;E&gt; next; //后继指针 Node&lt;E&gt; prev; //前驱指针 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) { this.item = element; this.next = next; this.prev = prev; }} 可以看到Node节点是一个标准的双向链表的节点，包含节点元素、前驱指针和后继指针。 构造方法1234567891011121314151617public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable{ transient int size = 0; //链表大小 transient Node&lt;E&gt; first; //头结点 transient Node&lt;E&gt; last; //尾结点 //1.无参构造 public LinkedList() { } //2.从现有集合构造 public LinkedList(Collection&lt;? extends E&gt; c) { this(); addAll(c); } add()方法123456789101112131415//1.添加到链表尾部public boolean add(E e) { linkLast(e); return true;}//2.指定索引处添加public void add(int index, E element) { checkPositionIndex(index); //索引检查 if (index == size) linkLast(element); //如果index和size相等则是直接添加到链表尾部 else linkBefore(element, node(index));} 首先我们来看普通add(E e)方法，它用来将元素添加到链表尾部，可以看到实际上是调用的linkLast方法。 123456789101112void linkLast(E e) { final Node&lt;E&gt; l = last; //链表尾结点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; //链表为空 if (l == null) first = newNode; else //链表非空 l.next = newNode; size++; modCount++;} 然后我们看看在指定位置添加新的节点add(int index, E element) 可以看到，实际在链表中添加节点时是调用的linkBefore()方法。 我们先来看linkBefore内层的node方法，其实很简单。 123456789101112131415Node&lt;E&gt; node(int index) { //index&lt;链表长度的一半 则从前往后找到index节点 if (index &lt; (size &gt;&gt; 1)) { Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; //否则从后往前找到index节点 } else { Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; }} 之后我们来看linkBefore()方法,其实也很简单 123456789101112131415161718//linkBefore(element, node(index));void linkBefore(E e, Node&lt;E&gt; succ) { // assert succ != null; final Node&lt;E&gt; pred = succ.prev; // 新结点指向前后结点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); //后结点的前驱指针指向新结点 succ.prev = newNode; //前结点的后继节点指向新结点 if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++;} remove()方法12345678910111213141516171819202122232425//1. 从指定索引处删除元素public E remove(int index) { checkElementIndex(index); return unlink(node(index)); //node()函数上面讲过了}//2. 删除执行元素public boolean remove(Object o) { if (o == null) { for (Node&lt;E&gt; x = first; x != null; x = x.next) { if (x.item == null) { unlink(x); return true; } } } else { for (Node&lt;E&gt; x = first; x != null; x = x.next) { if (o.equals(x.item)) { unlink(x); return true; } } } return false;} 从上面可以看到，无论是在指定索引处删除元素还是直接删除指定元素，都是在找到要删除的元素所在节点后调用的unlink()方法，所以我们直接看unlink方法 12345678910111213141516171819202122232425E unlink(Node&lt;E&gt; x) { // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) { first = next; } else { prev.next = next; x.prev = null; } if (next == null) { last = prev; } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element;} 整个过程如下图所示 get()方法1234public E get(int index) { checkElementIndex(index); //索引检查 return node(index).item; //node()方法上面已经讲过了} 可以看到get()方法非常简单，首先检查索引，索引有效则直接根据index的大小，决定从前往后遍历还是从后往前遍历，最后返回元素即可。 set()方法1234567public E set(int index, E element) { checkElementIndex(index); //索引检查 Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal;} set方法也是非常简单,和get方法一样先检查索引，以及遍历到index所在位置。之后直接进行值的替换即可。","link":"/2020/06/21/LinkedList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"List集合的简单使用","text":"","link":"/2020/06/21/List%E9%9B%86%E5%90%88%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"title":"Map集合的简单使用","text":"","link":"/2020/06/21/Map%E9%9B%86%E5%90%88%E7%9A%84%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"},{"title":"CentOS7环境下安装redis","text":"CentOS7环境下redis的安装教程 软件说明 CentOS 7 64位 redis5.0.8 Xshell 6 Xftp 6 安装过程1.使用远程连接工具将redis的压缩包上传到linux主机的/opt文件夹下 2.使用命令tar -zxvf redis-5.0.8.tar.gz将压缩包文件进行解压 进入到解压后的文件夹，可以看到redis的配置文件 3.查看是否安装gcc编译器，没有则直接安装 123gcc -v # 查看是否安装gcc编译器yum install gcc-c++ # 没有安装的话直接进行安装 安装完成后，检查是否安装成功 4.进入到解压后的redis文件中进行编译安装 123cd redis-5.0.8/makemake install 安装完成后如下图所示： 5.redis的默认安装路径/usr/local/bin 6.将redis默认配置文件复制到默认安装路径下 7.修改redis为默认后台启动 1vim cconfig/redis.conf 8.通过制定redis文件启动redis服务 9.使用redis-cli进行连接测试 10.如何关闭redis服务 到这里redis已经安装完成，之后要设置linux集群则可以在redis的配置文件redis.conf中进行修改。","link":"/2020/06/24/CentOS7%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85redis/"},{"title":"TreeMap源码分析","text":"基于JDK1.8的TreeMap源码分析 从上面的类结构图可以看到TreeMap实现了NavigableMap接⼝，⽽NavigableMap接⼝继承SortedMap接⼝，使得我们的TreeMap是有序的！ TreeMap底层是红⿊树，它⽅法的时间复杂度都不会太⾼:log(n)~ 使⽤Comparator或者Comparable来⽐较key是否相等与排序的问题 Entry节点1234567891011121314151617static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK; /** * Make a new cell with given key, value, and parent, and with * {@code null} child links, and BLACK color. */ Entry(K key, V value, Entry&lt;K,V&gt; parent) { this.key = key; this.value = value; this.parent = parent; } 我们知道TreeMap底层是由红黑树构成，从上面可以看到是一个标准的红黑树节点并且有指向父节点的指针。 构造方法123456789101112131415161718192021222324252627282930313233343536373839public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable, java.io.Serializable{ //comparator类用来对key进行比较排序 //为null则表示是自然排序 private final Comparator&lt;? super K&gt; comparator; //根节点 private transient Entry&lt;K,V&gt; root; //节点数量 private transient int size = 0; //1.无参构造，自然排序 public TreeMap() { comparator = null; } //2.使用指定比较器排序 public TreeMap(Comparator&lt;? super K&gt; comparator) { this.comparator = comparator; } //3.从现有Map中构造TreeMap并进行自然排序 public TreeMap(Map&lt;? extends K, ? extends V&gt; m) { comparator = null; putAll(m); } //4. 从已排序map中构建TreeMap并使用m的比较器排序 public TreeMap(SortedMap&lt;K, ? extends V&gt; m) { comparator = m.comparator(); try { buildFromSorted(m.size(), m.entrySet().iterator(), null, null); } catch (java.io.IOException cannotHappen) { } catch (ClassNotFoundException cannotHappen) { } } put()方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public V put(K key, V value) { Entry&lt;K,V&gt; t = root; //根节点为空,则 if (t == null) { compare(key, key); // type (and possibly null) check //构建新结点作为根节点 root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; } //到这里说明根节点不为空 int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) { //如果定义了比较器 do { parent = t; //使用自定义比较器的compare方法进行比较 cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } else { //如果没有自定义比较器 if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") //这里要求key不能为空,且必须实现了Comparable接口 Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do { parent = t; //使用Comparable接口的compareTo方法进行比较 cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } //创建新结点 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); //根据比较结果决定新结点为父节点的左孩子还是右孩子 if (cmp &lt; 0) parent.left = e; else parent.right = e; //新插入节点后调整红黑树使其重新平衡 fixAfterInsertion(e); size++; modCount++; return null;} 这个fixAfterInsertion(e)函数的代码就不在这里放了，其实就是红黑树的自平衡调整。参考我之前的文章二叉排序树、平衡二叉树、红黑树、B树、B+树和它的源代码就很容易理解。 从put方法中我们就可以看到TreeMap的排序方式: 如果在构造⽅法中传递了Comparator对象，那么就会以Comparator指定的方式进行排序。否则，则使⽤Comparable的compareTo(T o)⽅法来进行自然排序。 使⽤Comparable的compareTo(T o)⽅法比较时要求key不能为null，且key必须实现Comparable接口 即使是传⼊了Comparator对象，不⽤ compareTo(T o) ⽅法来⽐较，key也是不能为null的。 这里我们测试一下: 1234567891011121314import java.util.Map;import java.util.TreeMap;public class TestTreeMap { public static void main(String[] args) { Map&lt;String,String&gt; map = new TreeMap&lt;&gt;(); map.put(\"bcoderchen33\",\"haha\"); map.put(\"bcoderchen22\",\"haha\"); map.put(\"ccoderchen33\",\"haha\"); map.put(\"acoderchen33\",\"haha\"); for(String key:map.keySet()){ System.out.println(key+\"--\"+map.get(key)); } }} 可以看到运行结果如下，key值进行了自然排序。这是因为String类已经实现了我们的Coparable接口。当然我们常用的Integer等基本类型也都实现了Comparable接口 1234acoderchen33--hahabcoderchen22--hahabcoderchen33--hahaccoderchen33--haha 如果我们要存储自定义对象，并按照指定方式进行排序，则可以有两种方法: 自定义对象实现Comparale接口，重写compareTo方法 自定义对象不需要实现Comparale接口，只需要在构造时传入自定义的comparator比较器即可。 自定义Student对象实现Comparable接口 12345678910111213141516171819202122232425package com.company;import java.util.Objects;//不实现comparable接口，会报类型转换异常//java.lang.ClassCastException: com.company.Student cannot be cast to java.lang.Comparablepublic class Student implements Comparable{ private String name; private int age; //构造器 //set、get方法 //hashcode和equals方法 //由于篇幅原因，这里就不展示了 //先按姓名排序，姓名相同再按年龄排序 @Override public int compareTo(Object o) { Student student = (Student) o; int num = this.name.hashCode() - student.getName().hashCode(); return num==0 ? this.age-student.getAge() : num; }} 测试类 123456789101112public class TestTreeMap { public static void main(String[] args) { TreeMap&lt;Student, String&gt; map = new TreeMap&lt;&gt;(); map.put(new Student(\"coderchen33\", 18), \"3318\"); map.put(new Student(\"coderchen22\", 20), \"2220\"); map.put(new Student(\"coderchen11\", 19), \"1119\"); map.put(new Student(\"coderchen22\", 19), \"2219\"); for (Student s : map.keySet()) { System.out.println(s + \"---\" + map.get(s)); } }} 运行结果如下，可以看到自定义的compareTo方法生效 1234Student{name='coderchen11', age=19}---1119Student{name='coderchen22', age=19}---2219Student{name='coderchen22', age=20}---2220Student{name='coderchen33', age=18}---3318 当然，更加常用的是第二种，Student对象不需要实现Comparable接口，只需要我们传入自定义的comparator比较器即可 1234567891011121314151617package com.company;import java.util.TreeMap;public class TestTreeMap { public static void main(String[] args) { TreeMap&lt;Student, String&gt; map = new TreeMap&lt;Student,String&gt;((o1,o2)-&gt;{ int num = o1.getName().hashCode() - o2.getName().hashCode(); return num==0 ? o1.getAge()-o2.getAge() : num; }); map.put(new Student(\"coderchen33\", 18), \"3318\"); map.put(new Student(\"coderchen22\", 20), \"2220\"); map.put(new Student(\"coderchen11\", 19), \"1119\"); map.put(new Student(\"coderchen22\", 19), \"2219\"); for (Student s : map.keySet()) { System.out.println(s + \"---\" + map.get(s)); } }} 运行结果和上面相同 1234Student{name='coderchen11', age=19}---1119Student{name='coderchen22', age=19}---2219Student{name='coderchen22', age=20}---2220Student{name='coderchen33', age=18}---3318 get()方法看过了put方法，get方法也就比较简单了。可以看到get方法内部是调用了getEntry方法。 1234public V get(Object key) { Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);} 从getEntry方法中可以看到，和put方法一样: 有comparator比较器时，直接使用比较器进行比较查找 否则使用Comparable的compareTo方法进行比较查找 1234567891011121314151617181920212223final Entry&lt;K,V&gt; getEntry(Object key) { // Offload comparator-based version for sake of performance //比较器不为空，通过比较器进行比较查找 //getEntryUsingComparator(key)的代码就不放了，自己一看就明白了 if (comparator != null) return getEntryUsingComparator(key); //comparator为空，使用Comparable进行比较查找 if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; while (p != null) { int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; } return null;} remove()方法主要过程就3步： 找到要删除的节点 删除节点 红黑树保持自身平衡 123456789public V remove(Object key) { Entry&lt;K,V&gt; p = getEntry(key); if (p == null) return null; V oldValue = p.value; deleteEntry(p); return oldValue;} TreeMap总结 TreeMap底层由红黑树组成，时间复杂度可以保证为O(logn)。 TreeMap可以实现Map集合的key有序性: 如果在构造方法中传入了comparator比较器，则使用comparator的compare方法进行指定方式排序。 如果没有传入comparator比较器，则使用Comparable的compareTo方法进行自然排序 无论按哪种方式排序，key值都不能为null TreeMap⾮同步的，想要同步可以⽤Collections来进⾏封装","link":"/2020/06/21/TreeMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"LinkedHashMap源码分析","text":"基于JDK1.8的LinkedHashMap的源码解析 我们知道LinkedHashMap与HashMap的最大不同点在于，它可以按照我们插入节点的顺序进行遍历?那么它底层是如何做的呢? 阅读前建议先看我之前的HashMap的源码文章的解析。看过了HashMap的底层源码后，LinkedHashMap的源码就比较简单了。 首先我们通过类结构图可以看到LinkedHashMap继承自HashMap。并且LinkedHashMap底层是在HashMap的基础上维护了一个双向链表，来保证它的有序性。 Node节点123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); }} 可以看到LinkedHashMap的节点Entry&lt;K,V&gt;继承自HashMap.Node&lt;K,V&gt;，在其基础上扩展了成了一个双向链表的节点。 构造方法123456789101112131415161718192021222324252627282930313233343536373839404142434445public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt;{ //双向链表的头结点 transient LinkedHashMap.Entry&lt;K,V&gt; head; //双向链表的尾结点 transient LinkedHashMap.Entry&lt;K,V&gt; tail; //遍历时的顺序 //false(默认)则按节点插入的顺序进行遍历 //true则按访问的顺序进行遍历 final boolean accessOrder; //1. 指定初始容量和负载因子构造方法 public LinkedHashMap(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor); accessOrder = false; } //2.只指明初始容量的构造方法 public LinkedHashMap(int initialCapacity) { super(initialCapacity); accessOrder = false; } //3. 无参构造 public LinkedHashMap() { super(); accessOrder = false; } //4.从现有Map集合中构造 public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) { super(); accessOrder = false; putMapEntries(m, false); } //5.指明遍历顺序的构造方法 public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } 可以看到LinkedHashMap的5个构造方法中，都是调用的super()方法，即与HashMap的构造方法相同。唯一区别在于前四个中的accessOrder是false，而第5个是可以自己定义。当然如果使用第5个构造方法时，还把accessOrder定义成false的话就和第一个构造方法没有区别。所以，通常使用第5个构造方法时则是需要accessOrder是true的情形。 那么这个accessOrder究竟用来做什么呢？实际上它如果为true的话可以用来实现我们的LRU算法。这里先不解释什么是LRU算法，我们先来用一用看看，就知道是什么意思了 12345678910111213141516171819202122232425262728293031323334353637383940package com.company;import java.util.Iterator;import java.util.LinkedHashMap;import java.util.Map;import java.util.Set;/** * Created by chen on 2020/6/22 */public class TestAccessOrder { public static void main(String[] args) { Map&lt;String,String&gt; map = new LinkedHashMap&lt;&gt;(); map.put(\"1\",\"a\"); map.put(\"2\",\"b\"); map.put(\"3\",\"c\"); map.put(\"4\",\"d\"); Iterator&lt;Map.Entry&lt;String,String&gt;&gt; iterator = map.entrySet().iterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } System.out.println(\"以下是accessOrder=true的情况:\"); map = new LinkedHashMap&lt;String, String&gt;(10, 0.75f, true); map.put(\"1\", \"a\"); map.put(\"2\", \"b\"); map.put(\"3\", \"c\"); map.put(\"4\", \"d\"); //1-&gt;2-&gt;3-&gt;4 map.get(\"2\");//2移动到了内部的链表末尾 1-&gt;3-&gt;4-&gt;2 map.get(\"4\");//4调整至末尾 1-&gt;3-&gt;2-&gt;4 map.put(\"3\", \"e\");//3调整至末尾 1-&gt;2-&gt;4-&gt;3 map.put(null, null);//插入两个新的节点 null 1-&gt;2-&gt;4-&gt;3-&gt;null map.put(\"5\", null);//5 1-&gt;2-&gt;4-&gt;3-&gt;null-&gt;5 iterator = map.entrySet().iterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } }} 输出结果如下 12345678910111=a2=b3=c4=d以下是accessOrder=true的情况:1=a2=b4=d3=enull=null5=null 可以看到，当accessOrder是false时(默认情况)上面的1234即是我们插入时的顺序。而当accessOrder是true时，我们无论是调用get()方法还是put()方法时，都会将我们操作的元素移动到链表的末尾。 put()方法原本我是想要找put⽅法，看看是怎么实现的，后来没找着，就奇了个怪~ 再顿了⼀下，原来LinkedHashMap和HashMap的put⽅法是⼀样的！ LinkedHashMap继承自HashMap，LinkedHashMap没有重写HashMap的put⽅法。所以，LinkedHashMap的put⽅法和HashMap是⼀样的。 但是我们知道，在HashMap中put元素时没有直接把元素放到末尾。实际上，虽然LinkedList调用的是HashMap的put方法但是它重写了HashMap的put方法中的newNode方法，所以调用HashMap的put方法过程中，回调了LinkedList中重写的newNode方法。 12345678910111213141516171819//在构建新节点时，构建的是`LinkedHashMap.Entry` 不再是`Node`.Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) { LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;}//在这里可以看到LinkedList在put时将新增的节点，连接在链表的尾部private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) { LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; //集合之前是空的 if (last == null) head = p; else {//将新节点连接在链表的尾部 p.before = last; last.after = p; }} 另外，在HashMap源码分析时我们也提到了插入完成后对于LinkedHashMap还会执行一个函数afterNodeInsertion(evict)。 这个函数在HashMap中是空函数而在LinkedHashMap中进行了实现。 12345678910111213void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; //根据条件判断是否溢出 最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); }}// 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return false;} 这个方法我们目前只需要知道： 它是在HashMap中的put方法完成后被调用的 HashMap中这个方法是一个空实现，而在LinkedHashMap中进行了实现 具体如何发挥作用参考下文的LinkedHashMap中的LRU算法实现一节。 get()方法123456789public V get(Object key) { Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; //accessOrder为true时调用afterNodeAccess(e)方法 if (accessOrder) afterNodeAccess(e); return e.value;} 12345678910111213141516171819202122232425//下面的操作也是把节点移动到链表尾部void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; }} 同样对于上面这个方法，我们也是只需要知道它是在进行get操作后，如果accessOrder为true，它会被调用。 LinkedHashMap中的LRU算法实现LinkedHashMap靠重写这3个方法就完成了核心功能的实现。 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeInsertion(boolean evict) {}void afterNodeRemoval(Node&lt;K,V&gt; p) {}void afterNodeAccess(Node&lt;K,V&gt; p) {} 这里我们再贴一下这三个方法，好好分析一下它是如何实现我们的LRU算法的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879//首先这个方法上面已经提到了，是在put方法完成后调用的。//我们看下它的判断条件//1. evict在HashMap中传入的一直是true//2. 链表不为空//3. 只要我们的removeEldestEntry方法返回true就会执行删除第一个结点的操作,也就是我们的淘汰结点策略//但是它默认一直返回false，因此如果我们只要重写removeEldestEntry方法，//即可使其返回true执行下面的删除第一个节点的操作。void afterNodeInsertion(boolean evict) { LinkedHashMap.Entry&lt;K,V&gt; first; //根据条件判断是否溢出 最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); }}// 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return false;}//--------------------------------------------// removeNode其实是HashMap中删除节点的操作，删除操作很简单，这里就不说了//和上面一样，删除操作完成后，最后会调用afterNodeRemoval方法，这个同样在HashMap中是空实现//而在LinkedHashMap中也很简单，就是链表释放节点的操作void afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b; }//--------------------------------------------------------------------//这个方法是在get方法完成后调用的//首先我们看到需要accessOrder为true才会执行if中的方法//if中的方法其实很简单，就是把我们访问的节点放到链表尾部//步骤就是先删除原来的节点，后又把节点插入到尾部void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; //删除节点p的操作 p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; //将节点p重新插入链表尾部 if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } } 到这里，大家就清楚了LinkedHashMap中是如何实现LRU算法的 对于put操作，如果内存没有满，我们直接在链表末尾添加。如果内存满了，会将我们链表的第一个节点删除(因为第一个是最近最少使用的)，并且在链表末尾添加我们的新结点 对于get操作，会首先将我们的节点删除，然后重新将节点添加到链表末尾。这样就是最近访问过的节点。 另外，别忘了，我们上面的流程是在accessOrder为true，并且重写了removeEldestEntry方法的流程。 这里我们重写removeEldestEntry方法来实现一把自定义缓存策略的LRU缓存。比如我们可以根据节点数量判断是否移除最近最少被访问的节点，或者根据节点的存活时间判断是否移除该节点等。 这里所实现的缓存是基于判断节点数量是否超限的策略。在构造缓存对象时，传入最大节点数。当插入的节点数超过最大节点数时，移除最近最少被访问的节点。实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637import java.util.LinkedHashMap;import java.util.Map;public class LRUCache&lt;K,V&gt; extends LinkedHashMap&lt;K,V&gt; { private static final int MAX_MODE_NUM = 100; private final int CACHE_SIZE; // 默认100个 public LRUCache(){ this(MAX_MODE_NUM); } /** * 传递进来最多能缓存多少数据 * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize){ super(cacheSize,0.75f,true); this.CACHE_SIZE = cacheSize; } public V putInCache(K key,V val){ return put(key,val); } public V getFromCache(K key){ return get(key); } @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { //当map中的数据量大于指定的缓存个数的时候，就自动删除最老数据 return size()&gt;CACHE_SIZE; }} 测试一下是否正确 1234567891011121314151617public static void main(String[] args) { LRUCache&lt;String,Object&gt; cache = new LRUCache&lt;&gt;(4); cache.putInCache(\"1\",1); cache.putInCache(\"2\",2); cache.putInCache(\"3\",3); cache.putInCache(\"4\",4); System.out.println(cache); //{1=1, 2=2, 3=3, 4=4} cache.getFromCache(\"1\"); System.out.println(cache); //{2=2, 3=3, 4=4, 1=1} cache.getFromCache(\"3\"); System.out.println(cache); //{2=2, 4=4, 1=1, 3=3} cache.putInCache(\"5\",5); System.out.println(cache); //{4=4, 1=1, 3=3, 5=5} } 可以看到运行结果完全正确，实现了LRU算法 遍历的方法entrySet()方法被重写了 12345678910public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() { Set&lt;Map.Entry&lt;K,V&gt;&gt; es; //返回LinkedEntrySet return (es = entrySet) == null ? (entrySet = new LinkedEntrySet()) : es;}final class LinkedEntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; { public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() { return new LinkedEntryIterator(); }} 最终的EntryIterator: 1234final class LinkedEntryIterator extends LinkedHashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; { public final Map.Entry&lt;K,V&gt; next() { return nextNode(); } } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051abstract class LinkedHashIterator { //下一个节点 LinkedHashMap.Entry&lt;K,V&gt; next; //当前节点 LinkedHashMap.Entry&lt;K,V&gt; current; int expectedModCount; LinkedHashIterator() { //初始化时，next 为 LinkedHashMap内部维护的双向链表的头 next = head; //记录当前modCount，以满足fail-fast expectedModCount = modCount; //当前节点为null current = null; } //判断是否还有next public final boolean hasNext() { //就是判断next是否为null，默认next是head 表头 return next != null; } //nextNode() 就是迭代器里的next()方法 。 //该方法的实现可以看出，迭代LinkedHashMap，就是从内部维护的双链表的表头开始循环输出。 final LinkedHashMap.Entry&lt;K,V&gt; nextNode() { //记录要返回的e。 LinkedHashMap.Entry&lt;K,V&gt; e = next; //判断fail-fast if (modCount != expectedModCount) throw new ConcurrentModificationException(); //如果要返回的节点是null，异常 if (e == null) throw new NoSuchElementException(); //更新当前节点为e current = e; //更新下一个节点是e的后置节点 next = e.after; //返回e return e; } //删除方法 最终还是调用了HashMap的removeNode方法 public final void remove() { Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; }} 值得注意的就是：nextNode() 就是迭代器里的next()方法 。该方法的实现可以看出，迭代LinkedHashMap，实际上就是从内部维护的双链表的表头开始循环输出。而双链表节点的顺序在LinkedHashMap的增、删、改、查时都会更新。以满足按照插入顺序输出，还是访问顺序输出。 总结 LinkedHashMap继承自HashMap,并且在HashMap的基础上维护了一个双向链表。 accessOrder默认为false，表示我们可以按插入顺序进行遍历，通过设置accessOrder为true，并且重写了几个方法可以实现我们的LRU算法。 accessOrder为true可以使得LinkedHashMap的增改查都会使被访问的元素移动到链表的尾部。(注意LinkedHashMap没有重写HashMap的put和remove方法，只是重写了put中的newNode()方法) 使用迭代器遍历LinkedHashMap，实际上就是从内部维护的双向链表的表头开始循环输出。","link":"/2020/06/21/LinkedHashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"redis发布订阅Pub/Sub","text":"主要内容 redis的pub/sub模型和原理 什么是发布订阅Redis发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。Redis 客户端可以订阅任意数量的频道。 下图展示了publish1 ，频道 channel1 ， 以及订阅这个频道的三个客户端 —— client1 、 client2 和 client3之间的关系： 发布者通过 PUBLISH 命令发送给频道 channel1 新的消息， 这个消息就会被发送给订阅它的三个客户端 使用从上面我们知道发布订阅的关键是：发布者、频道、订阅者,我们看看redis的发布订阅是如何使用的。 下面是所有关于pub/sub的所有命令： SUBSCRIBE 订阅频道 UNSUBSCRIBE 取消订阅 PUBLISH 发布消息 PSUBSCRIBE 订阅模式匹配的频道 PUNSUBSCRIBE 取消订阅模式匹配的所有频道 PUBSUB 列出当前活跃频道 使用其实非常简单： 订阅端订阅频道后自动进行监听 (这里我们分别设置两名订阅者订阅coderchen33频道) 发送端发送消息 (可以看到发送给的人数是所有订阅人数) 订阅端接收到消息（发送端发送消息后，订阅该频道的订阅者就会接收到消息） 使用场景 1.实时消息系统！ 2.实时聊天！（频道当做聊天室，将信息回显给所有人即可！） 3.订阅，关注系统都是可以的！ 稍微复杂的场景我们就会使用消息中间件MQ(RocketMQ、RabbitMQ、Kafka等) 底层实现原理","link":"/2020/06/25/redis%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85/"},{"title":"ArrayList源码分析","text":"基于JDK1.8的ArrayList源码分析 今天，我们来看看ArrayList的源码，它可以说是我们⽤得⾮常⾮常多的⼀个集合，所以了解他的源码还是十分有必要的。 构造函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable{ //默认初始容量10 private static final int DEFAULT_CAPACITY = 10; //指明初始容量为0时的数组 private static final Object[] EMPTY_ELEMENTDATA = {}; //未指明容量时的初始数组 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; //数据真正存放的地方，这个对象不参与序列化 transient Object[] elementData; //ArrayList的容量 private int size; //1.有参构造 //指明初始容量大于0时，构造初始容量大小的数组 //指明初始容量等于0时，构造空数组 //小于0时直接抛异常 public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+initialCapacity); } } //2.无参构造 //未指明参数时，构造初始容量为10的Object数组 public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } //3.从已有Collection集合中构造 public ArrayList(Collection&lt;? extends E&gt; c) { elementData = c.toArray(); //size&gt;0 if ((size = elementData.length) != 0) { //如果c.toArray()返回的数组类型不是Object[]， //则利用 Arrays.copyOf(); 来构造一个大小为 size 的 Object[] 数组 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // size等于0 直接使用空对象 this.elementData = EMPTY_ELEMENTDATA; } } 从上面我们可以看到，除了指明初始容量时或者使用一个非空集合构造ArrayList时ArrayList会构建一个初始容量的数组外，无论是无参构造还是指明初始容量为0的构造都会是一个空的Object数组，那么我们的数据放在哪里呢? 实际上,ArrayList会在我们第一次添加元素时进行数组的扩容。接下来我们就看看是如何添加元素，以及如何扩容的。 add()方法ArrayList中add方法是重载的: 普通add方法，将元素添加到数组尾部 在指定索引处添加元素 1234567891011121314151617181920//1.普通add方法，将元素添加到数组尾部public boolean add(E e) { ensureCapacityInternal(size + 1); // 判断添加一个元素时即size+1是否需要扩容 elementData[size++] = e; return true; }//2. 在指定索引处添加元素public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //将elementData从index开始到数组末尾的元素拷贝到index+1到数组末尾 System.arraycopy(elementData, index, elementData, index + 1, size - index); //index处加入元素 elementData[index] = element; size++;} add方法其实很简单，主要就是我们的ensureCapacityInternal方法，也就是ArrayList的扩容机制。我们点进源码看一下。 123private void ensureCapacityInternal(int minCapacity) { ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));} 可以看到ensureCapacityInternal()方法中调用了ensureExplicitCapacity()和calculateCapacity()两个方法我们先看里面的calculateCapacity()方法 12345678private static int calculateCapacity(Object[] elementData, int minCapacity) { //第一次添加元素,返回minCapacity为10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { return Math.max(DEFAULT_CAPACITY, minCapacity); } //否则直接返回传入的参数 即size+1 return minCapacity;} 再来看我们外层的ensureExplicitCapacity()方法，这里上面函数返回的结果是这个函数的参数。 1234567private void ensureExplicitCapacity(int minCapacity) { modCount++; //如果所需最小数组容量大于数组长度则调用grow方法进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);} 最后看看我们的grow方法是如何进行扩容的。 1234567891011121314private void grow(int minCapacity) { int oldCapacity = elementData.length; //扩容为原来的1.5倍 //右移相当于除以2 10+10&gt;&gt;2 = 15s int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //扩容后还是小于minCapacity则直接扩容为minCapacity if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //数组容量最大不能超过Integer.MAX_VALUE if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); //扩容后，将elementData对象指向新的数组对象 elementData = Arrays.copyOf(elementData, newCapacity); } 这里我们对ArrayList的add(E e)进行一下总结: 首先add方法时会去检查我们的size+1是否需要进行扩容 不需要扩容，则直接在末尾添加元素，需要扩容则调用grow方法进行扩容。 grow方法会将数组长度扩充为原来的1.5倍，如果扩充后仍然小于mincapacity则直接扩充为mincapacity大小。这里尤其需要注意，如果是无参构造，则会直接扩容为初始容量10。 扩容后将elementData对象指向扩容后的新的数组对象。 get()方法看完了add方法，get方法就十分简单了。首先检查索引是否合法，合法则直接返回索引处的值即可。 1234public E get(int index) { rangeCheck(index); //检查索引是否合法 return elementData(index);} 1234private void rangeCheck(int index) { if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));} set()方法set方法同样也十分简单。首先检查索引是否合法，合法则替换旧值，并将旧值返回即可 1234567public E set(int index, E element) { rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;} remove()方法123456789101112131415161718192021222324252627282930313233// 1.删除指定索引处的元素值public E remove(int index) { rangeCheck(index); //索引检查 modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; //数组元素移动 if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;}//2.删除指定元素值//需要从头开始遍历数组，时间复杂度O(n)public boolean remove(Object o) { //说明ArrayList可以存放null值。 if (o == null) { for (int index = 0; index &lt; size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false;} ArrayList细节再说明 ArrayList是基于动态数组实现的，在指定索引处增删，需要数组的拷⻉复制(时间复杂度O(n))。 ArrayList的默认初始化容量是10，每次扩容时候增加原先容量的⼀半，也就是变为原来的1.5倍 删除元素时不会减少容量，若希望减少容量则调⽤trimToSize() 它不是线程安全的。它能存放null值。","link":"/2020/06/21/ArrayList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"title":"redis配置文件redis.conf详解","text":"redis.conf中的常用的也是比较重要的配置项进行了说明 单位 redis配置文件对unit单位大小写不敏感 引用 可以引用其他配置文件，和我们Java中import导包是一个概念 网络 12345bind 127.0.0.1 # 绑定的ip地址,redis可以监听我们配置的一个或多个ip地址protected-mode yes # 保护模式，默认是yesport 6379 # 端口设置 通用 GENERAL 123456789101112131415161718daemonize yes # 以守护进程的方式运行，默认是 no，我们需要自己开启为yespidfile /var/run/redis_6379.pid # 如果以后台的方式运行，我们就需要指定一个 pid 文件！# redis日志级别# debug (a lot of information, useful for development/testing) 开发测试环境# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably) 生产环境# warning (only very important / critical messages are logged)loglevel notice # 日志级别是notice级别logfile \"\" # 日志的文件位置名databases 16 # 数据库的数量，默认是 16 个数据库，编号0-15 always-show-logo yes # 是否总是显示LOGO 快照 持久化，在规定的时间内，执行了多少次操作，则会持久化到文件 我们知道redis 是内存数据库，如果没有持久化，那么数据断电及失！ 1234567891011121314151617# 如果900s内，如果至少有一个1 key进行了修改，我们就进行持久化操作save 900 1# 如果300s内，至少10 key进行了修改，我们及进行持久化操作save 300 10# 如果60s内，如果至少10000 key进行了修改，我们及进行持久化操作save 60 10000# 我们之后学习持久化，会自己定义这个测试！stop-writes-on-bgsave-error yes # 持久化如果出错，是否还需要继续工作！rdbcompression yes # 是否压缩 rdb 文件，需要消耗一些cpu资源！rdbchecksum yes # 保存rdb文件的时候，进行错误的检查校验！dir ./ # rdb 文件保存的目录！ 复制 REPLICATION 我们之后讲解主从复制的时候再进行讲解配置 安全 SECURITY 可以在这里设置redis的密码，默认是没有密码！ 12345678910111213141516127.0.0.1:6379&gt; pingPONG127.0.0.1:6379&gt; config get requirepass # 获取redis的密码1) \"requirepass\"2) \"\"127.0.0.1:6379&gt; config set requirepass \"123456\" # 设置redis的密码OK127.0.0.1:6379&gt; config get requirepass # 发现所有的命令都没有权限了(error) NOAUTH Authentication required.127.0.0.1:6379&gt; ping(error) NOAUTH Authentication required.127.0.0.1:6379&gt; auth 123456 # 使用密码进行登录！OK127.0.0.1:6379&gt; config get requirepass1) \"requirepass\"2) \"123456\" 限制 CLIENTS 1234567891011maxclients 10000 # 设置能连接上redis的最大客户端的数量maxmemory &lt;bytes&gt; # redis 配置最大的内存容量maxmemory-policy noeviction # 内存到达上限之后的处理策略 1、volatile-lru：只对设置了过期时间的key进行LRU（默认值） 2、allkeys-lru ： 删除lru算法的key 3、volatile-random：随机删除即将过期key 4、allkeys-random：随机删除 5、volatile-ttl ： 删除即将过期的 6、noeviction ： 永不过期，返回错误 APPEND ONLY 模式 aof配置 1234567appendonly no # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！appendfilename \"appendonly.aof\" # 持久化的文件的名字# appendfsync always # 每次修改都会 sync。消耗性能appendfsync everysec # 每秒执行一次 sync，可能会丢失这1s的数据！# appendfsync no # 不执行 sync，这个时候操作系统自己同步数据，速度最快！ 具体的配置，我们在后面Redis持久化时再详细详解","link":"/2020/06/24/redis%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6redis-conf%E8%AF%A6%E8%A7%A3/"},{"title":"redis的持久化RDB和AOF","text":"主要内容 RDB持久化的配置和原理 AOF持久化的配置和原理 RDB和AOF的对比和选择 Redis的持久化我们知道Redis是在内存中工作的数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失。所以 Redis为提供了持久化功能！ Redis为我们提供了不同级别的持久化方式: RDB持久化方式能够在指定的时间间隔对数据进行快照存储，恢复时将快照直接读取到内存. AOF持久化方式记录每次对服务器写的操作命令,当服务器重启的时候会重新执行这些命令来恢复原始的数据。AOF命令以redis协议追加保存每次写的操作到文件末尾。 如果只希望你的数据在服务器运行的时候存在,则可以不使用任何持久化方式。 也可以同时开启两种持久化方式, 在这种情况下, 当redis重启的时候会优先载入AOF文件来恢复原始的数据。因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。 从上面我们知道最重要的还是了解RDB和AOF持久化方式的不同,让我们以RDB持久化方式开始吧。 RDB(Redis DataBase)可以把RDB简单理解为是一台给Redis内存数据存储拍照的照相机，并且生成快照保存到磁盘的过程。Redis重启读取RDB速度快，但是无法做到实时持久化，因此一般用于数据冷备和复制传输。 RDB持久化配置12345678910111213141516171819# 时间策略save 900 1save 300 10save 60 10000# 持久化文件名称dbfilename dump.rdb# 文件保存路径dir ./# 如果持久化出错，主进程是否停止写入stop-writes-on-bgsave-error yes# 是否压缩rdbcompression yes# 导入时是否检查rdbchecksum yes 配置其实非常简单，这里说一下持久化的时间策略具体是什么意思。 save 900 1 表示900s内如果有1条是写入命令，就触发产生一次快照，可以理解为就进行一次备份 save 300 10 表示300s内有10条写入，就产生快照 下面的类似，那么为什么需要配置这么多条规则呢？因为Redis每个时段的读写请求肯定不是均衡的，为了平衡性能与数据安全，我们可以自由定制什么情况下触发备份。所以这里就是根据自身Redis写入情况来进行合理配置。 stop-writes-on-bgsave-error yes 这个配置也是非常重要的一项配置，这是当备份进程出错时，主进程就停止接受新的写入操作，是为了保护持久化的数据一致性问题。如果自己的业务有完善的监控系统，可以禁止此项配置， 否则请开启。 关于压缩的配置rdbcompression yes，建议没有必要开启，毕竟Redis本身就属于CPU密集型服务器，再开启压缩会带来更多的CPU消耗，相比硬盘成本，CPU更值钱。 当然如果想要禁用RDB配置，也是非常容易的，只需要在save的最后一行写上：save &quot;&quot; RDB文件恢复1、只需要将rdb文件放在我们redis启动目录就可以，redis启动的时候会自动检查dump.rdb 恢复其中的数据！ 2、查看需要存在的位置 123127.0.0.1:6379&gt; config get dir1) \"dir\"2) \"/usr/local/bin\" # 如果在这个目录下存在 dump.rdb 文件，启动就会自动恢复其中的数据 可以知道，如果我们没有修改配置文件中dir ./选项，那么dump.rdb默认存储到我们的启动目录下，也就是/usr/local/bin目录下。 所以我们几乎不需要配置什么，redis自己默认的配置就够用了！ RDB工作流程在Redis中RDB持久化的触发分为两种：手动触发与Redis定时触发。 手动触发 save：会阻塞当前Redis服务器，直到持久化完成，线上应该禁止使用。 bgsave：该触发方式会fork一个子进程，由子进程负责持久化过程，因此阻塞只会发生在fork子进程的时候，所以Redis内部涉及到RDB都采用bgsave命令。 自动触发 一般我们是不会直接用命令生成RDB文件的，Redis支持自动触发RDB持久化机制。自动触发的场景主要是有以下几点： 根据我们的 save m n 配置规则自动触发； 从节点全量同步时，主节点发送rdb文件给从节点完成复制操作，主节点会触发 bgsave； 执行debug reload时； 执行shutdown时，如果没有开启aof，也会触发。 但是看过redis的源码后，其实发现在rdb.c文件中redis的自动触发底层子进程名为”redis-rdb-bgsave” 所以我们看一下RDB的工作流程: 1.redis执行bgsave命令，父进程判断当前是否存在正在进行执行的子进程，如RDB/AOF子进程，存在则bgsave命令直接返回 2.不存在则父进程fork出子进程，fork操作中Redis父进程会阻塞 3.父进程fork完成后，bgsave命令返回“Background saving started”信息并不再阻塞父进程，可以继续响应其他命令。 4.子进程创建RDB文件，根据父进程内存生成临时快照文件，完成后 对原有文件进行原子替换 5.进程发送信号给父进程表示完成，父进程更新统计信息 RDB工作原理了解了上面RDB的工作流程，工作原理就非常简单了。给出两个词汇就可以了，fork和cow。 fork是指上面的redis通过创建子进程来进行RDB操作。 cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务。写服务会对数据段进行不断的修改。这时候cow机制会进行数据段页面的分离。当父进程对其中一个页面的数据进行修改时，会将被共享的页面复 制一份分离出来，然后 对这个复制的页面进行修改。这时 子进程 相应的页面是 没有变化的，还是进程产生时那一瞬间的数据。 RDB优缺点优点： RDB会生成很多的数据文件，每个数据文件分别代表了Redis在某个时间点上的数据快照。因此非常适合冷备和全量同步 Redis加载RDB恢复数据远远快于AOF的方式 缺点： RDB方式数据完整性无法保证。因为是每隔一段时间才生成一次，可能会造成一段时间的数据丢失。而AOF最多丢失1秒钟的数据。 RDB在生成数据快照的时候，如果文件比较大，会导致客户端暂停几ms。 AOF(Append Only File)上面提到过两次RDB之间的数据可能会丢失消失。而AOF的出现很好的解决了数据持久化的实时性。 AOF以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中的命令来恢复数据。AOF会先把命令追加在AOF缓冲区，然后根据对应策略写入硬盘（appendfsync） AOF持久化配置1234567891011121314151617181920212223# 是否开启aof 默认是no如果需要手动设置为yesappendonly yes# aof文件名称appendfilename \"appendonly.aof\"# 同步频率appendfsync everysec# aof重写期间是否同步no-appendfsync-on-rewrite no# 重写触发配置# 第一行的意思是 Redis记录最近的一次AOF操作的文件大小，如果当前AOF文件大小增长超过这个百分比则触发一次重写，默认100# 第二行的意思是如果文件大小大于此值则会触发AOF，默认64MBauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 加载aof时如果有错如何处理aof-load-truncated yes# 文件重写策略aof-rewrite-incremental-fsync yes 还是重点解释一些关键的配置： appendfsync everysec它其实有三种模式: always：命令写入aof缓冲区后，每一次写入都需要同步，直到写入磁盘（阻塞，系统调用fsync）结束后返回。显然和Redis高性能背道而驰，不建议配置 everysec：命令写入aof缓冲区后，在写入系统缓冲区直接返回（系统调用write），然后有专门线程每秒执行写入磁盘（阻塞，系统调用fsync）后返回 no：命令写入aof缓冲区后，在写入系统缓冲区直接返回（系统调用write）。之后写入磁盘（阻塞，系统调用fsync）的操作由操作系统负责，通常最长30s 一般情况下都采用everysec配置，这样可以兼顾速度与安全，最多损失1s的数据。 aof-load-truncated yes 如果该配置启用，在加载时发现aof尾部不正确时，会向客户端写入一个log，但是会继续执行，如果设置为 no ，发现错误就会停止，必须修复后才能重新加载。 AOF工作流程和RDB方式一样，AOF持久化方式也支持自动触发和手动触发两种方式: 手动触发 使用bgrewriteaof命令：Redis主进程fork子进程来执行AOF重写，这个子进程创建新的AOF文件来存储重写结果，防止影响旧文件。因为fork采用了写时复制机制，子进程不能访问在其被创建出来之后产生的新数据。Redis使用 AOF重写缓冲区 保存这部分新数据，最后父进程将AOF重写缓冲区的数据写入新的AOF文件中然后使用新AOF文件替换老文件。 自动触发和RDB一样，配置在redis.conf文件里，当然你也可以通过调用CONFIG SET命令设置。 同样看过aof.c的代码后，我们知道fork出名为”redis-aof-rewrite”的子进程 我们看一下aof的工作流程： 1.所有的写入命令追加到aof缓冲区 2.AOF缓冲区根据对应appendfsync配置向硬盘做同步操作 3.定期对AOF文件进行重写 4.Redis重启时，可以加载AOF文件进行数据恢复 我们可以看下重写的流程： 对于上图有四个关键点补充一下： 在重写期间，由于主进程依然在响应命令，为了保证最终备份的完整性；因此它依然会写入旧的AOF file中，如果重写失败，能够保证数据不丢失。 为了把重写期间响应的写入信息也写入到新的文件中，因此也会为子进程保留一个buf，防止新写的file丢失数据。 重写是直接把当前内存的数据生成对应命令，并不需要读取老的AOF文件进行分析、命令合并。 AOF文件直接采用的文本协议，主要是兼容性好、追加方便、可读性高可认为修改修复。 AOF优缺点优点： 保证了数据的完整性，做到最多丢失1s内的数据 AOF文件写入性能高，文件不易损坏 AOF在对日志文件进行操作的时候是以append-only的方式去写的，他只是追加的方式写数据，自然就少了很多磁盘寻址的开销了 缺点： AOF文件比RDB文件还要大 可能导致追加阻塞 RDB和AOF的对比和选择因为两个持久化机制你明白了，剩下的就是看自己的需求了，需求不同选择的也不一定，但是通常都是结合使用。有一张图可供总结： 对比了这几个特性，剩下的就是看结合具体情况使用了。当然两个一块用肯定是好的了，不过在文章开头也提到了如果两个一块用的话，redis重启的时候会优先载入AOF文件来恢复原始的数据。（因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。） 如果两种方式一起使用，在进行数据恢复时流程如上图所示。所以一定要注意。","link":"/2020/06/25/redis%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96RDB%E5%92%8CAOF/"},{"title":"redis的主从、哨兵、集群","text":"主要内容 主从复制的配置和原理 哨兵模式的配置和sentinel配置文件详解 Redis Cluster的概念和使用 主从复制(Replication)我们知道单机QPS是有上限的，而且Redis的特性就是必须支撑读高并发的，如果只用一台redis既处理读请求又处理写请求，那么如果请求数量过多的话，一方面redis处理速度会变慢，但更有可能的是redis直接挂掉。 这时候我们可以和Mysql一样，在redis中让一个master节点去处理写请求，然后将数据同步给其他slave节点，让slave节点去处理读请求，从而减轻主服务器的压力，实现读写分离。而且扩容的时候还可以轻松实现水平扩容。 配置使用 准备三台装好了redis的服务器 Node1 192.168.0.129 Node2 192.168.0.135 Node3 192.168.0.127 打算把Node1配置为master节点，Node2、Node3为从节点 1.首先通过info replication查看Node1的主从相关信息 Node2和Node3同样是这个样子，说明redis中默认所有节点都是主节点。因此我们只需要配置从节点即可。 2.配置从节点信息(只需要认主人即可) 配置Node2节点 1slave of 主节点IP 主节点redis端口号 同样的配置Node3节点后，我们在来看下Node1节点 到这里，我们的主从就配置完成了，我们可以简单测试一下效果。 另外需要注意到我们的从节点是不可以set值的，这主要和我们的redis.cong中的设置有关， 另外我们上面这种通过命令行的方式设置的主从，redis关闭后就会丢失。所以都需要设置到我们的配置文件redis.conf中的REPLICATION部分。这里就不再设置了。 复制原理Redis的主从同步策略 主从刚刚连接的时候，进行全量同步；全量同步结束后，进行增量同步。 如果连接过程中网络中断或者服务器挂了，从服务器会进行重新连接，并且尝试使用增量同步把数据补充完整 如果增量同步无法实现的时候，再执行全量同步 全量同步Redis全量同步一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。具体步骤如下： 从服务器连接主服务器，发送SYNC命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令； 增量同步Redis增量同步是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。 增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令。 哨兵模式(Sentinel)上面讲的Redis还只是主从方案。但是注意到我们主节点只有一个，如果我们的主节点宕机，该怎么办？比如上面我们的Node1宕机了，这时候我们只能手工将Node2设置为主节点slaveof no one,然后重新配置Node3，使其主节点为Node2，再重启。 两台机器还好，但是如果有更多台呢？每台机器都这样操作一遍，得多少时间。 毫无疑问，这样的人工运维效率太低，事故发生时估计得至少 1 个小时才能缓过来。 为了防止这种事情的发生Redis官方提供了一种保证redis高可用的方案 Redis Sentinel(哨兵)，它可以在故障发生时自动进行从主切换，并且程序也可以不用重启。 一个经典的哨兵集群如下图所示： 其中M表示主节点、R表示从节点、S表示Sentinal节点 当我们的主节点M1所在的服务器挂了，就可以通过从节点的两个哨兵S2、S3选举处一个出来执行failover(故障转移)即可。 Redis官网也告诉了我们哨兵组件的主要功能： 集群监控：负责监控 Redis master 和 slave 进程是否正常工作。 消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 另外我们在配置使用前对于Sentinel还需要知道： Sentinel本身也是一个分布式系统，因此需要至少3个Sentinel，来保证它的健壮性。多个哨兵也可以减少我们的误报率。 因为redis使用异步复制，因此主从+哨兵并不能保证数据不丢失，但是可以保证集群的高可用。 Sentinel默认运行在26379端口，因此一定要注意打开端口 配置使用所有关于sentinel的配置都在我们redis目录下的sentinel.conf文件中。我们按照上面的经典哨兵模式进行配置。在上面一主二从的基础上，每台服务器开启一个Sentinel进程 1.主节点Node1中sentinel的配置 12daemonize yessentinel monitor Node1master 192.168.0.129 6379 2 配置完成后使用命令redis-sentinel 配置文件启动S1 2.从节点Node2和Node3中sentinel的配置(和主节点相同) 12daemonize yessentinel monitor Node1master 192.168.0.129 6379 2 Node2和Node3配置和Node1完全相同，同样配置完成后启动S2和S3 其实到这里，已经配置完成了。我们接下来测试一下。这里我们直接断开Master节点 Node2中查看相关信息 Node3中查看相关信息 从上面我们可以看到，我们的哨兵模式起到了作用，当master节点宕机后，重新选举出了Node2 slave节点作为了master节点，并且重新调整了Node3成为了Node2的slave节点。 Redis集群(Redis Cluster)","link":"/2020/07/03/redis%E7%9A%84%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/"},{"title":"windows环境下基于vmware的linux集群环境搭建","text":"windows环境下基于vmware的linux集群环境搭建 最近学习到了redis的集群，需要一个分布式的linux系统，可是考虑到云主机的价格，还是打算用虚拟机来搭建linux集群。并且之后学习微服务时还是需要用到linux的集群，所以这里就直接记录下整个linux集群的搭建过程。 软件说明 物理宿主机系统：Windows 10 专业版 虚拟机软件：VMware Workstation 12版 CentOS操作系统ISO镜像：CentOS7 64位 一.、VMware安装虚拟机以及CentOS7的安装这里网上的教程非常多，这里就不详细说了，只需要注意一点，就是在网络设置时我们选择桥接模式。 接下来就是我们这篇文章的重点了。 二、网络配置(重点)1、首先尝试查看虚拟机系统的IP地址 安装好系统后，首先使用ifconfig命令进行查看。我们会发现装好的系统并没有为它设置IP地址。 这时候，无论我们是ping百度还是ping宿主机都是无法ping通的，也就是说无法与外网和宿主机进行网络通信。 2、设置虚拟机与物理宿主机的网络连接 选择桥接模式，并选择桥接到物理宿主机的上网网卡即可 3、为虚拟机配置固定静态IP 首先使用dhclient工具为本机分配一个网络内可用的IP地址： 接下来编辑虚拟机系统网卡配置，将上面分配所得的IP地址配置进去：使用命令编辑：vim /etc/sysconfig/network-scripts/ifcfg-ens33 可以看到未修改时的配置文件如下，我们需要修改的也就是下面标出的3个位置 修改过后的配置文件如下 12345678910111213141516171819TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=6364fb85-350b-44a5-a465-b525137d6482DEVICE=ens33ONBOOT=yesIPADDR=192.168.0.127NETMASK=255.255.255.0GATEWAY=192.168.0.1DNS1=114.114.114.114 修改完成后，一定不要忘了重启网卡 systemctl restart network.service 4、检查配置结果 检验虚拟机系统网络和外界的联通性： 反过来，宿主机也可以ping通我们的虚拟机 用宿主机远程连接虚拟机进行测试 三、搭建多个互相连通的linux节点到这里，我们一台主机就配好了。可以完全重复以上步骤再打造出多个Linux节点，当然更简单的方式则是直接通过上面已经装好了的虚拟机节点直接克隆，来快速生成其他节点。 创建完成克隆即可。 克隆完成后，再按照上述网络配置的方法直接配置就好了 后记好啦，现在多节点的Linux环境终于搭建完成了，后续不管是学Linux、用Linux，还是Linux环境编程、应用和项目部署、工具实验，都有可以动手实践的地方了。","link":"/2020/06/24/windows%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%9F%BA%E4%BA%8Evmware%E7%9A%84linux%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"title":"Java多线程之阻塞队列BlockingQueue","text":"主要内容 BlockingQueue概念 BlockingQueue的7个实现类 BlockingQueue的4种核心API BlockingQueue版生产者消费者模型 ArrayBlockingQueue底层源码分析 阻塞队列概念 当队列是满时，往队列中添加元素的操作将会被阻塞,直到队列不满 当队列是空时，从队列中获取元素的操作将会被阻塞,直到队列不空 阻塞队列最常用于生产者消费者模式。生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。 七种阻塞队列JDK1.8中给我们提供了7种不同的阻塞队列，接下来我们看一下都是哪些： ArrayBlockingQueue：由数组结构组成的有界阻塞队列。 LinkedBlockingQueue：由链表结构组成的有界(大小默认为Integer.MAX_VALUE)阻塞队列。 PriorityBlockingQueue：支持优先级排序的无界阻塞队列。 DelayQueue：使用优先级队列实现的无界阻塞队列。 SynchronousQueue：不存储元素的阻塞队列，也即单个元素的队列。 LinkedTransferQueue：由链表结构组成的无界阻塞队列。 LinkedBlockingDeque：由链表结构组成的双向阻塞队列。 这里我们重点学习标注的3个阻塞队列，ArrayBlockingQueue、LinkedBlockingQueue和SynchronousQueue,因为这3个也是后面线程池的底层使用到的，其余的简单提一下功能即可。 BlockingQueue核心APIJDK的API中也告诉了我们BlockingQueue支持四种不同的方式处理操作，接下来我们就看看这四种方法有什么不同 方法类型 抛出异常 特殊值 一直阻塞 超时退出 插入 add(e) offer(e) put(e) offer(e, time, unit) 删除 remove() poll() take() poll(time, unit) 检查 element() peek() 不可用 不可用 抛出异常 当阻塞队列满时，再往队列里使用add插入元素会抛出IllegalStateException: Queue full 当阻塞队列空时，在往对了里使用remove删除元素会抛出NoSuchElementException 1234567891011121314151617181920package com.company.MultiThread;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;public class BlockingQueueDemo { public static void main(String[] args) { //指定队列容量为2 BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(2); System.out.println(blockingQueue.add(1)); //true System.out.println(blockingQueue.add(2)); //true // 抛出异常IllegalStateException: Queue full // System.out.println(blockingQueue.add(3)); System.out.println(blockingQueue.remove()); //1 System.out.println(blockingQueue.remove()); //2 //抛出异常java.util.NoSuchElementException //System.out.println(blockingQueue.remove()); }} 特殊值 插入方法,成功true失败false 删除方法，成功返回出队列元素，队列里面没有返回null 1234567891011121314151617package com.company.MultiThread;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;public class BlockingQueueDemo { public static void main(String[] args) { BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(2); System.out.println(blockingQueue.offer(1)); //true System.out.println(blockingQueue.offer(2)); //true System.out.println(blockingQueue.offer(3)); //false System.out.println(blockingQueue.poll()); //1 System.out.println(blockingQueue.poll()); //2 System.out.println(blockingQueue.poll()); //null }} 一直阻塞 当阻塞队列满时，生产者线程继续往队列里put元素，生产者线程会一直阻塞，直到put数据或者响应中断退出 当阻塞队列满时，消费者线程继续往队列里take元素，消费者线程会一直阻塞，直到队列可用 123456789101112131415161718192021222324package com.company.MultiThread;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;public class BlockingQueueDemo { public static void main(String[] args) { BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(2); try { blockingQueue.put(1); blockingQueue.put(2); System.out.println(\"-----------\"); //会一直阻塞 //blockingQueue.put(3); System.out.println(blockingQueue.take()); System.out.println(blockingQueue.take()); //会一直阻塞 //System.out.println(blockingQueue.take()); } catch (InterruptedException e) { e.printStackTrace(); } }} 超时退出 当阻塞队列满时，生产者线程会阻塞一定时间，超过时间后生产者线程会退出。 消费者线程同理 12345678910111213141516171819202122232425262728package com.company.MultiThread;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;public class BlockingQueueDemo { public static void main(String[] args) { BlockingQueue&lt;Integer&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(2); try { blockingQueue.offer(1,2, TimeUnit.SECONDS); blockingQueue.offer(2,2, TimeUnit.SECONDS); System.out.println(\"-----------\"); //阻塞2秒钟，队列仍然没有可用空间，则直接结束，不会一直阻塞 boolean flag = blockingQueue.offer(3, 2, TimeUnit.SECONDS); System.out.println(flag);// false System.out.println(blockingQueue.poll(2,TimeUnit.SECONDS)); System.out.println(blockingQueue.poll(2,TimeUnit.SECONDS)); //阻塞2秒钟，队列仍然没有，则直接结束，不会一直阻塞 System.out.println(blockingQueue.poll(2,TimeUnit.SECONDS));//null } catch (InterruptedException e) { e.printStackTrace(); } }} BlockingQueue实现生产者消费者模型到目前为止，我们学过的生产者-消费者模型有： wait-notify/notifyAll await-signal/signalAll 上面这两种都需要我们自己来控制每个线程的阻塞和唤醒，实现起来比较麻烦，我们接下来看看阻塞队列是如何实现生产者消费者模型的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.company.MultiThread;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;public class BlockingQueueProConDemo { private volatile boolean flag = true; //默认开启,进行生产+消费 private AtomicInteger atomicInteger = new AtomicInteger();//默认为0 BlockingQueue&lt;String&gt; blockingQueue = null; public BlockingQueueProConDemo(BlockingQueue&lt;String&gt; blockingQueue){ this.blockingQueue = blockingQueue; System.out.println(blockingQueue.getClass().getName()); } public void myProduct(){ String data; boolean retValue; while(flag){ data = atomicInteger.incrementAndGet()+\"\"; try { retValue = blockingQueue.offer(data,2, TimeUnit.SECONDS); if(retValue){ System.out.println(Thread.currentThread().getName()+\"插入队列\"+data+\"成功\"); }else{ System.out.println(Thread.currentThread().getName()+\"插入队列\"+data+\"失败\"); } TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(Thread.currentThread().getName()+\"flag为false,整个程序停止\"); } public void myConsumer() { String result = null; while(flag){ try { result = blockingQueue.poll(2,TimeUnit.SECONDS); if(result == null || result.equalsIgnoreCase(\"\")){ flag = false; System.out.println(Thread.currentThread().getName()+\"超过2秒钟队列中没有值，消费退出\"); return; } System.out.println(Thread.currentThread().getName()+\"消费\"+result+\"成功\"); } catch (InterruptedException e) { e.printStackTrace(); } } } private void stop() { this.flag = false; } public static void main(String[] args) { BlockingQueueProConDemo blockingQueueProConDemo = new BlockingQueueProConDemo(new ArrayBlockingQueue&lt;&gt;(3)); new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"生产者线程启动\"); blockingQueueProConDemo.myProduct(); },\"Product\").start(); new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+\"消费者线程启动\"); blockingQueueProConDemo.myConsumer(); },\"Consumer\").start(); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"3秒钟时间到，生产消费停止\"); blockingQueueProConDemo.stop(); }} 运行结果如下： 123456789101112java.util.concurrent.ArrayBlockingQueueProduct生产者线程启动Consumer消费者线程启动Product插入队列1成功Consumer消费1成功Product插入队列2成功Consumer消费2成功Product插入队列3成功Consumer消费3成功3秒钟时间到，生产消费停止Productflag为false,整个程序停止Consumer超过2秒钟队列中没有值，消费退出","link":"/2020/03/31/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97BlockingQueue/"},{"title":"redis数据结构底层实现原理","text":"主要内容 redis5种基本数据结构的底层实现原理 1.StringRedis中的字符串是由 简单动态字符串SDS(Simple Dynamic String) 实现的。它的底层实现有点类似于 Java 中的 ArrayList，我们从源码的 sds.h/sdshdr 文件中可以看到 Redis底层对于字符串的定义结构： 12345678910111213141516171819202122232425262728293031323334//小于1字节struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 低3位用来存储类型，高5位用来存储字符串长度 */ char buf[];};//1字节struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* 字符串在buf中实际占用的字节数(不包括\\0) */ uint8_t alloc; /* 给buf分配的实际空间大小（见后面的空间预分配）减去一个字节的'\\0'，所以剩余可用空间的大小就等于alloc-len */ unsigned char flags; /* 低位的3个bit位用来表示结构类型，其余5个bit位未使用 */ char buf[]; //真正存放字符串是使用字节数组};//2字节struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* used */ uint16_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[];};//4字节struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* used */ uint32_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[];};//8字节struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[];}; 从上面源码中可以看到同样是字符串，Redis使用泛型定义了好多次，为什么不直接使用int类型呢？这其实是Redis为了对内存做极致的优化，不同长度的字符串使用不同的结构体来表示。 SDS与C字符串对比我们知道redis中没有使用C自带的字符串，而是自己封装了SDS，存储结构在上面的源码中已经给出，这里就通过一个图示例： SDS对于C字符串优点 获取字符串长度的时间复杂度O(1)： C字符串由于没有记录字符串长度，因此获得字符串长度时要进行数组的遍历，时间复杂度为O(n)。而SDS由于记录了字符串长度时间复杂度为O(1) 杜绝缓冲区溢出问题： C字符串由于没有记录字符串长度，所以使用strcat进行字符串拼接时，可能会出现目标分配内存不够的情况导致缓冲区溢出。 而SDS的空间分配策略则会先检查SDS空间是否满足要求，不满足则扩容至所需大小，再执行响应操作 减少修改字符串时带来的内存重分配次数： 还是由于C字符串由于没有记录字符串长度，所以对于一个包含N个字符的C字符串底层总是一个N+1的数组，如果对C字符串进行了增加或减少程序总是要进行内存的重新分配。如果没有重新分配，则增加字符串出现缓冲区溢出、减少字符串出现内存泄露的问题。而SDS通过空间分配策略解决这个问题。 二进制安全 ： C字符串中字符必须符合某种编码格式，并且除了字符串的末尾，字符串中不能包含空字符，因此只能用来保存文本数据。而redis中则可以保存任何二进制文件，如图像、音频、视频、压缩文件等 兼容部分C字符串函数： 因为redis中的字符串末尾和C字符串一样，也是有’\\0’的，因此可以使用C语言中的部分C字符串函数操作我们的redis中的SDS。 可以简单总结为下表 空间分配策略上面一直提到内存分配策略，但是一直没有详细描述，这里就对空间分配策略做一个简单介绍。 1.空间预分配 空间预分配用于优化 sds 的字符串增长操作：当 sds 的 API 对一个 sds 进行修改，并且需要对 sds 进行空间扩展的时候，程序不仅会为 sds 分配修改所必须要的空间，还会为 sds 分配额外的未使用空间，并根据新分配的空间重新定义 sds 的 header。这一部分代码如下： 123456789101112/* Return ASAP if there is enough space left. */ if (avail &gt;= addlen) return s; len = sdslen(s); sh = (char*)s-sdsHdrSize(oldtype); newlen = (len+addlen); if (newlen &lt; SDS_MAX_PREALLOC) newlen *= 2; else newlen += SDS_MAX_PREALLOC; type = sdsReqType(newlen); 简单来说就是： 如果对 sds 进行修改之后，sds的长度（也即是 len 属性的值）将小于 1 MB ，那么程序分配和len 属性同样大小的未使用空间，这时 SDSsdsalloc属性的值将正好为 len 属性的值的两倍。举个例子，如果进行修改之后，sds的len将变成 13 字节，那么程序也会分配 13 字节的未使用空间，alloc 属性将变成 13字节，sds 的 buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 sds 进行修改之后，sds 的长度将大于等于 1 MB ，那么程序会分配 1 MB 的未使用空间。举个例子， 如果进行修改之后，sds 的 len 将变成 30 MB，那么程序会分配 1 MB 的未使用空间，alloc 属性将变成 31 MB ，sds 的 buf 数组的实际长度将为 30 MB + 1 MB + 1 byte。 通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。通过这种空间换时间的预分配策略，sds 将连续增长 N 次字符串所需的内存重分配次数从必定 N 次降低为最多 N 次。内存预分配策略仅在 sds 扩展的时候才触发，而新创建的 sds 长度和 C 字符串一致，是长度 + 1byte。 2.惰性空间释放惰性空间释放用于优化 sds 的字符串缩短操作：当 sds 的 API 需要缩短 sds 保存的字符串时， 程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性（数组中的神谕部分）将这些字节的数量记录起来， 并等待将来使用。 通过惰性空间释放策略，sds 避免了缩短字符串时所需的内存重分配操作， 并为将来可能有的增长操作提供了优化。与此同时，sds 也提供了相应的 API sdsfree，让我们可以在有需要时， 真正地释放 sds 里面的未使用空间，所以不用担心惰性空间释放策略会造成内存浪费。 2.List由于C语言中没有内置链表这种数据结构，所以redis构建了自己的链表实现。类似于Java中的LinkedList是一个双向链表。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)。 我们可以从源码的adlist.h/listNode来看到对其的定义： 1234567/* Node, List, and Iterator are the only data structures used currently. */typedef struct listNode { struct listNode *prev; struct listNode *next; void *value;} listNode; 可以看到，多个 listNode 可以通过 prev 和 next 指针组成双向链表： 虽然仅仅使用多个 listNode 结构就可以组成链表，但是使用adlist.h/list结构来持有链表的话，操作起来会更加方便： 12345678typedef struct list { listNode *head; listNode *tail; void *(*dup)(void *ptr); void (*free)(void *ptr); int (*match)(void *ptr, void *key); unsigned long len;} list; 所以从上面可以看到，在redis实现的链表有如下特点： 双向无环链表： 每个链表节点都有prev和next指针。 并且表头结点的prev指针和表尾结点的next指针都指向NULL。 带表头指针和表尾指针 ： list结构的head指针和tail指针。 因此获取表头节点和表尾结点的复杂度为O（1） 带链表长度计数器 即上面的len属性 多态 可以使用void*指针保存节点值，并且通过dup、free、match竖向为节点设置特定类型函数。 3.Hash由于C语言中没有内置map这种数据结构，所以redis构建了自己的map实现。相当于 Java 中的 HashMap，内部实现也差不多类似，都是通过 “数组 + 链表” 的链地址法来解决部分哈希冲突。 哈希表源码 dict.h/dictht 定义： 12345678910typedef struct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值，总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used;} dictht; table 属性是一个数组，数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针，而每个 dictEntry 结构保存着一个键值对： 哈希表节点定义: 12345678910111213typedef struct dictEntry { // 键 void *key; // 值 union { void *val; uint64_t u64; int64_t s64; double d; } v; // 指向下个哈希表节点，形成链表 struct dictEntry *next;} dictEntry; 下图就是redis中哈希表的示意图： 但其实这里还没有结束，实际上redis中整个hash的构成还包括字典的定义： 123456789101112typedef struct dict { //上面两个属性用来实现多态 dictType *type; void *privdata; // 内部有两个哈希表结构 dictht ht[2]; //rehash索引 //当rehash不再进行时，值为-1 long rehashidx; /* rehashing not in progress if rehashidx == -1 */} dict; 从上面dict的源码中注意到，实际上字典结构的内部包含两个 hashtable，通常情况下只有一个 hashtable 是有值的，但是在字典扩容缩容时，需要使用ht[1]，然后进行渐进式搬迁 (下面说原因)。 普通状态下没有进行rehash的字典结构示意图： 添加元素和扩容 在我们的字典中添加元素时和我们Java中的HashMap一样，也是通过hash值判断位置后添加元素，并且解决Hash冲突时同样是使用的链地址法。但是当元素较多的时候，也需要对字典进行扩容。 什么时候扩容呢?首先我们定义下负载因子 负载因子=哈希表已保存节点数量/哈希表大小即load_factor = ht[0].used/ht[0].size 当一下条件中的任意一个被满足时，程序会自动开始对哈希表进行扩展操作： 服务器目前没有在执行BGSAVE或者BGREWRITEAOF命令，并且哈希表的负载因子大于等于1. 服务器正在执行BGSAVE或者BGREWRITEAOF命令，并且哈希表的负载因子大于等于5 为什么需要根据是否执行BGSAVE和BGREWRITEAOF命令来进行区别呢？ 因为在执行上述命令的过程中，redis需要创建当前服务器进程的子进程，而大多数操作系统都采用写时复制(copy-on-write)来优化子进程的使用效率，所以在子进程存在期间，服务器会提高扩展所必须的负载因子，从而尽可能的避免在子进程存在期间进行哈希表的扩展操作，这颗避免不必要的内存写入操作，尽最大限度的节约内存。 redis中的字典扩容过程： 为字典的ht[1]哈希表分配空间，这个大小取决于操作类型和ht[0]中包含的键值对数量(即used属性) 扩展操作 ht[1]的大小为第一个大于等于ht[0].used*2的2的n次方 收缩操作 ht[1]的大小为第一个大于等于ht[0].used的2的n次方 将保存在ht[0]中的所有键值对rehash(全部移动)到ht[1]上。 rehash完成后，释放ht[0],将ht[1]设置为ht[0],并且在ht[1]新建一个空白哈希表，为下次rehash做准备。 渐进式 rehash 上面我们只是说进行rehash就是将ht[0]中的元素移动到ht[1]中，但是如果直接一次性完成的话，是一个 O(n) 级别的操作，作为单线程的 Redis 很难承受这样耗时的过程，所以 Redis使用渐进式 rehash小步搬迁： 为ht[1]分配空间，让字典同时持有ht[0]和ht[1]两个哈希表 在字典中维持一个索引计数器变脸rehashidx，并将它的值设置为0，表示rehash正式开始 每次对字典执行添加、删除、查找、更新操作时，程序除了执行指定的操作外，还会将ht[0]在rehashidx索引上的所有键值对rehash到ht[1]上，当rehash完成后，程序将rehashidx属性值加1 随着字典操作的不断执行，最终在某个时间点上，ht[0]的值全部被rehash到h[1]上，这时将rehashidx设置为-1，表示rehash完成。 另外，在rehash过程中，字典会同时使用两个哈希表，删除、查找、更新等操作会在两个哈希表上进行。 先找ht[0]再找ht[1]。 并且插入操作会直接保存到ht[1] 总结 redis中hash底层是使用了两个哈希表(数组加链表)，一个平时使用，另一个仅在rehash时使用 rehash是将ht[0]上的元素全部rehash到ht[1]上，使用的是MurmurHash2算法计算哈希值 rehash是渐进式的，每次插入、删除、更新操作时会将ht[0]中的一个元素rehash到ht[1]上，最终完成rehash。 4.SetRedis的集合相当于Java 语言中的 HashSet，它内部的键值对是无序、唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值 NULL。 上面已经详细介绍了字典，这里就不介绍了。 5.Zset这可能是 Redis 最具特色的一个数据结构了，它类似于 Java中SortedSet 和 HashMap 的结合体，一方面它是一个set，保证了内部 value 的唯一性，另一方面它可以为每个 value 赋予一个 score 值，用来代表排序的权重。 它的内部实现用的是一种叫做 跳跃表(skipList) 的数据结构。我们先来看看什么是跳跃表。 跳跃表什么是跳跃表? 首先我们考虑一个问题，对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度是O(n)。 但假如我们每相邻两个节点之间就增加一个指针，让指针指向下一个节点，如下图： 这样所有新增的指针连成了一个新的链表，但它包含的数据却只有原来的一半 （图中的为 3，11，15，19）。 现在假设我们想要查找数据时，可以根据这条新的链表查找，如果碰到比待查找数据大的节点时，再回到原来的链表中进行查找，比如，我们想要查找 13，查找的路径则是沿着下图中标注出的红色指针所指向的方向进行的： 这是一个略微极端的例子，但我们仍然可以看到，通过新增加的指针查找，我们不再需要与链表上的每一个节点逐一进行比较，这样改进之后需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在新产生的链表上，继续为每两个相邻的节点增加一个指针，从而产生第三层链表： 在这个新的三层链表结构中，我们继续试着 查找 13，那么沿着最上层链表首先比较的是11，发现11比13 小，于是我们就知道只需要到11后面继续查找，从而一下子跳过了 11 前面的所有节点。 可以想象，当链表足够长，这样的多层链表结构可以帮助我们跳过很多下层节点，从而加快查找的效率。 更进一步的跳跃表跳跃表 skiplist就是受到这种多层链表结构的启发而设计出来的。按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到 O(logn)。 但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的 2:1 的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点 （也包括新插入的节点） 重新进行调整，这会让时间复杂度重新蜕化成 O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是 为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是 3，那么就把它链入到第 1 层到第 3 层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个 skiplist 的过程： 从上面的创建和插入的过程中可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点并不会影响到其他节点的层数，因此，插入操作只需要修改节点前后的指针，而不需要对多个节点都进行调整，这就降低了插入操作的复杂度。 为什么要跳跃表 从上面我们也可以知道，跳跃表可以使得我们的查询的时间复杂度平均值从O(n)提高到了O(logn),并且通过随机层数的方法也解决了插入的问题，因此我们可以使用跳跃表。 二叉查找树、AVL树、红黑树查找的时间复杂度也是Olog(n),为什么不用他们呢？ 对于二叉查找树而言，可能我们直接插入的数据就是按score排序的，因此二叉查找树会退化为单链表，时间复杂度还是O(n) 对于AVL树和红黑树而言，插入和删除结点时，是通过调整结构来保持树的平衡，比起跳跃表直接通过一个随机数来决定跨越几层，在时间复杂度的花销上是要高于跳跃表的。 Redis中实现的跳跃表Redis 中的跳跃表由server.h/zskiplistNode和 server.h/zskiplist两个结构定义，前者为跳跃表节点，后者则保存了跳跃节点的相关信息，同之前的 集合 list 结构类似，其实只有 zskiplistNode 就可以实现了，但是引入后者是为了更加方便的操作 跳表节点： 12345678910111213141516/* ZSETs use a specialized version of Skiplists */typedef struct zskiplistNode { // 节点的成员对象，指向一个字符串对象 sds ele; // 分值 就是我们排序时的依据 double score; // 后退指针 用于从表尾向头结点方向访问节点，每次只能后退一个节点 struct zskiplistNode *backward; // 层 第一层level[0]、第二层level[1],依次类推 struct zskiplistLevel { // 前进指针 指向表尾方向的指针 struct zskiplistNode *forward; // 跨度 用于记录两个节点之间的距离，用来计算排位 unsigned long span; } level[];} zskiplistNode; 下图就是一个高度分别为1层、3层和5层的节点 由多个节点构成的跳跃表 如上图所示，仅靠多个跳跃表节点，就可以组成一个跳跃表，但是通过使用一个zskiplist来持有这些节点，可以更方便的对跳跃表进行处理。 server.h/zskiplist文件下的zskipList定义 123456789typedef struct zskiplist { // header： 指向跳跃表的头结点 //tail： 指向跳跃表的尾结点 struct zskiplistNode *header, *tail; // 跳跃表长度，即跳跃表中目前包含节点的数量 unsigned long length; // 记录目前跳跃表中层数最大的节点的层数 int level;} zskiplist; 带有zskipList结构的跳跃表 总结 zset底层通过跳跃表实现。其中zskipListNode用于表示跳跃表节点，zskiplist用于保存跳跃表信息(表头表尾结点、长度等)。 每个跳跃表节点的层高都是1-32之间的随机数。 在同一个跳跃表中，多个节点可以包含相同的分值，但是每个节点的成员对象必须唯一。 跳跃表中的节点按照分值大小进行排序，当分值相同时，节点按照成员对象大小进行排序。","link":"/2020/07/21/redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"},{"title":"Redis过期策略和内存淘汰策略","text":"主要内容 定期删除+惰性删除 内存淘汰策略 手写LRU算法 过期策略之前我们知道如果给我们的key设置了过期时间，到了时间后就无法查到我们的key，那redis中是如何实现到时间就删除我们的过期key的呢？ 这就是我们今天要讲的过期策略。redis中的过期策略包含定期删除+惰性删除 定期删除redis 会将每个设置了过期时间的 key 放入到一个独立的字典中，以后会定时遍历这个字典来删除到期的 key。默认会每隔100ms进行一次过期扫描，过期扫描不会遍历过期字典中所有的 key，而是采用了一种简单的贪心策略： 从过期字典中随机 20 个 key； 删除这 20 个 key 中已经过期的 key； 如果过期的 key 比率超过 1/4，那就重复步骤 1； 同时，为了保证过期扫描不会出现循环过度，导致线程卡死现象，算法还增加了扫描时间的上限，默认不会超过 25ms 从库被动删除从库不会进行定期扫描，而是主库在key到期时，会在 AOF文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的key。 因为指令同步是异步进行的，所以主库过期的 key 的 del 指令没有及时同步到从库的话，会出现主从数据的不一致，主库没有的数据在从库里还存在，比如集群环境分布式锁的算法漏洞就是因为这个同步延迟产生的。 惰性删除上面的定期删除可能会导致很多过期的key到了时间并没有被删除掉，那该怎么办呢？ 这时候就需要用到我们的惰性删除。 见名知意，惰性嘛，我不主动删，我懒，我等你来查询了我看看你过期没，过期就删了还不给你返回，没过期该怎么样就怎么样。 内存淘汰机制如果上面的定期删除没有删除掉过期的key，并且我们也没有去查询过期的key即也没有惰性删除，这时候当我们redis内存达到最大限制(可以使用maxmemory指令进行配置)时，要怎么做呢？ 答案就是走我们的内存淘汰机制，官网上给到的内存淘汰机制是以下几个： noeviction:当达到内存限制并且客户端尝试执行可能导致使用更多内存的命令时（大多数写入命令，但DEL和一些其他例外），将返回错误。 allkeys-lru: 尝试回收最近最少使用的键（LRU），使得新添加的数据有空间存放。 volatile-lru: 尝试回收最近最少使用的键（LRU），但仅限于在设置了过期时间的key,使得新添加的数据有空间存放。 allkeys-random: 回收随机的键使得新添加的数据有空间存放。 volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在设置了过期时间的key。 volatile-ttl: 回收设置了过期时间的key，不过淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl越小越优先被淘汰,使得新添加的数据有空间存放。 从上面可以看到volatile-xxx策略只会针对带过期时间的 key进行淘汰，allkeys-xxx 策略会对所有的key进行淘汰。其中比较常用的也是我们经常听到的就是LRU淘汰算法。 LRU算法LRU(Least recently used)最近最少使用是一种内存淘汰策略。如它的名字那样，会淘汰掉内存中最近最少使用的内容。 我们想一想自己实现LRU算法需要什么？ 首先要知道LRU算法是用在缓存中，因此首先它得是一个Map用来存储我们的key-value键值对，其次因为HashMap是无序的，而我们需要使用它来进行key的删除，则还需要一个双向链表。 所以LRU底层是由HashMap+双向链表组成 leetcode的第146题就是要我们自己实现一个LRUCache https://leetcode.com/problems/lru-cache/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182class LRUCache { private class Node{ int key,value; Node prev,next; Node(int k,int v){ this.key = k; this.value = v; } Node(){ this(0,0); } } private int capacity,count; private Map&lt;Integer,Node&gt; map; private Node head,tail; public LRUCache(int capacity) { this.capacity = capacity; this.count = 0; map = new HashMap&lt;&gt;(); head = new Node(); tail = new Node(); head.next = tail; tail.prev = head; } //删除旧节点 //重新在表头添加访问的节点 public int get(int key) { Node n = map.get(key); if(n==null) return -1; update(n); return n.value; } //如果没有则直接在表头添加新结点 //如果存在则删除后再在表头添加新结点 public void put(int key, int value) { Node n = map.get(key); if(n == null){ n = new Node(key,value); map.put(key,n); add(n); count++; }else{ n.value = value; update(n); } if(count&gt;capacity){ Node toDel = tail.prev; remove(toDel); map.remove(toDel.key); count--; } } private void update(Node node){ remove(node); add(node); } private void add(Node node){ Node after = head.next; head.next = node; node.prev = head; node.next = after; after.prev = node; } private void remove(Node node){ Node before = node.prev, after = node.next; before.next = after; after.prev = before; }}/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 另外，上面提到HashMap和双向链表我们会想到什么，对。就是我们的LinkedHashMap。其实LinkedHashMap中已经实现了LRU的功能(参考我的文章LinkedHashMap源码解析)，我们只需要重写removeEldestEntry方法就能自定义淘汰策略。 12345678910111213141516171819202122232425262728293031323334353637import java.util.LinkedHashMap;import java.util.Map;public class LRUCache&lt;K,V&gt; extends LinkedHashMap&lt;K,V&gt; { private static final int MAX_MODE_NUM = 100; private final int CACHE_SIZE; // 默认100个 public LRUCache(){ this(MAX_MODE_NUM); } /** * 传递进来最多能缓存多少数据 * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize){ super(cacheSize,0.75f,true); this.CACHE_SIZE = cacheSize; } public V putInCache(K key,V val){ return put(key,val); } public V getFromCache(K key){ return get(key); } @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { //当map中的数据量大于指定的缓存个数的时候，就自动删除最老数据 return size()&gt;CACHE_SIZE; }} Redis中的LRU算法Redis 使用的是一种近似 LRU 算法，它跟 LRU 算法还不太一样。之所以不使用严格的LRU算法，是因为需要消耗大量的额外的内存，并且需要对现有的数据结构进行较大的改造。 近似LRU算法的执行过程当 Redis 执行写操作时，发现内存超出 maxmemory，就会执行一次LRU淘汰算法。这个算法也很简单，就是随机采样出 5(可以配置) 个 key，然后淘汰掉最旧的 key，如果淘汰后内存还是超出 maxmemory，那就继续随机采样淘汰，直到内存低maxmemory 为止。 如何采样就是看maxmemory-policy的配置，如果是allkeys-lru 就是从所有的 key 字典中随机，如果是 volatile-lru 就从带过期时间的 key 字典中随机。每次采样多少个 key 看的是maxmemory_samples 的配置，默认为 5。 下面是随机 LRU 算法和严格 LRU 算法的效果对比图： 你可以看到三种点在图片中, 形成了三种带: 绿色带是新添加的对象。 深灰色带是老旧的key。 浅灰色带是通过LRU计算出要淘汰的key。 从图中可以看出： 随着采样数量越大，近似 LRU 算法的效果越接近严格LRU算法。 Redis3.0在算法中增加了淘汰池，进一步提升了近似 LRU 算法的效果。 淘汰池是一个数组，它的大小是 maxmemory_samples，在每一次淘汰循环中，新随机出来的 key 列表会和淘汰池中的 key 列表进行融合，淘汰掉最旧的一个 key 之后，保留剩余较旧的 key 列表放入淘汰池中留待下一个循环。","link":"/2020/07/22/Redis%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5%E5%92%8C%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"title":"缓存穿透、缓存击穿、缓存雪崩","text":"主要内容 缓存穿透概念和解决方案 缓存击穿概念和解决方案 缓存雪崩概念和解决方案 使用缓存的方式首先，我们回忆一下我们使用缓存的方式： 用户发起查询请求时，首先去redis缓存中进行查找 如果redis中存在请求数据则直接返回结果 如果redis中不存在数据则需要到持久层数据库中进行查询 查询出结果后将结果返回并写入到redis缓存中。 整个流程如下图所示: Redis缓存的使用，极大的提升了应用程序的性能和效率，特别是数据查询方面。但同时，它也带来了一些问题。其中，最要害的问题，就是数据的一致性问题(之后讲)，从严格意义上讲，这个问题无解。如果对数据的一致性要求很高，那么就不能使用缓存。 另外的一些典型问题就是，缓存穿透、缓存击穿和缓存雪崩。目前，业界也都有比较流行的解决方案。 缓存穿透缓存穿透指的是请求查询数据库中压根就不存在的数据，每次请求直接打在DB上 举个简答的例子：我们数据库的 id 都是1开始自增上去的，如果发起为id值为 -1 的数据或 id 为特别大不存在的数据。请求过多全部都打在DB上，在DB上走一圈查询requset过多导致DB宕掉。 解决方案 增加接口层的校验 比如用户权限校验、不合法参数直接return。比如id&lt;=0的直接拦截。 将没有的key的value设为null 之所以发生穿透是因为缓存中没有存储该值的key,那么我们给这个key的value设置为null然后放在缓存里面，防止请求打进DB 使用BloomFilter 布隆过滤器的原理也很简单就是利用高效的数据结构和算法快速判断出你这个Key是否在数据库中存在，不存在你return就好了，存在你就去查了DB刷新KV再return 缓存击穿缓存击穿是指一个Key非常热点，在不停的扛着高并发，高并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的高并发就穿破缓存，直接打在DB上，就像在一个完好无损的桶上凿开了一个洞。 解决方案 设置热点数据永不过期 从缓存层面来看，没有设置过期时间，所以不会出现热点key过期后产生的问题 加互斥锁 使用分布式锁，保证对于每个key同时只有一个线程去查询后端服务，其他线程没有获得分布式锁的权限，因此只需要等待即可。这种方式将高并发的压力转移到了分布式锁，因此对分布式锁的考验很大。 缓存雪崩缓存雪崩指的是某一个时间段，缓存中key大面积过期，或者直接出现redis宕机导致请求全部打在DB上 举个简单的例子：如果所有首页的Key失效时间都是12小时，中午12点刷新的，我零点有个秒杀活动大量用户涌入，假设当时每秒 6000 个请求，本来缓存在可以扛住每秒 5000 个请求，但是缓存当时所有的Key都失效了。此时 1 秒 6000 个请求全部落数据库，数据库必然扛不住。 解决方案 设置随机过期时间: 在批量往Redis存数据的时候，把每个Key的失效时间都加个随机值，从而使得缓存失效的时间点尽量均匀，这样可以保证数据不会在同一时间大面积失效 1setRedis（Key，value，time + Math.random() * 10000）； 或者直接设置热点数据永不过期。 避免上述三种情况的措施上面的三种情况，我们是绝对不希望在实际生产环境中发生的，因此除了针对每个问题的解决方案外，整体上我们应该发挥主动性进行解决： 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + Hystrix 限流+降级，避免MySQL被打死。 事后：Redis 持久化 RDB+AOF，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。","link":"/2020/07/23/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E3%80%81%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9/"},{"title":"23种设计模式总结","text":"主要内容 设计模式的基本概念 设计模式的六大原则 23种设计模式分类 设计模式的概念设计模式：是一套被反复使用、多数人知晓的、经过分类的、代码设计经验的总结。 使用设计模式的目的：为了代码可重用性、让代码更容易被他人理解、保证代码可靠性。 设计模式使代码编写真正工程化；设计模式是软件工程的基石脉络，如同大厦的结构一样。 设计模式的六大原则总原则：开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。 1、单一职责原则不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。 2、里氏替换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科 里氏替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 3、依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 4、接口隔离原则（Interface Segregation Principle）这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 5、迪米特法则（最少知道原则）（Demeter Principle）就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。 最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 6、合成复用原则（Composite Reuse Principle）原则是尽量首先使用合成/聚合的方式，而不是使用继承。 Java的23种设计模式 系统性的学习没有必要，用到哪个了就学哪个就行了。 一、创建型(5种)创建型模式(Creational Pattern)对类的实例化过程进行了抽象，能够将软件模块中对象的创建和对象的使用分离。为了使软件的结构更加清晰，外界对于这些对象只需要知道它们共同的接口，而不清楚其具体的实现细节，使整个系统的设计更加符合单一职责原则。 创建型模式在创建什么(What)，由谁创建(Who)，何时创建(When)等方面都为软件设计者提供了尽可能大的灵活性。创建型模式隐藏了类的实例的创建细节，通过隐藏对象如何被创建和组合在一起达到使整个系统独立的目的。 单例模式 原型模式 建造者模式 工厂模式 抽象工厂模式 二、结构型(7种)结构型模式(Structural Pattern)： 描述如何将类或者对象结合在一起形成更大的结构，就像搭积木，可以通过简单积木的组合形成复杂的、功能更强大的结构 结构型模式可以分为类结构型模式和对象结构型模式： 类结构型模式关心类的组合，由多个类可以组合成一个更大的系统，在类结构型模式中一般只存在继承关系和实现关系。 对象结构型模式关心类与对象的组合，通过关联关系使得在一个类中定义另一个类的实例对象，然后通过该对象调用其方法。根据“合成复用原则”，在系统中尽量使用关联关系来替代继承关系，因此大部分结构型模式都是对象结构型模式。 桥接模式 外观模式 组合模式 装饰模式 适配器模式 代理模式 享元模式 三、行为型(11种) 行为型模式(Behavioral Pattern)是对在不同的对象之间划分责任和算法的抽象化。 行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。 通过行为型模式，可以更加清晰地划分类与对象的职责，并研究系统在运行时实例对象之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。 行为型模式分为类行为型模式和对象行为型模式两种： 类行为型模式： 类的行为型模式使用继承关系在几个类之间分配行为，类行为型模式主要通过多态等方式来分配父类与子类的职责。 对象行为型模式： 对象的行为型模式则使用对象的聚合关联关系来分配行为，对象行为型模式主要是通过对象关联等方式来分配两个或多个类的职责。根据“合成复用原则”，系统中要尽量使用关联关系来取代继承关系，因此大部分行为型设计模式都属于对象行为型设计模式。 迭代器模式 解析器模式 观察者模式 中介者模式 访问者模式 备忘录模式 状态模式 策略模式 模板方法模式 命令模式 职责链模式","link":"/2020/08/17/23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%80%BB%E7%BB%93/"},{"title":"单例设计模式","text":"主要内容 单例模式的基本概念和使用场景 单例模式的5种实现方式 反射机制和反序列化的方式破解单例和预防 概述 1.什么是单例设计模式 单例模式(Singleton Pattern)：单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例的全局访问点。 UML图如下所示: 2.为什么要使用单例设计模式 在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。 简单来说使用单例模式可以带来下面几个好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 3.为什么不使用全局变量确保一个类只有一个实例呢？ 我们知道全局变量分为静态变量和实例变量，静态变量也可以保证该类的实例只存在一个。 只要程序加载了类的字节码，不用创建任何实例对象，静态变量就会被分配空间，静态变量就可以被使用了。但是，如果说这个对象非常消耗资源，而且程序某次的执行中一直没用，这样就造成了资源的浪费。 利用单例模式的话，我们就可以实现在需要使用时才创建对象，这样就避免了不必要的资源浪费。 不仅仅是因为这个原因，在程序中我们要尽量避免全局变量的使用，大量使用全局变量给程序的调试、维护等带来困难。 单例模式的实现 饿汉式（线程安全，调用效率高。不能延时加载） 懒汉式（线程安全，调用效率不高。可以延时加载） 双重检测锁式（由于jvm底层内部模型，偶尔会出问题，不建议使用） 静态内部类式（线程安全，调用效率高。可以延时加载） 枚举单例（线程安全，调用效率高，不能延时加载） 实现要点： 成员变量静态化 构造器私有化 提供开放方法，返回对象 1.饿汉式要点：类初始化时立即加载对象，不是延时加载 123456789101112public class Singleton { //类初始化时就生成了实例，饿汉式 private static Singleton instance = new Singleton(); private Singleton(){} public static Singleton getInstance(){ return instance; }} 问题：如果只是加载本类，而不是调用getInstance方法，或者永远没有调用，则会浪费资源。解决就是使用下面的懒汉式。 2.懒汉式要点：延时加载，调用getInstance方法时再加载 123456789101112public class Singleton { private static Singleton instance; private Singleton(){} //加上synchronized关键字保证线程安全 public synchronized static Singleton getInstance(){ if(instance==null) instance = new Singleton(); return instance; }} 问题：资源利用率高了，但是每次调用getInstance()方法时都要同步，并发效率低 3.双重检查锁式要点：将懒汉式中的同步内容下放到了if内部，不用每次获取对象时都需要同步，只有第一次才同步，创建了以后就没必要了 123456789101112131415161718public class Singleton { private static Singleton instance; private Singleton(){} public static Singleton getInstance(){ if(instance==null){ synchronized (Singleton.class){ //进入之后再检查一次是否为null if(instance==null){ instance = new Singleton(); } } } return instance; }} 问题：由于编译器优化原因和jvm底层内部模型原因，偶尔会出问题，JDK1.4之前的版本不建议使用 4. 静态内部类式要点：外部类没有static属性，不会像饿汉式那样立即加载对象，只有真正调用getInstance方法时才会加载静态内部类。 12345678910111213public class Singleton { private static class SingletonInstance{ //使用static final修饰保证内存中只有一个实例存在而且只能被赋值一次 private static final Singleton instance = new Singleton(); } private Singleton(){} public static Singleton getInstance(){ return SingletonInstance.instance; }} 5.枚举式要点：枚举本身就是单例，由JVM从根本上进行保障，避免通过反射和反序列化的漏洞 1234567891011121314151617public enum Singleton { //定义一个枚举元素，它就代表了一个Singleton的一个实例 INSTANCE; //单例可以有自己的操作 public void singletonOperation(){ //功能处理 } public static void main(String[] args){ Singleton sd = Singleton.INSTANCE; Singleton sd2 = Singleton.INSTANCE; System.out.println(sd==sd2); //true }} 反射和反序列化破解单例反射可以破解上面几种(不包含枚举)实现方式防止破解：可以在构造方法中手动抛出异常控制 以懒汉式为例： 使用反射机制获得单例的私有构造器来创建对象，或破坏单例模式 12345678910111213141516171819202122232425import java.lang.reflect.Constructor;public class Test { public static void main(String[] args) throws Exception { Singleton s1 = Singleton.getInstance(); Singleton s2 = Singleton.getInstance(); System.out.println(s1==s2); //true 同一对象 //1.使用反射的方式获得私有构造器创建对象 //加载Singleton类 //Class.forName中使用的是类的全限定名 Class&lt;Singleton&gt; clazz = (Class&lt;Singleton&gt;) Class.forName(\"com.company.Sort.Singleton\"); //获得私有构造器 Constructor&lt;Singleton&gt; constructor = clazz.getDeclaredConstructor(); //跳过权限检查 constructor.setAccessible(true); //通过构造器构建对象 Singleton s3 = constructor.newInstance(); Singleton s4 = constructor.newInstance(); System.out.println(s3==s4); //false 说明单例模式已经被破坏了 }} 防止反射破解： 在构造方法中手动抛出异常控制 123456789101112131415public class Singleton { private static Singleton instance; //构造方法中手动抛出异常防止反射机制破解 private Singleton(){ if(instance!=null) { throw new RuntimeException(); } } public static synchronized Singleton getInstance(){ if(instance==null) instance = new Singleton(); return instance; }} 再去执行上面的破解程序则会直接抛出异常java.lang.reflect.InvocationTargetException 反序列化可以破解面几种(不包含枚举)实现方式防止破解：可以通过定义readResolve（）防止获得不同对象，反序列化时，如果对象所在类定义了readResolve()(实际是一种回调)定义返回哪个对象 还是以懒汉式为例，通过反序列化的方式来破坏单例模式 12345678910111213141516171819202122232425import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;public class Test { public static void main(String[] args) throws Exception { Singleton s1 = Singleton.getInstance(); Singleton s2 = Singleton.getInstance(); System.out.println(s1==s2); //true 同一对象 //2.通过反序列化的方式构造多个对象 //当前前提是我们的Singleton类实现了Serializable接口 //序列化 ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"d:/a.txt\")); oos.writeObject(s1); oos.close(); //反序列化 ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\"d:a.txt\")); Singleton s3 = (Singleton) ois.readObject(); System.out.println(s3==s2); //false 新的对象 }} 防止反序列化破解： 可以通过定义readResolve()方法防止获得不同对象 12345678910111213141516171819202122232425262728import java.io.Serializable; /** * 懒汉式单例模式（如何防止反射和反序列化漏洞） */public class SingletonDemo06 implements Serializable { private static SingletonDemo06 instance; private SingletonDemo06() { //在构造器中加入判断，多次调用直接抛出异常，防止反射破解 if(instance != null){ throw new RuntimeException(); } } public static synchronized SingletonDemo06 getInstance(){ if(instance == null){ instance = new SingletonDemo06(); } return instance; } //防止反序列化的方式直接破解 //反序列化时，如果定义了readResolve()方法则直接返回此方法指定的对象，不需要再重新创建对象 private Object readResolve(){ return instance; }}","link":"/2020/08/17/%E5%8D%95%E4%BE%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"工厂方法模式","text":"主要内容 简单工厂模式 工厂方法模式 抽象工厂模式 概述工厂模式一共有下面三种： 简单工厂模式 工厂方法模式 抽象工厂模式 其中，简单工厂模式严格的说并不是23种常用的设计模式之一，它只算工厂模式的一个特殊实现。简单工厂模式在实际中的应用相对于其他2个工厂模式用的还是相对少得多，因为它只适应很多简单的情况。不过很多时候都会问到，这里也讲一下吧。 简单工厂模式 1.什么是简单工厂模式 简单工厂模式(Simple Factory Pattern)：又称为静态工厂方法(Static Factory Method)模式。它可以根据参数的不同返回不同类的实例。简单工厂模式专门定义一个类来负责创建其他类的实例，被创建的实例通常都具有共同的父类。 总结： 将对象的创建和对象本身业务处理分离可以降低系统的耦合度，使得两者修改起来都相对容易。 我们只需要传入相应的参数，就可以获得所需要的对象，而无须知道其创建细节。 2.为什么要有简单工厂模式 考虑一个简单的软件应用场景，一个软件系统可以提供多个外观不同的按钮（如圆形按钮、矩形按钮、菱形按钮等）， 这些按钮都源自同一个基类，不过在继承基类后不同的子类修改了部分属性从而使得它们可以呈现不同的外观，如果我们希望在使用这些按钮时，不需要知道这些具体按钮类的名字，只需要知道表示该按钮类的一个参数，并提供一个调用方便的方法，把该参数传入方法即可返回一个相应的按钮对象，此时，就可以使用简单工厂模式。 3.工厂模式实现 简单工厂模式包含如下角色： Factory：工厂角色 工厂角色负责实现创建所有实例的内部逻辑 Product：抽象产品角色 抽象产品角色是所创建的所有对象的父类，负责描述所有实例所共有的公共接口 ConcreteProduct：具体产品角色 具体产品角色是创建目标，所有创建的对象都充当这个角色的某个具体类的实例。 UML图如下所示： 123public interface Product {} 12public class ConcreteProduct implements Product {} 12public class ConcreteProductA implements Product {} 12public class ConcreteProductB implements Product {} 1234567891011public class SimpleFactory { public Product createProduct(String type) { if (type == \"A\") { return new ConcreteProductA(); } else if (type == \"B\") { return new ConcreteProduct2(); } return new ConcreteProduct(); }} 客户端使用时就可以像下面这样使用： 12345678public class Client { public static void main(String[] args) { SimpleFactory simpleFactory = new SimpleFactory(); Product productA = simpleFactory.createProduct(\"A\"); Product productB = simpleFactory.createProduct(\"B\"); // do something with the product }} 工厂方法模式 1.什么是工厂方法模式 工厂方法模式(Factory Method Pattern)：在工厂方法模式中，工厂父类负责定义创建产品对象的公共接口，而工厂子类则负责生成具体的产品对象，将产品类的实例化操作延迟到工厂子类中完成，即通过工厂子类来确定究竟应该实例化哪一个具体产品类。 2.为什么要有工厂方法模式 现在对该系统进行修改，不再设计一个按钮工厂类来统一负责所有产品的创建，而是将具体按钮的创建过程交给专门的工厂子类去完成，我们先定义一个抽象的按钮工厂类，再定义具体的工厂类来生成圆形按钮、矩形按钮、菱形按钮等，它们实现在抽象按钮工厂类中定义的方法。这种抽象化的结果使这种结构可以在不修改具体工厂类的情况下引进新的产品，如果出现新的按钮类型，只需要为这种新类型的按钮创建一个具体的工厂类就可以获得该新按钮的实例，这一特点无疑使得工厂方法模式具有超越简单工厂模式的优越性，更加符合“开闭原则”。 3.具体实现 工厂方法模式包含如下角色： Factory：抽象工厂 ConcreteFactory：具体工厂 Product：抽象产品 ConcreteProduct：具体产品 1234//抽象产品：提供了产品的接口public interface Product{ public void show();} 123456//具体产品A：实现抽象产品中的抽象方法public class ConcreteProductA implements Product{ public void show(){ System.out.println(\"具体产品A显示...\"); }} 123456//具体产品B：实现抽象产品中的抽象方法class ConcreteProductB implements Product{ public void show(){ System.out.println(\"具体产品B显示...\"); }} 1234//抽象工厂：提供了厂品的生成方法public interface AbstractFactory{ public Product newProduct();} 1234567//具体工厂A：实现了厂品的生成方法public class ConcreteFactoryA implements AbstractFactory{ public Product newProduct(){ System.out.println(\"具体工厂A生成--&gt;具体产品A...\"); return new ConcreteProductA(); }} 12345678//具体工厂B：实现了产品B的生成方法public class ConcreteFactoryB implements AbstractFactory{ public Product newProduct(){ System.out.println(\"具体工厂B生成--&gt;具体产品B...\"); return new ConcreteProductB(); }} 客户端使用： 1234567public class Client { public static void main(String[] args) { AbstractFactory factory = new ConcreteFactoryA(); Product productA = factory.newProduct(); // do something with the product }} 抽象工厂模式 1.什么是抽象工厂模式 抽象工厂模式(Abstract Factory Pattern)：提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类。抽象工厂模式又称为Kit模式，属于对象创建型模式。 2.为什么要有抽象工厂模式 前面介绍的工厂方法模式中考虑的是一类产品的生产，如畜牧场只养动物、电视机厂只生产电视机、计算机软件学院只培养计算机软件专业的学生等。 同种类称为同等级，也就是说：工厂方法模式只考虑生产同等级的产品，但是在现实生活中许多工厂是综合型的工厂，能生产多等级（种类） 的产品，如农场里既养动物又种植物，电器厂既生产电视机又生产洗衣机或空调，大学既有软件专业又有生物专业等。 当系统所提供的工厂所需生产的具体产品并不是一个简单的对象，而是多个位于不同产品等级结构中属于不同类型的具体产品时需要使用抽象工厂模式。 3.抽象工厂模式的实现 抽象工厂模式包含如下角色： AbstractFactory：抽象工厂 ConcreteFactory：具体工厂 AbstractProduct：抽象产品 Product：具体产品 (1) 抽象工厂：提供了产品的生成方法。 1234public interface AbstractFactory{ public ProductA newProductA(); public ProductB newProductB();} (2) 具体工厂：实现了产品的生成方法。 1234567891011public class ConcreteFactory1 implements AbstractFactory{ public ProductA newProductA(){ System.out.println(\"具体工厂 1 生成--&gt;具体产品 A...\"); return new ConcreteProductA(); } public ProductB newProductB() { System.out.println(\"具体工厂 1 生成--&gt;具体产品 B...\"); return new ConcreteProductB(); }} 其他的抽象和具体Product对象和方法和上面相同。","link":"/2020/08/17/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"title":"代理设计模式","text":"主要内容 代理设计模式概念和应用场景 代理模式的实现 动态代理 概述 1.什么是代理设计模式 代理模式(Proxy Pattern) ：给某一个对象提供一个代理，并由代理对象控制对原对象的引用。 2.为什么要有代理设计模式 在某些情况下，一个客户不想或者不能直接引用一个对 象，此时可以通过一个称之为“代理”的第三者来实现 间接引用。代理对象可以在客户端和目标对象之间起到 中介的作用，并且可以通过代理对象去掉客户不能看到 的内容和服务或者添加客户需要的额外服务。 代理模式的主要优点有： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度； 代理设计模式的实现代理模式包含如下角色： Subject: 抽象主题角色 Proxy: 代理主题角色 RealSubject: 真实主题角色 UML图如下所示： 1234//抽象主题public interface Subject{ void Request();} 123456//真实主题public class RealSubject implements Subject{ public void Request(){ System.out.println(\"访问真实主题方法...\"); }} 123456789101112131415161718//代理public class Proxy implements Subject { private RealSubject realSubject; public void Request(){ if (realSubject==null){ realSubject=new RealSubject(); } preRequest(); realSubject.Request(); postRequest(); } public void preRequest(){ System.out.println(\"访问真实主题之前的预处理。\"); } public void postRequest(){ System.out.println(\"访问真实主题之后的后续处理。\"); }} 动态代理在前面介绍的代理模式中，代理类中包含了对真实主题的引用，这种方式存在两个缺点： 真实主题与代理主题一一对应，增加真实主题也要增加代理。 设计代理以前真实主题必须事先存在，不太灵活。采用动态代理模式可以解决以上问题，如 SpringAOP，其结构图如图 4 所示。","link":"/2020/08/17/%E4%BB%A3%E7%90%86%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"模板方法模式","text":"主要内容 模板方法模式概述 模板方法模式实现 概述 1.什么是模板方法模式 模板方法（Template Method）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。它是一种类行为型模式。 2.为什么要有模板方法模式 例如，去银行办理业务一般要经过以下4个流程：取号、排队、办理具体业务、对银行工作人员进行评分等，其中取号、排队和对银行工作人员进行评分的业务对每个客户是一样的，可以在父类中实现，但是办理具体业务却因人而异，它可能是存款、取款或者转账等，可以延迟到子类中实现。 这样的例子在生活中还有很多，例如，一个人每天会起床、吃饭、做事、睡觉等，其中“做事”的内容每天可能不同。我们把这些规定了流程或格式的实例定义成模板，允许使用者根据自己的需求去更新它，例如，简历模板、论文模板、Word 中模板文件等。 模板方法模式的主要优点如下。 封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 模板方法模式的实现模板方法模式包含以下主要角色。 (1) 抽象类（Abstract Class）：负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下： ① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 ② 基本方法：是整个算法中的一个步骤，包含以下几种类型。 抽象方法：在抽象类中申明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 (2) 具体子类（Concrete Class）：实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。 1234567891011121314151617181920212223242526272829303132public class TemplateMethodPattern{ public static void main(String[] args){ AbstractClass tm=new ConcreteClass(); tm.TemplateMethod(); }}//抽象类abstract class AbstractClass{ //模板方法 public void TemplateMethod(){ SpecificMethod(); abstractMethod1(); abstractMethod2(); } //具体方法 public void SpecificMethod() { System.out.println(\"抽象类中的具体方法被调用...\"); } //抽象方法1 public abstract void abstractMethod1(); //抽象方法2 public abstract void abstractMethod2();}//具体子类class ConcreteClass extends AbstractClass{ public void abstractMethod1(){ System.out.println(\"抽象方法1的实现被调用...\"); } public void abstractMethod2(){ System.out.println(\"抽象方法2的实现被调用...\"); }}","link":"/2020/08/17/%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"HTTPS","slug":"HTTPS","link":"/tags/HTTPS/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"AQS","slug":"AQS","link":"/tags/AQS/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","link":"/tags/ConcurrentHashMap/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Springboot","slug":"Springboot","link":"/tags/Springboot/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"categories":[{"name":"计算机基础","slug":"计算机基础","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"计算机网络","slug":"计算机基础/计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"JVM","slug":"Java基础/JVM","link":"/categories/Java%E5%9F%BA%E7%A1%80/JVM/"},{"name":"多线程","slug":"Java基础/多线程","link":"/categories/Java%E5%9F%BA%E7%A1%80/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"数据库(MySQL)","slug":"计算机基础/数据库-MySQL","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93-MySQL/"},{"name":"操作系统(linux)","slug":"计算机基础/操作系统-linux","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-linux/"},{"name":"数据结构与算法","slug":"计算机基础/数据结构与算法","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"集群与分布式","slug":"集群与分布式","link":"/categories/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Redis","slug":"集群与分布式/Redis","link":"/categories/%E9%9B%86%E7%BE%A4%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F/Redis/"},{"name":"集合类","slug":"Java基础/集合类","link":"/categories/Java%E5%9F%BA%E7%A1%80/%E9%9B%86%E5%90%88%E7%B1%BB/"},{"name":"工具与环境配置","slug":"工具与环境配置","link":"/categories/%E5%B7%A5%E5%85%B7%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"软件安装和环境配置","slug":"工具与环境配置/软件安装和环境配置","link":"/categories/%E5%B7%A5%E5%85%B7%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"},{"name":"设计模式","slug":"Java基础/设计模式","link":"/categories/Java%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]}